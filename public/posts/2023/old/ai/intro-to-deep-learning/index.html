<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Intro to Deep Learning | Hylu Blog</title>
<meta name="keywords" content="ai">
<meta name="description" content="Neural Networks See Machine Learning
How do you decide how many neurons to use per layer?
One way is to start with all layers uing the same amount of neurons and continue adding them until they start overfitting the data
Dropout: Regularlization technique to avoid overfitting. Leaves out data to better deal with general cases. 20%-50% dropout is a good starting range. Momentum: Helps finding the direction of next descent and prevent oscillations.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/2023/old/ai/intro-to-deep-learning/">
<link rel="stylesheet" href="http://localhost:1313/css/custom.css">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8198515fa77a6b7ffb462312ed9ac85b9c0c3f31618c387a9779d093d86bbbac.css" integrity="sha256-gZhRX6d6a3/7RiMS7ZrIW5wMPzFhjDh6l3nQk9hru6w=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2023/old/ai/intro-to-deep-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Hylu Blog (Alt + H)">Hylu Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title">
      Intro to Deep Learning
    </h1>
    <div class="post-meta"><span title='2023-06-12 17:22:24 -0400 EDT'>June 12, 2023</span>

</div>
    
    
    
    
    
<script
  type="application/javascript"
  id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  async
></script>
<script 
  type="text/x-mathjax-config"
  async
  >
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
           extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
</script>
    
  </header> 
<figure class="entry-cover"><a href="https://www.simplilearn.com/ice9/free_resources_article_thumb/Convolutional_Neural_Network_Tutorial.jpg" target="_blank"
            rel="noopener noreferrer"><img loading="lazy" src="https://www.simplilearn.com/ice9/free_resources_article_thumb/Convolutional_Neural_Network_Tutorial.jpg" alt=""></a>
        
</figure>
<style>
    .entry-cover img {
        height: 200px !important;
        object-fit: cover !important;
    }
</style>
<div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#neural-networks" aria-label="Neural Networks">Neural Networks</a><ul>
                        
                <li>
                    <a href="#back-propagation" aria-label="Back Propagation">Back Propagation</a></li>
                <li>
                    <a href="#common-problems" aria-label="Common Problems">Common Problems</a><ul>
                        
                <li>
                    <a href="#overfitting" aria-label="Overfitting">Overfitting</a></li>
                <li>
                    <a href="#vanishing-gradients" aria-label="Vanishing Gradients">Vanishing Gradients</a></li>
                <li>
                    <a href="#exploding-gradients" aria-label="Exploding Gradients">Exploding Gradients</a></li></ul>
                </li>
                <li>
                    <a href="#stabilizing-gradients" aria-label="Stabilizing Gradients">Stabilizing Gradients</a><ul>
                        
                <li>
                    <a href="#batch-normalization" aria-label="Batch Normalization">Batch Normalization</a></li>
                <li>
                    <a href="#gradient-clipping" aria-label="Gradient Clipping">Gradient Clipping</a></li></ul>
                </li>
                <li>
                    <a href="#regulariation" aria-label="Regulariation">Regulariation</a><ul>
                        
                <li>
                    <a href="#l1--l2-regularization" aria-label="L1 &amp; L2 Regularization">L1 &amp; L2 Regularization</a></li></ul>
                </li>
                <li>
                    <a href="#dropout" aria-label="Dropout">Dropout</a></li></ul>
                </li>
                <li>
                    <a href="#convolutions" aria-label="Convolutions">Convolutions</a><ul>
                        
                <li>
                    <a href="#convolution-operation-in-1d" aria-label="Convolution Operation in 1D">Convolution Operation in 1D</a></li>
                <li>
                    <a href="#convolutions-in-2d-images" aria-label="Convolutions in 2D (Images)">Convolutions in 2D (Images)</a></li>
                <li>
                    <a href="#building-a-convolutional-network-with-tensorflow" aria-label="Building a Convolutional Network with TensorFlow">Building a Convolutional Network with TensorFlow</a><ul>
                        
                <li>
                    <a href="#prepare-the-input-data" aria-label="Prepare the Input Data">Prepare the Input Data</a></li>
                <li>
                    <a href="#constructing-the-model" aria-label="Constructing the Model">Constructing the Model</a></li></ul>
                </li>
                <li>
                    <a href="#compiling-the-model" aria-label="Compiling the Model">Compiling the Model</a></li>
                <li>
                    <a href="#transfer-learning" aria-label="Transfer Learning">Transfer Learning</a></li></ul>
                </li>
                <li>
                    <a href="#deep-computer-vision" aria-label="Deep Computer Vision">Deep Computer Vision</a><ul>
                        
                <li>
                    <a href="#object-classification-and-localization" aria-label="Object Classification and Localization">Object Classification and Localization</a></li>
                <li>
                    <a href="#tf-model-for-identifying-single-objects" aria-label="TF Model for Identifying Single Objects">TF Model for Identifying Single Objects</a></li>
                <li>
                    <a href="#multiple-object-classification" aria-label="Multiple Object Classification">Multiple Object Classification</a><ul>
                        
                <li>
                    <a href="#sliding-window-method" aria-label="Sliding Window Method">Sliding Window Method</a></li>
                <li>
                    <a href="#single-shot-detector" aria-label="Single Shot Detector">Single Shot Detector</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#recurrent-neural-networks" aria-label="Recurrent Neural Networks">Recurrent Neural Networks</a><ul>
                        
                <li>
                    <a href="#unfolding-layers" aria-label="Unfolding Layers">Unfolding Layers</a></li>
                <li>
                    <a href="#backpropagation-in-time-bptt" aria-label="Backpropagation in Time (BPTT)">Backpropagation in Time (BPTT)</a></li>
                <li>
                    <a href="#lstm" aria-label="LSTM">LSTM</a></li></ul>
                </li>
                <li>
                    <a href="#natural-language-processing-nlp" aria-label="Natural Language Processing (NLP)">Natural Language Processing (NLP)</a><ul>
                        
                <li>
                    <a href="#common-problems-1" aria-label="Common Problems">Common Problems</a></li>
                <li>
                    <a href="#tokenization" aria-label="Tokenization">Tokenization</a></li>
                <li>
                    <a href="#text-preprocessing" aria-label="Text Preprocessing">Text Preprocessing</a></li>
                <li>
                    <a href="#integer-encoding-vs-word-embeddings" aria-label="Integer Encoding vs Word Embeddings">Integer Encoding vs Word Embeddings</a></li>
                <li>
                    <a href="#tf-idf--similarity" aria-label="TF-IDF &amp; Similarity">TF-IDF &amp; Similarity</a></li></ul>
                </li>
                <li>
                    <a href="#encoder-decoder-architecture" aria-label="Encoder-Decoder Architecture">Encoder-Decoder Architecture</a><ul>
                        
                <li>
                    <a href="#auto-encoders" aria-label="Auto Encoders">Auto Encoders</a></li></ul>
                </li>
                <li>
                    <a href="#generative-algorithms" aria-label="Generative Algorithms">Generative Algorithms</a><ul>
                        
                <li>
                    <a href="#variational-auto-encoders" aria-label="Variational Auto Encoders">Variational Auto Encoders</a></li></ul>
                </li>
                <li>
                    <a href="#course-retrospective" aria-label="Course Retrospective">Course Retrospective</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="neural-networks">Neural Networks<a hidden class="anchor" aria-hidden="true" href="#neural-networks">#</a></h2>
<p>See Machine Learning</p>
<p>How do you decide how many neurons to use per layer?</p>
<blockquote>
<p>One way is to start with all layers uing the same amount of neurons and continue adding them until they start overfitting the data</p>
</blockquote>
<ul>
<li><strong>Dropout</strong>: Regularlization technique to avoid overfitting. Leaves out data to better deal with general cases. 20%-50% dropout is a good starting range.</li>
<li><strong>Momentum</strong>: Helps finding the direction of next descent and prevent oscillations. Typical choice between 0.5 to 0.9</li>
<li><strong>Epochs</strong> #times the whole training data is shown to netowrk during training. 1 epoch = one forward and one backward pass of all training data</li>
</ul>
<h3 id="back-propagation">Back Propagation<a hidden class="anchor" aria-hidden="true" href="#back-propagation">#</a></h3>
<p>See Machine Learning</p>
<p>Using the gradient (slope) of the cost function, we take steps down the slope through multiple learning iterations in order to find some local minima to the cost. Ideally leading to a high accuracy model.</p>
<h3 id="common-problems">Common Problems<a hidden class="anchor" aria-hidden="true" href="#common-problems">#</a></h3>
<h4 id="overfitting">Overfitting<a hidden class="anchor" aria-hidden="true" href="#overfitting">#</a></h4>
<p>Model gets too comfortable with the test set and becomes less able to classify data outside of it.
Accuracy improves, validation (outside data) worsens.</p>
<h4 id="vanishing-gradients">Vanishing Gradients<a hidden class="anchor" aria-hidden="true" href="#vanishing-gradients">#</a></h4>
<p>Gradients often get smaller as back propagation moves to lower layers (closer to start input). Can result earlier layers being virtually unchanged
Result</p>
<ul>
<li>Training doesn&rsquo;t converge to a good solution</li>
<li>Prevent network from learning long term dependencies</li>
</ul>
<h4 id="exploding-gradients">Exploding Gradients<a hidden class="anchor" aria-hidden="true" href="#exploding-gradients">#</a></h4>
<p>Gradients get too large and algorithm starts to diverge.</p>
<h3 id="stabilizing-gradients">Stabilizing Gradients<a hidden class="anchor" aria-hidden="true" href="#stabilizing-gradients">#</a></h3>
<p>Techniques to avoid and remedy problems such as the aforementioned.</p>
<h4 id="batch-normalization">Batch Normalization<a hidden class="anchor" aria-hidden="true" href="#batch-normalization">#</a></h4>
<p>Normalize the layers inputs (recenter and scale to a normal distribution) at each given layer</p>
<ul>
<li>Can remove the need of normalizing your inputs</li>
<li>Adds complexity to model as well as runtime penality</li>
</ul>
<h4 id="gradient-clipping">Gradient Clipping<a hidden class="anchor" aria-hidden="true" href="#gradient-clipping">#</a></h4>
<p>Clamps every component of the gradient vector to [-1.0, 1.0]</p>
<h3 id="regulariation">Regulariation<a hidden class="anchor" aria-hidden="true" href="#regulariation">#</a></h3>
<p>Methods to avoid overfitting data</p>
<h4 id="l1--l2-regularization">L1 &amp; L2 Regularization<a hidden class="anchor" aria-hidden="true" href="#l1--l2-regularization">#</a></h4>
<p>Both these methods involve adding another term to the cost function that increases the cost as complexity of the model increases. Typically done before activation.</p>
<p><strong>L1</strong>: Used in Lasso Regression adds an absolute value value of weights. This linearly increases cost of certain features and unimportant costs will eventually converge to zero, effectively reducing our relevant feature set.</p>
<p><strong>L2</strong>: Used in Ridge Regression adds the sum of squares weights. This increases the cost of features down to near but not quite zero. Doesn&rsquo;t remove complexity as effectively but can better retain accuracy of the model.</p>
<h3 id="dropout">Dropout<a hidden class="anchor" aria-hidden="true" href="#dropout">#</a></h3>
<p>Every training step, any neuron has a chance of being left out and ignored for that training step. Reduces model complexity and avoid picking up too much noise. Typically set at 50%.</p>
<h2 id="convolutions">Convolutions<a hidden class="anchor" aria-hidden="true" href="#convolutions">#</a></h2>
<p>Analying images is a complex matter for machine learning as our feature vector scales with the number of pixels in our image. If we were to directly use those inputs, we are likely to get a highly overfit model with very long training times.</p>
<blockquote>
<p>The role of ConvNet is to reduce the images into a form that is easier to process, without losing features that are critical for getting a good prediction. This is important when we are to design an architecture that is not only good at learning features but also scalable to massive datasets. - <a href="https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/">https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/</a></p>
</blockquote>
<h3 id="convolution-operation-in-1d">Convolution Operation in 1D<a hidden class="anchor" aria-hidden="true" href="#convolution-operation-in-1d">#</a></h3>
<p>The standard operation in 1D works as follows:</p>
<p>Let&rsquo;s take two arrays</p>
<p>$$A = [\textbf{5,3,7},5,9,7]$$
$$B = [1,2,3]$$</p>
<p>We then we take the elements in B and multiply it by the first n elements in A where n is the length of B. We then sum up the values.</p>
<p>$$A\cdot B = 5\cdot 1 +  3\cdot 2 + 7\cdot 3 = 17$$</p>
<p>We then slide our choice of n elements by 1 and repeat the operation.</p>
<p>$$A = [5,\textbf{3,7,5},9,7]$$
$$B = [1,2,3]$$</p>
<p>$$A\cdot B = 3\cdot 1 +  7\cdot 2 + 5\cdot 3 = 22$$</p>
<p>We then repeat this operation until our choice of n elements reaches the end, giving the following convoluted array.</p>
<p>$$A\times B = [17, 22, 39, 44]$$</p>
<p>Notice the length of our convoluted array is \(len(A)-len(B)+1\)</p>
<h3 id="convolutions-in-2d-images">Convolutions in 2D (Images)<a hidden class="anchor" aria-hidden="true" href="#convolutions-in-2d-images">#</a></h3>
<p>The operations for 2D is largely the same, the difference being we are now operating on matrices.</p>
<p>We take the matrix of the pixels in our original image and multiply it by a filter matrix known as a <strong>kernel</strong>. The output of the convolution is known as the <strong>feature map</strong>.</p>
<p>$$
\begin{bmatrix}
1&amp;0&amp;1\\
0&amp;1&amp;0\\
1&amp;0&amp;1
\end{bmatrix}
$$</p>
<blockquote>
<p>Filters can be configured as any combination of 1&rsquo;s and 0&rsquo;s. For example, you might have one filter of a vertical line and another with a horizontal line. A vertical filter would blur the horizontal features of the image and similarly vice versa. Imagine how choosing specific filters can allow your feature map to focus on specific details of your image.</p>
</blockquote>
<p>As we slide our kernel across the pixel array, we must move across the vertial and horizontal pixels to cover the whole image.</p>
<div class="center-flex">
  <a class="center-flex link " href="https://d33wubrfki0l68.cloudfront.net/bd7d4b44eee0adfc2a64a6bf35c2e058e25b5aa2/38785/images/blog/convoluting-a-5x5x1-image-with-a-3x3x1-kernel-to-get-a-3x3x1-convolved-feature.gif" target="_blank">
    <img alt="https://d33wubrfki0l68.cloudfront.net/bd7d4b44eee0adfc2a64a6bf35c2e058e25b5aa2/38785/images/blog/convoluting-a-5x5x1-image-with-a-3x3x1-kernel-to-get-a-3x3x1-convolved-feature.gif" src="https://d33wubrfki0l68.cloudfront.net/bd7d4b44eee0adfc2a64a6bf35c2e058e25b5aa2/38785/images/blog/convoluting-a-5x5x1-image-with-a-3x3x1-kernel-to-get-a-3x3x1-convolved-feature.gif" class="img">
  </a>
</div>

<style>
  .img {
    box-shadow: 1px 1px 5px #000;
  }

  .img-xs, .img-xs img {
    width: 35%;
  }

  .img-sm, .img-sm img {
    width: 50%;
  }

  .img-md, .img-md img {
    width: 70%;
  }

  .img-lg, .img-lg img {
    width: 85%
  }

  .img-xl, .img-xl img {
    width: 100%
  }

  .banner {
    width: 100%;
  }

  .banner img {
    width: 100%;
    object-fit: cover;
    height: 150px;
  }

  .link {
    text-decoration: none;
    outline: none;
    box-shadow: none !important;
  }

  .center-flex {
    display: flex;
    justify-content: center;
  }

  .white img {
    background-color: white;
  }
</style>
<h3 id="building-a-convolutional-network-with-tensorflow">Building a Convolutional Network with TensorFlow<a hidden class="anchor" aria-hidden="true" href="#building-a-convolutional-network-with-tensorflow">#</a></h3>
<h4 id="prepare-the-input-data">Prepare the Input Data<a hidden class="anchor" aria-hidden="true" href="#prepare-the-input-data">#</a></h4>
<p>Convolutional networks are most commonly used for image analysis so we want to prepare our data as a 2D array of pixels. The dimensions of our data will depend on the <strong>dimensions</strong> of the image, the <strong>colour channels</strong>, the numbers of <strong>classifiers</strong>, as well as how many <strong>samples</strong> we have.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># we are using mnist in which which we have 10 digit classification</span>
</span></span><span style="display:flex;"><span>num_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># input image dimensions</span>
</span></span><span style="display:flex;"><span>img_rows, img_cols <span style="color:#f92672">=</span> <span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># colour channels, in this case we&#39;re using b/w images</span>
</span></span><span style="display:flex;"><span>num_channels <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the data, split between train and test sets</span>
</span></span><span style="display:flex;"><span>(X_train, y_train), (X_test, y_test) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data() <span style="color:#75715e"># returns numpy arrays</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 80/20 training and validation set</span>
</span></span><span style="display:flex;"><span>(X_valid, y_valid) <span style="color:#f92672">=</span> X_train[:<span style="color:#ae81ff">12000</span>], y_train[:<span style="color:#ae81ff">12000</span>]  
</span></span><span style="display:flex;"><span>X_train, y_train <span style="color:#f92672">=</span> X_train[<span style="color:#ae81ff">12000</span>:], y_train[<span style="color:#ae81ff">12000</span>:]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Shape of X_train: (60000, 28, 28)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Shape of y_train: (60000,)</span>
</span></span></code></pre></div><p>Here we do some shaping of our arrays to better used in our model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># numpy reshape from array of 2D arrays to array of 3D arrays</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The first dimension is the number of samples in our dataset, the other 3 are the image dimensions, we add another dimension for the colour channels</span>
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">60000</span>, img_rows, img_cols, num_channels) 
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> X_test<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">10000</span>, img_rows, img_cols, num_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Converts our y_train from a 1D vector to a 2D matrix</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Before our y_train contains 60000 truth samples from 0-9</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Now it contains 60000 vectors of size &lt;num_classes&gt; where instead of containing the number explicitly,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the number is one-hot encoded into the indices</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Ex. 4 would now be encoded [0,0,0,1,0,0,0,0,0,0] in the matrix</span>
</span></span><span style="display:flex;"><span>y_train <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(y_train, num_classes)
</span></span><span style="display:flex;"><span>y_test <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(y_test, num_classes)
</span></span></code></pre></div><h4 id="constructing-the-model">Constructing the Model<a hidden class="anchor" aria-hidden="true" href="#constructing-the-model">#</a></h4>
<p>Creating the model is straight forward and involves setting hyperparameters depending on how we want to tune our training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(
</span></span><span style="display:flex;"><span>    Conv2D(
</span></span><span style="display:flex;"><span>        filters<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, 
</span></span><span style="display:flex;"><span>        kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>),
</span></span><span style="display:flex;"><span>        activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, 
</span></span><span style="display:flex;"><span>        input_shape<span style="color:#f92672">=</span>(img_rows, img_cols, num_channels)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Conv2D(<span style="color:#ae81ff">1</span>, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># apply max pooling to downsize the output by a factor of 2</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(MaxPooling2D(pool_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># apply dropout for regularization</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Dropout(<span style="color:#ae81ff">0.25</span>))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># flatten the output features to perform standard softmax classification</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Flatten())
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the dense layer learns the &#39;classification&#39; part of our task</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Dense(num_classes, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)) <span style="color:#75715e"># output layer</span>
</span></span></code></pre></div><h3 id="compiling-the-model">Compiling the Model<a hidden class="anchor" aria-hidden="true" href="#compiling-the-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>categorical_crossentropy,
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(),
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(
</span></span><span style="display:flex;"><span>    X_train, 
</span></span><span style="display:flex;"><span>    y_train,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span>batch_size,
</span></span><span style="display:flex;"><span>    epochs<span style="color:#f92672">=</span>epochs,
</span></span><span style="display:flex;"><span>    verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    validation_data<span style="color:#f92672">=</span>(X_valid, y_valid)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>evaluate(X_test, y_test, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Validation loss:&#39;</span>, score[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Validation accuracy:&#39;</span>, score[<span style="color:#ae81ff">1</span>])
</span></span></code></pre></div><h3 id="transfer-learning">Transfer Learning<a hidden class="anchor" aria-hidden="true" href="#transfer-learning">#</a></h3>
<p>Datasets are integral to the success of a neural network but often there just isn&rsquo;t big enough datasets that exist for every problem. In the case of images, trying to build a model to classify a niche subject matters can be difficult without ample data.</p>
<p><strong>Transfer learning allows us to use some or all of pre-trained models and iterate over them to create models</strong>. Many fantastic image models have already been professionally built across many talented minds so it&rsquo;s advantageous to piggy back off of them.</p>
<p><strong>But how is someone elses model useful if we&rsquo;re trying to solve a niche problem?</strong> Early layers in neural nets are more generalized in the features they pick up and become more and more fine tuned the deeper the net goes. <strong>We can leverage the earlier layers in an existing model</strong> that&rsquo;s well trained to classify broad shapes in images and <em>freeze</em> the remaining layers and instead use our own layers that are trained to more specifically classify those shapes in the way we want. That&rsquo;s the general idea.</p>
<p><strong>VGG16</strong> is one such powerful neural net for image recognition we can leverage in our own model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># VGG16 pre-trained model without fully connected layers and with different input dimensions</span>
</span></span><span style="display:flex;"><span>image_w, image_h <span style="color:#f92672">=</span> <span style="color:#ae81ff">180</span>, <span style="color:#ae81ff">180</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>VGG16(weights <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;imagenet&#34;</span>, include_top<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, input_shape <span style="color:#f92672">=</span> (image_w, image_h, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We freeze several layers in VGG16 to avoid training them</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>layers[:<span style="color:#ae81ff">17</span>]:
</span></span><span style="display:flex;"><span>    layer<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span></code></pre></div><p>We can preprend a new model with the existing model and add our own layers for training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>new_model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    model,
</span></span><span style="display:flex;"><span>    Flatten(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;flatten&#39;</span>),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">260</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;new_fc1&#39;</span>, kernel_initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HeNormal&#34;</span>),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">100</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;new_fc2&#39;</span>, kernel_initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HeNormal&#34;</span>),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">5</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;new_predictions&#39;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>new_model<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>SparseCategoricalCrossentropy(from_logits<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>),
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Sometimes it can be advantageous to freeze all of the layers of the pretrained to avoid overfitting, especially if we have a small dataset. As always, finding methods to the madness in a neural network is highly involved with trial and error and constant comparison.</p>
<h2 id="deep-computer-vision">Deep Computer Vision<a hidden class="anchor" aria-hidden="true" href="#deep-computer-vision">#</a></h2>
<p>Computer vision is the field of computer science dedicated to enabling computers to understand images and videos.</p>
<h3 id="object-classification-and-localization">Object Classification and Localization<a hidden class="anchor" aria-hidden="true" href="#object-classification-and-localization">#</a></h3>
<p>Being able to identifY specific objects within an image frequently used in a variety of technological applications. For example, face filters need to identify where human faces and specific features are. This can become compilicated as any single picture can have a variety of sizes, aspect ratios, colours, and objects of the interest the computer needs to filter through in order to identify an object.</p>
<h3 id="tf-model-for-identifying-single-objects">TF Model for Identifying Single Objects<a hidden class="anchor" aria-hidden="true" href="#tf-model-for-identifying-single-objects">#</a></h3>
<p>A simple model involves training a network to identify an object in an image by outputting a bounding box at the desired location. For identifying a single object, this is a fairly simple task</p>
<p>First we acquire a dataset of images with box labelling</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Construct a tf.data.Dataset</span>
</span></span><span style="display:flex;"><span>ds_train <span style="color:#f92672">=</span> tfds<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;voc/2012&#39;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train&#39;</span>, shuffle_files<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>ds_test, ds_validation <span style="color:#f92672">=</span> tfds<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;voc&#39;</span>, split<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;test&#39;</span>,<span style="color:#e6db74">&#39;validation&#39;</span>], shuffle_files<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>This particular dataset has lot&rsquo;s of components as shown below</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>print(ds_train<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">1</span>)) <span style="color:#75715e"># creates a new dataset with a max size of 1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#f92672">&lt;</span>_TakeDataset element_spec<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;image&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>, <span style="color:#ae81ff">3</span>), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>uint8, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;image/filename&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>string, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;labels&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int64, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;labels_no_difficult&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int64, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;objects&#39;</span>: {<span style="color:#e6db74">&#39;bbox&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, <span style="color:#ae81ff">4</span>), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;is_difficult&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>bool, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;is_truncated&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>bool, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;label&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int64, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>), <span style="color:#e6db74">&#39;pose&#39;</span>: TensorSpec(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int64, name<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)}}<span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><p>Some of the important components include</p>
<ul>
<li><strong>image</strong>: An image tensor</li>
<li><strong>labels</strong>: The label for the image</li>
<li><strong>bbox</strong>: The bounding box of object in this sample.
<ul>
<li>bbox[0]: \(l\) from top bound to top edge of box</li>
<li>bbox[1]: \(l\) from left bound to left edge of box</li>
<li>bbox[2]: \(l\) from top bound to bottom edge of box</li>
<li>bbox[3]: \(l\) from left bound to right edge of box</li>
</ul>
</li>
</ul>
<p>Next we section off our dataset for training</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Map our dataset to just the image, label, and bounding box and form into batches of 32 for training</span>
</span></span><span style="display:flex;"><span>ds_train_image <span style="color:#f92672">=</span> ds_train<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> a: (tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(a[<span style="color:#e6db74">&#39;image&#39;</span>], [<span style="color:#ae81ff">300</span>,<span style="color:#ae81ff">300</span>]), (a[<span style="color:#e6db74">&#39;objects&#39;</span>][<span style="color:#e6db74">&#39;label&#39;</span>][<span style="color:#ae81ff">0</span>], a[<span style="color:#e6db74">&#39;objects&#39;</span>][<span style="color:#e6db74">&#39;bbox&#39;</span>][<span style="color:#ae81ff">0</span>])))<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>ds_test_image <span style="color:#f92672">=</span> ds_test<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> a: (tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(a[<span style="color:#e6db74">&#39;image&#39;</span>], [<span style="color:#ae81ff">300</span>,<span style="color:#ae81ff">300</span>]), (a[<span style="color:#e6db74">&#39;objects&#39;</span>][<span style="color:#e6db74">&#39;label&#39;</span>][<span style="color:#ae81ff">0</span>], a[<span style="color:#e6db74">&#39;objects&#39;</span>][<span style="color:#e6db74">&#39;bbox&#39;</span>][<span style="color:#ae81ff">0</span>])))<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>ds_validation_image <span style="color:#f92672">=</span> ds_validation<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> a: (tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>resize(a[<span style="color:#e6db74">&#39;image&#39;</span>], [<span style="color:#ae81ff">300</span>,<span style="color:#ae81ff">300</span>]), (a[<span style="color:#e6db74">&#39;objects&#39;</span>][<span style="color:#e6db74">&#39;label&#39;</span>][<span style="color:#ae81ff">0</span>], a[<span style="color:#e6db74">&#39;objects&#39;</span>][<span style="color:#e6db74">&#39;bbox&#39;</span>][<span style="color:#ae81ff">0</span>])))
</span></span></code></pre></div><p>Create the model</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># We start with the Xception model (71 layers) as well with pre-trained weights (transfer learning)</span>
</span></span><span style="display:flex;"><span>base_model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>Xception(weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;imagenet&#34;</span>, include_top<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Teras model functional API https://keras.io/guides/functional_api/ as opposed to sequentially adding layers</span>
</span></span><span style="display:flex;"><span>avg <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>GlobalAveragePooling2D()(base_model<span style="color:#f92672">.</span>output)
</span></span><span style="display:flex;"><span>avg <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">200</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>)(avg)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Nadam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.00001</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>class_output <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(n_classes, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;softmax&#34;</span>,name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;class&#34;</span>)(avg)
</span></span><span style="display:flex;"><span>loc_output <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">4</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;loc&#34;</span>)(avg)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>base_model<span style="color:#f92672">.</span>input,
</span></span><span style="display:flex;"><span>                    outputs<span style="color:#f92672">=</span>[class_output, loc_output])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Freeze layers from training</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>layers[:<span style="color:#ae81ff">110</span>]:
</span></span><span style="display:flex;"><span>  layer<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>layers[<span style="color:#ae81ff">110</span>:]:
</span></span><span style="display:flex;"><span>  layer<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;sparse_categorical_crossentropy&#34;</span>, <span style="color:#e6db74">&#34;mse&#34;</span>],
</span></span><span style="display:flex;"><span>              loss_weights<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.9</span>], <span style="color:#75715e"># depends on what you care most about</span>
</span></span><span style="display:flex;"><span>              optimizer<span style="color:#f92672">=</span>optimizer, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>summary()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(ds_train_image, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, validation_data<span style="color:#f92672">=</span>ds_test_image)
</span></span></code></pre></div><h3 id="multiple-object-classification">Multiple Object Classification<a hidden class="anchor" aria-hidden="true" href="#multiple-object-classification">#</a></h3>
<p>Things become much more complicated when you want your model to identify multiple objects witin a given image. Different bounding boxes are needed with their associated locations and dimensions.</p>
<h4 id="sliding-window-method">Sliding Window Method<a hidden class="anchor" aria-hidden="true" href="#sliding-window-method">#</a></h4>
<p>A naive approach involves sliding a bounding box across the image and having the model identify the bounding boxes that identify an object. This is computationally expensive as we need to consider differing box dimensions.</p>
<div class="center-flex">
  <a class="center-flex link img-sm" href="https://developers.arcgis.com/python/guide/images/slidingwindow.gif" target="_blank">
    <img alt="https://developers.arcgis.com/python/guide/images/slidingwindow.gif" src="https://developers.arcgis.com/python/guide/images/slidingwindow.gif" class="img">
  </a>
</div>

<style>
  .img {
    box-shadow: 1px 1px 5px #000;
  }

  .img-xs, .img-xs img {
    width: 35%;
  }

  .img-sm, .img-sm img {
    width: 50%;
  }

  .img-md, .img-md img {
    width: 70%;
  }

  .img-lg, .img-lg img {
    width: 85%
  }

  .img-xl, .img-xl img {
    width: 100%
  }

  .banner {
    width: 100%;
  }

  .banner img {
    width: 100%;
    object-fit: cover;
    height: 150px;
  }

  .link {
    text-decoration: none;
    outline: none;
    box-shadow: none !important;
  }

  .center-flex {
    display: flex;
    justify-content: center;
  }

  .white img {
    background-color: white;
  }
</style>
<h4 id="single-shot-detector">Single Shot Detector<a hidden class="anchor" aria-hidden="true" href="#single-shot-detector">#</a></h4>
<h2 id="recurrent-neural-networks">Recurrent Neural Networks<a hidden class="anchor" aria-hidden="true" href="#recurrent-neural-networks">#</a></h2>
<p>These are a special type of neural net designed to work for a sequence of data. Normally each set of inputs are independent from eachother but there are needs for them to depend on each other such as sequences of words or sounds. The main difference is that we feed our output back into the next set of inputs and keep a concept of &ldquo;memory&rdquo; in our net.</p>
<h3 id="unfolding-layers">Unfolding Layers<a hidden class="anchor" aria-hidden="true" href="#unfolding-layers">#</a></h3>
<p>Unfolding is a terminology to describe a very simple process. Examining a recurrent layer over multiple timesteps. Rather than showing one recurrent network, we show it as multiple networks that feed into eachother to create that recurrent behavior. This makes it easier to see how our recursion occurs as well as how backpropagation occurs.</p>
<div class="center-flex">
  <a class="center-flex link " href="https://machinelearningmastery.com/wp-content/uploads/2021/09/rnn1-1536x726.png" target="_blank">
    <img alt="https://machinelearningmastery.com/wp-content/uploads/2021/09/rnn1-1536x726.png" src="https://machinelearningmastery.com/wp-content/uploads/2021/09/rnn1-1536x726.png" class="img">
  </a>
</div>

<style>
  .img {
    box-shadow: 1px 1px 5px #000;
  }

  .img-xs, .img-xs img {
    width: 35%;
  }

  .img-sm, .img-sm img {
    width: 50%;
  }

  .img-md, .img-md img {
    width: 70%;
  }

  .img-lg, .img-lg img {
    width: 85%
  }

  .img-xl, .img-xl img {
    width: 100%
  }

  .banner {
    width: 100%;
  }

  .banner img {
    width: 100%;
    object-fit: cover;
    height: 150px;
  }

  .link {
    text-decoration: none;
    outline: none;
    box-shadow: none !important;
  }

  .center-flex {
    display: flex;
    justify-content: center;
  }

  .white img {
    background-color: white;
  }
</style>
<h3 id="backpropagation-in-time-bptt">Backpropagation in Time (BPTT)<a hidden class="anchor" aria-hidden="true" href="#backpropagation-in-time-bptt">#</a></h3>
<p>Our normal backpropagation algorithm is updated for use in RNN&rsquo;s.</p>
<ul>
<li>choose \((k\)) time steps for training</li>
<li>Propagate through unfolded network for \(k\) time steps compute output at all timesteps</li>
<li>Calcualte error as: \e = (y_{t+k} - p_{t+k})
<ul>
<li>\(y\) is our output \(p\) is our target output at the time step</li>
</ul>
</li>
<li>Backpropagate the error across unfolded network and update weights.</li>
</ul>
<h3 id="lstm">LSTM<a hidden class="anchor" aria-hidden="true" href="#lstm">#</a></h3>
<p>Standard RNN&rsquo;s often suffer from vanishing or exploding gradients when the sequence of data gets too large. It doesn&rsquo;t do well on remembering data from very old time steps. LSTMs add another component to the network that learns what timesteps should be remembered or forgotten. This way, we can keep track of long term sequential data.</p>
<h2 id="natural-language-processing-nlp">Natural Language Processing (NLP)<a hidden class="anchor" aria-hidden="true" href="#natural-language-processing-nlp">#</a></h2>
<p>The computational treatment of human language. Many large datasets of text exist for training known as a <em>corpus</em> that are large, structured sets of text for machine learning.</p>
<p>Generally, the process of creating an NLP model requires a lot of preprocessing of your training text and figuring which method of text chunking and labelling best suits the model you want.</p>
<p>Later when we talk about word embeddings, the learning component of NLP comes into play where can train a model to set a weights for each word in a vocabulary.</p>
<div class="center-flex">
  <a class="center-flex link img-sm" href="https://www.researchgate.net/profile/Kim-Schouten/publication/318138528/figure/fig4/AS:667674447204357@1536197401651/The-NLP-pipeline-used-at-the-basis-of-the-methods-features-compared-to-the-number-of.png" target="_blank">
    <img alt="https://www.researchgate.net/profile/Kim-Schouten/publication/318138528/figure/fig4/AS:667674447204357@1536197401651/The-NLP-pipeline-used-at-the-basis-of-the-methods-features-compared-to-the-number-of.png" src="https://www.researchgate.net/profile/Kim-Schouten/publication/318138528/figure/fig4/AS:667674447204357@1536197401651/The-NLP-pipeline-used-at-the-basis-of-the-methods-features-compared-to-the-number-of.png" class="img">
  </a>
</div>

<style>
  .img {
    box-shadow: 1px 1px 5px #000;
  }

  .img-xs, .img-xs img {
    width: 35%;
  }

  .img-sm, .img-sm img {
    width: 50%;
  }

  .img-md, .img-md img {
    width: 70%;
  }

  .img-lg, .img-lg img {
    width: 85%
  }

  .img-xl, .img-xl img {
    width: 100%
  }

  .banner {
    width: 100%;
  }

  .banner img {
    width: 100%;
    object-fit: cover;
    height: 150px;
  }

  .link {
    text-decoration: none;
    outline: none;
    box-shadow: none !important;
  }

  .center-flex {
    display: flex;
    justify-content: center;
  }

  .white img {
    background-color: white;
  }
</style>
<h3 id="common-problems-1">Common Problems<a hidden class="anchor" aria-hidden="true" href="#common-problems-1">#</a></h3>
<ul>
<li>Ambiguity: The same sentence can have multiple meanings</li>
<li>Synonyms and Homonyms</li>
<li>Mispellings</li>
<li>Sarcasm</li>
<li>Allegory</li>
<li>Dialects</li>
</ul>
<h3 id="tokenization">Tokenization<a hidden class="anchor" aria-hidden="true" href="#tokenization">#</a></h3>
<p>A strategy to deal with complex sentences is to break it up into smaller chunks. Most often this simply by splitting the word by whitespace. You may also decide to drop any punctuation to simplify the problem at the cost of some understanding.</p>
<h3 id="text-preprocessing">Text Preprocessing<a hidden class="anchor" aria-hidden="true" href="#text-preprocessing">#</a></h3>
<p>Raw text often contains loads of unnecessary words, punctuations, grammar, and <em>word variations</em> that can make it harder to discern between words and phrases as well as being computationally expensive. To remedy this, before we enter any machine learning, we do some preprocessing on the text to help simplify our task.</p>
<p>Some common processing includes removing <em>stop word</em>, which are common words that don&rsquo;t add much meaning to a text (&ldquo;the&rdquo;, &ldquo;this&rdquo;, &ldquo;and&rdquo;). We can do lemmatization do reduce variations of words to their root (&ldquo;finally&rdquo;, &ldquo;final&rdquo;) so we can better compare them. You can find some prebuilt text preprocessors among ML libraries that do more or less depending on what you&rsquo;re looking for.</p>
<h3 id="integer-encoding-vs-word-embeddings">Integer Encoding vs Word Embeddings<a hidden class="anchor" aria-hidden="true" href="#integer-encoding-vs-word-embeddings">#</a></h3>
<p>Traditional word encoding is done by <em>one-hot-encoding</em> every word in your vocabulary. As you can imagine, vectors become huge and memory heavy as well as the fact that each word is independent and we don&rsquo;t capture meaning or context.</p>
<p>Word Embeddings instead represent each word as a multidimensional vector of floating point values. In essence, weights. The value here as that use floating point values are trainable and we can therefore train our model to capture more meaning out of each word by training those weights.</p>
<p><strong>word2vec</strong> are a family of techniques used to train word embeddings</p>
<p>When you&rsquo;re using word embeddings, you actually don&rsquo;t need to do any text preprocessing. Since we&rsquo;re embedding trainable features into each word, we can get a lot of granular meaning trained into a corpus whether they&rsquo;re stop words or punctuations. The training should do the job of placing less emphasis on less important words as well as developing meaningful weights to compare words together with. Nonetheless, this doesn&rsquo;t mean you <em>can&rsquo;t</em> do any preprocessing, but it&rsquo;s not as necessary as it is for integer encoding and it&rsquo;s effect is more nuanced.</p>
<h3 id="tf-idf--similarity">TF-IDF &amp; Similarity<a hidden class="anchor" aria-hidden="true" href="#tf-idf--similarity">#</a></h3>
<p><strong>Term Frequency</strong> is purely the frequency that a term appears in a document. It can be measured many different ways such as just the raw occurences or logarithmically \(log(1+ rawcount\)).</p>
<p><strong>Inverse Document Frequency</strong> is how common a word is among a given corpus&ndash;and then we take the inverse. We take the inverse because often we want to minimize the weight of common terms like &ldquo;the&rdquo; or &ldquo;a&rdquo; so infrequent terms can have a greater impact.</p>
<p><strong>TF-IDF</strong> puts both of these concepts together and places importances on terms that are frequently used and have rarity in a corpus.</p>
<p>When it comes to actually comparing text, a common method to use is known as <strong>Cosine Similarity</strong></p>
<h2 id="encoder-decoder-architecture">Encoder-Decoder Architecture<a hidden class="anchor" aria-hidden="true" href="#encoder-decoder-architecture">#</a></h2>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p>
<h3 id="auto-encoders">Auto Encoders<a hidden class="anchor" aria-hidden="true" href="#auto-encoders">#</a></h3>
<p>These are a special case of encoder-decoder archetecture where the input and output domains are (typically) the same. More specifically, we take out input and compress them down to a lower dimensionality (called a <em>bottleneck</em>) before reconstructing it back to its original dimensions in the output</p>
<p>For example, here we take an image and encode it down to a smaller dimension of <code>latent_size</code> before decoding it back to size.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># set random seeds to aid reproducibility</span>
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>latent_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">30</span> <span style="color:#75715e"># defines the dimensionality of the bottleneck</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rounded_accuracy</span>(y_true, y_pred):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>binary_accuracy(tf<span style="color:#f92672">.</span>round(y_true), tf<span style="color:#f92672">.</span>round(y_pred))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># define our encoder</span>
</span></span><span style="display:flex;"><span>stacked_encoder <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten(input_shape<span style="color:#f92672">=</span>[<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>]),
</span></span><span style="display:flex;"><span>    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">100</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;selu&#34;</span>),
</span></span><span style="display:flex;"><span>    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(latent_size, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;selu&#34;</span>),
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># define our decoder</span>
</span></span><span style="display:flex;"><span>stacked_decoder <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">100</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;selu&#34;</span>, input_shape<span style="color:#f92672">=</span>[latent_size]),
</span></span><span style="display:flex;"><span>    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">28</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">28</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>),
</span></span><span style="display:flex;"><span>    keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Reshape([<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>])
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># compile and train the model</span>
</span></span><span style="display:flex;"><span>stacked_ae <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential([stacked_encoder, stacked_decoder])
</span></span><span style="display:flex;"><span>stacked_ae<span style="color:#f92672">.</span>compile(
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary_crossentropy&#34;</span>,
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">=</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>), 
</span></span><span style="display:flex;"><span>    metrics<span style="color:#f92672">=</span>[rounded_accuracy]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Note, in this case, the targets are the same as the input! That&#39;s why we&#39;ve</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># passed X_train for the &#39;y_train&#39; parameter. </span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> stacked_ae<span style="color:#f92672">.</span>fit(
</span></span><span style="display:flex;"><span>    X_train, 
</span></span><span style="display:flex;"><span>    X_train, 
</span></span><span style="display:flex;"><span>    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,
</span></span><span style="display:flex;"><span>    validation_data<span style="color:#f92672">=</span>(X_valid, X_valid)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>But what&rsquo;s the point of training in a loop like this? Well after we&rsquo;ve trained the model, the bottleneck layer learns the important features of the inputs. We can then drop the decoder and use just the encoder for other classification tasks. This is the autoencoder.</p>
<p>Another application, instead of having the exact same inputs and targets, we can use this same model and use noisy images as the input and the source images as the outputs. This would be training to model on how to denoise images!</p>
<h2 id="generative-algorithms">Generative Algorithms<a hidden class="anchor" aria-hidden="true" href="#generative-algorithms">#</a></h2>
<p>First, for a refresher on <em>Naive Bayes</em>, refer back to <strong><a href="http://localhost:1313/posts/2023/old/ai/intro-to-machine-learning/#naive-bayes">here</a></strong></p>
<h3 id="variational-auto-encoders">Variational Auto Encoders<a hidden class="anchor" aria-hidden="true" href="#variational-auto-encoders">#</a></h3>
<p><a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></p>
<p>Where traditional autoencoders encodes an input to a single point, VAEs encode inputs into a probability distribution over the <strong>latent space</strong>. Specifically, we want the latent space created by the encoder to be regular enough that we can take a point within the space and decode it to generate new content. With a normal autoencoder, this latent space is typically unorganized and overfitted so some points in the space will decode into meaningless content. <strong>VAEs regularize the latent space so it has good properties for the generative process.</strong></p>
<blockquote>
<p>A latent space is a representation of compressed data. You can imagine this as the bottleneck of an autoencoder. Latent attributes are single piece of that representation.</p>
</blockquote>
<h2 id="course-retrospective">Course Retrospective<a hidden class="anchor" aria-hidden="true" href="#course-retrospective">#</a></h2>
<p>As of 8/8/2023, I have finished SCS3546 Deep Learning course from UoTSCS. Overall, I&rsquo;ve gained a much better understanding of the fundamentals of deep learning and insight into how modern AI models are built today. There&rsquo;s a lot more to the building of ML models than I expected. Logically, I knew there was a ton of incredibly complicated math and theory behind ML but I was kind of hoping a lot of that would be abstracted behind the code. To be fair, a lot of it is but there&rsquo;s a huge importance to understanding what&rsquo;s happening behind the seens to know how good models are built.</p>
<p>Another surprise was how much data processing is core to machine learning. I&rsquo;m realizing now it&rsquo;s really all about the data you use, the quality of it, and how you can format it, that creates the basis of the model you build. Furthermore, it&rsquo;s just the pure fact that machine learning is relatively new and fast growing. While there are libaries that can help you process data, it&rsquo;s rarely ever a &ldquo;one function does it all&rdquo; solution. Depending on the data and problem you&rsquo;re dealing with, it&rsquo;s on you to figure out how to get that data is a ML ready form.</p>
<p>Statistics is also a very core part or ML, particularly for gauging and comparing the quality of a model. There is a lot of probability happening within the models themselves but I find it&rsquo;s the graphs and charts where you really benefit from having a strong statistics background.</p>
<p>All this stuff I&rsquo;m honestly quite bad at and have mostly disliked in school but I think my fascination AI use cases will have me applying my new knowledge is future projects.</p>


    
    
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/ai/">Ai</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/2023/old/dev/3d-matrix-rotations/">
    <span class="title">Â« Prev</span>
    <br>
    <span>3D Matrix Rotations</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2023/old/ai/overview-of-probability/">
    <span class="title">Next Â»</span>
    <br>
    <span>Overview of Probability</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Hylu Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
