[{"content":"What is Responsive Combat Whether it\u0026rsquo;s an attack, dodge, or block, responsive combat ensures each feels immediate and with appropriate feedback. Games like Hades got this nailed down to a tee. Attacks feel punchy with minimal delay and the player rarely feels like their fighting with the controls to get the game to do what they want. By providing precise control over the character\u0026rsquo;s abilities, responsive combat enhances the sense of agency and satisfaction, leading to more intense and rewarding gameplay experiences.\nInput Queuing When inputs are timing dependent, you can run into the situation where inputs don\u0026rsquo;t register because the inherent imprecision of humans. For example, we previously talked about combo attacks where one input leads to the other. Depending on the speed of the combo there\u0026rsquo;s a certain time window between the first and second attack where the player cannot attack. Then at the exact minute picosecond that the time windows passes, the player is able to attack.\nTwo things can happen on the second attack input:\nThe player attacks late\nThe player will be able to attack but they\u0026rsquo;re attacking slower than it is possible. Ideally, if you want to attack at full speed, you need to attack on the precise frame that the first attack window ends. This is incredibly difficult and unreasonable to expect from the player.\nThe player attacks early\nEven if they\u0026rsquo;re attacking one frame too early, the attack won\u0026rsquo;t register as we\u0026rsquo;re still in the time window of the first attack. Of course, the player can just input again but it\u0026rsquo;s already disorientating to have an input not register when you expect and you\u0026rsquo;ll likely end up back in aforementioned late attack situation.\nWe can solve this be queuing inputs. This involves holding the player\u0026rsquo;s input for a short timeframe so it still registers within that window. For example, let\u0026rsquo;s say we input queue for 0.3 seconds. If we click attack 0.1s before the attack is available, then 0.1s later, the input will still be registered as we\u0026rsquo;re still running that input by the time we can attack.\nThis fixes our issues as we\u0026rsquo;re now giving the player a window of *input forgiveness where as long as they perform the input within 0.3s, they will get the full expected outcome.\nIt\u0026rsquo;s important to set the input queuing window to a reasonably low value (ex. 0.2s \u0026lt; t \u0026lt; .0.5s) otherwise inputs can start to feel too delayed. Imagine clicking an attack when it isn\u0026rsquo;t available and then having it attack 2 seconds later!\nLet\u0026rsquo;s implement general input queuing!\nThe simplest version of input queuing involves repeating an input until it succeeds. I\u0026rsquo;m using the new input system which typically involves hooking a callback function to the input event like this.\nPlayerInputActions _inputActions; _inputActions.Player.Attack.performed += Attack; Instead of hooking the Attack function directly. Let\u0026rsquo;s create a middleman function that calls Attack and queues it.\nusing InputContext = UnityEngine.InputSystem.InputAction.CallbackContext; // For brevity Action\u0026lt;InputContext\u0026gt; attackQueuedAction = e =\u0026gt; QueueInput(_weaponController.Attack, e); private void QueueInput(Func\u0026lt;InputContext, bool\u0026gt; inputCallback, InputContext e) { // Queue the function } Technically we could just assign the lambda function directly to _inputActions.Player.Attack.performed to save lines. The reason we create a new Action\u0026lt;InputContext\u0026gt; is so we have a reference for later checks and if we want to remove the callback from the event.\nRemember the goal here is to keep trying the input for a short interval of time until it either succeeds or the time passes. To do this, we\u0026rsquo;ll decide an inputQueueDelay to repeat an input for a specified time and repeat the input using a coroutine.\npublic float inputQueueDelay = .3f; Action\u0026lt;InputContext\u0026gt; queuedAction; private void QueueInput(Func\u0026lt;InputContext, bool\u0026gt; inputCallback, InputContext e) { StartCoroutine(QueueInputCoroutine(inputCallback, e)); } IEnumerator QueueInputCoroutine(Func\u0026lt;InputContext, bool\u0026gt; inputCallback, InputContext e) { float timer = 0; while (timer \u0026lt; inputQueueDelay) { inputCallback(e) timer += Time.deltaTime; yield return null; } } We now have some basic input queuing setup. There\u0026rsquo;s a still a couple more things to do. If we happen to queue inputs too quickly, we want to refresh the previous input queue rather than having two run simultaneously. Additionally, if the input succeeds, the queuing should stop otherwise we may get repeat inputs.\nFirst thing we need is a way to track what inputs are currently being queued. Here I\u0026rsquo;m using Unity\u0026rsquo;s new input system, so I\u0026rsquo;ll create a dictionary to keep track of what inputs we\u0026rsquo;re currently queuing.\nprivate Dictionary\u0026lt;Func\u0026lt;InputContext, bool\u0026gt;, InputContext\u0026gt; QueuedInputMap = new Dictionary\u0026lt;Func\u0026lt;InputContext, bool\u0026gt;, InputContext\u0026gt;(); Next, we\u0026rsquo;ll add an input to the map whenever we start a new queue and replace it if it already exists. We also check if the method returns true to exist out early from the coroutine.\nprivate void QueueInput(Func\u0026lt;InputContext, bool\u0026gt; inputCallback, InputContext e) { if (!QueuedInputMap.ContainsKey(inputCallback)) { StartCoroutine(QueueInputCoroutine(inputCallback, e)); } else { QueuedInputMap.Remove(inputCallback); StartCoroutine(QueueInputCoroutine(inputCallback, e)); } } IEnumerator QueueInputCoroutine(Func\u0026lt;InputContext, bool\u0026gt; inputCallback, InputContext e) { QueuedInputMap.Add(inputCallback, e); float timer = 0; while (timer \u0026lt; inputQueueDelay) { if (!QueuedInputMap.ContainsKey(inputCallback)) StopCoroutine(); if (inputCallback(e)) { StopCoroutine(); } else { timer += Time.deltaTime; yield return null; } } QueuedInputMap.Remove(inputCallback); } Attack Interrupts For systems like comboing, inputs lead to some running animation that must play and will likely lock out the player from inputs. Locking out the player for some time is necessary otherwise the player would constantly skip animations and be able to sequence attacks instantaneously.\nHowever, it\u0026rsquo;s usually a bad idea to lock players in for the entirety of an animation. For example, a standard attack animation typically has the attack itself followed by some sort of follow through. This is just the animation of the player returning back to their idle state. If we force the player to wait for the whole animation, every single attack will not only take too long but also feel awkwardly seperate. Imagine taking out your sword to attack 3-times but needing to sheathe your sword between every attack.\nWe could just chop off the end of the animation but you likely want to have an ending animation for single attacks or last attacks in a sequence.\nThe solution is to give a window for the player to perform another action while the current action is running, effectively interrupting the current animation. In code, we can expose two function for checking whether we\u0026rsquo;re able to interrupt and a function for actually interrupting an attack.\npublic bool IsInterruptable() { // Check if the attack is interruptable } public bool InterruptAttack() { if (IsInterruptable()) { // Interrupt attack e.g. cancel animation, reset combo, etc. } } Attack States We talked about melee attacks having different states with some states being interruptable. Let\u0026rsquo;s codify all the states of a melee attack.\npublic enum AttackState { WIND_UP, ACTIVE, WIND_DOWN, // This state should be interruptable INACTIVE } Depending on your game, you may have more states than this like sheathing/unsheathing a weapon or maybe a charge state. This may change the later code examples but the structure should largely work.\nNow that we have states, we can assign one of these states to a variable so we know where in the attack animation we are. We can do this in multiple ways such as adding an event using the animator to or using a timer to wait for the correct intervals. I\u0026rsquo;ll give an example of what it may look like using a Coroutine for an attack.\npublic class AttackMove { public AnimationClip clip; public float animationOffset; // When the attack starts in the animation public float duration = 0.5f; // How long the attacks lasts public Vector2 direction; public float damageMultiplier = 1.0f; public float rangeMultiplier = 1.0f; public float knockbackMultiplier = 1.0f; } private IEnumerator PerformAttack(AttackMove move) { currentAttackState = AttackState.WIND_UP; SetupAttackAnimation(move); // Start attack _animator.SetTrigger(attackStartTriggerHash); yield return new WaitForSeconds(move.animationOffset); currentAttackState = AttackState.ACTIVE; yield return new WaitForSeconds(move.duration); currentAttackState = AttackState.WIND_DOWN; yield return new WaitForSeconds(move.clip.length - (move.animationOffset + move.duration)); currentAttackState = AttackState.INACTIVE; } Now that we have the states currently assigned, all we have to is to only allow an attack when we deem the current attack state to be interruptable.\npublic bool IsInterruptable() { return currentAttackState == AttackState.INACTIVE || currentAttackState == AttackState.WIND_DOWN; } public bool InterruptAttack() { if (IsInterruptable()) { if (_animator.GetCurrentAnimatorStateInfo(0).shortNameHash == attackStateHash) { _animator.CrossFade(baseStateHash, 0.0f); } return true; } return false; } public bool Attack() { if (InterruptAttack()) { StartCoroutine(PerformAttack(CurrentWeapon.attackMoves[_attackMoveIndex])); return true; } return false; } The beauty of this system is you can freely alter when we can interrupt out of attacks for any moves. For example, in our game we have a dash move that original could only interrupt attacks during the WIND_DOWN state since that\u0026rsquo;s how it worked for attacks. It turned out this felt clunky and we wanted it to be interrupted even earlier. All we had to do was write another IsInterrupted function for dashes that allows more states to return true and use that when checking for whether we can dash.\npublic bool IsDashInterruptable() { return currentAttackState != AttackState.WIND_UP; } Stylized Melee Hitboxes I\u0026rsquo;ve been continuing work on Slime Hunter which uses a top-down perspective for combat. One of the challenges with melee combat is how to deal with hitboxes for the player\u0026rsquo;s attack. For realistic games, it\u0026rsquo;s often enough to just attach a collider to the player\u0026rsquo;s weapon. The problem comes with stylized combat that focuses on weapon trails to indicate hitboxes.\nHere\u0026rsquo;s an example from the amazing Hades 2:\nInstead of the model of the blade impacting with the enemy, the hitbox is determined by the trail left by the weapon. There\u0026rsquo;s a few qualities to note.\nIt appears instantly The weapon animation skips between the wind up and wind down Its size is irrespective to the weapon It changes between combos The classic collider approach won\u0026rsquo;t work as the animation doesn\u0026rsquo;t have enough frames for the collider to cover the entire curve. Additionally, we want the collider to cover an instaneous area for the whole attack rather then following a rotation.\nApproach #1: Convex Meshes Approach #2: Repeated Box Curve This approach is simlilar to the convex mesh in implementation but instead of building the curve using points, we use a set of box colliders to build up the curve. We can use more boxes to get closer to the ideal curve shape.\nThe Benefits\nWe get an overal hitbox that can get pretty close to the curve The Drawbacks\nUsing multiple box colliders is pretty inefficient Approach #3: Sphere Angle Cutoff This approach takes advantage of the fact that sphere colliders are quick and already perform most of the collision calculations that we need. Whenever we get a collision within the sphere all we need to check is that the collision happens within an angle range and (if we want) we can check the collision happens in a height range as well.\n","permalink":"http://localhost:1313/posts/2024/06/responsive-melee-combat/","summary":"What is Responsive Combat Whether it\u0026rsquo;s an attack, dodge, or block, responsive combat ensures each feels immediate and with appropriate feedback. Games like Hades got this nailed down to a tee. Attacks feel punchy with minimal delay and the player rarely feels like their fighting with the controls to get the game to do what they want. By providing precise control over the character\u0026rsquo;s abilities, responsive combat enhances the sense of agency and satisfaction, leading to more intense and rewarding gameplay experiences.","title":"Responsive Melee Combat"},{"content":"Toon Shading We\u0026rsquo;ll be using a numbers of methods from Unity\u0026rsquo;s URP package by importing Lighting.hlsl. That file has some other inclusions that we will also make direct use of. In all, we\u0026rsquo;ll be using the following files.\nLighting.hlsl RealTimeLights.hlsl Shadows.hlsl SpaceTransforms.hlsl Lighting.hlsl RealTimeLights.hlsl. In order to ensure our reacts to light in real-time, we need to hook on to some of Unity\u0026rsquo;s lighting methods to acquire the lights in the scene as well as the shadow maps.\nhttps://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/use-built-in-shader-methods-shadows.html\nPhong reflection\nDepthTexture don\u0026rsquo;t work with SSAO enabled in URP https://forum.unity.com/threads/depth-texture-doesnt-work-correctly-without-ssao.1555910/\nShaderTagId https://forum.unity.com/threads/srp-relationship-between-drawrenderers-and-drawsettingss-shadertagid.674014/\n","permalink":"http://localhost:1313/posts/2024/05/real-time-toon-shading/","summary":"Toon Shading We\u0026rsquo;ll be using a numbers of methods from Unity\u0026rsquo;s URP package by importing Lighting.hlsl. That file has some other inclusions that we will also make direct use of. In all, we\u0026rsquo;ll be using the following files.\nLighting.hlsl RealTimeLights.hlsl Shadows.hlsl SpaceTransforms.hlsl Lighting.hlsl RealTimeLights.hlsl. In order to ensure our reacts to light in real-time, we need to hook on to some of Unity\u0026rsquo;s lighting methods to acquire the lights in the scene as well as the shadow maps.","title":"Real Time Toon Shading in Unity"},{"content":"I\u0026rsquo;m documenting some of my adventures looking into the Scriptable Render Pipelines in Unity. I have the goal of creating a stylized 3D pixel-art render pipeline heavily inspired by t3ssel8r.\nWhat is URP Cel-Shading Rendering To an Intermediate Texture During rendering passes, it\u0026rsquo;s common to store information by rendering to a texture instead of immediately rendering to the screen. For example, if you want to just do a pass to collect depth information, you can draw that information in memory and hold on to it to reference for later usage. You probably wouldn\u0026rsquo;t want to draw normals on the screen directly and mess with the rest of your render passes.\nIn Unity you would use RenderTexture which is simply just a buffer that you can write texture information too.\nRenderTexture Better yet you can use some higher-level abstractions like RTHandle which provides resolution management, memory pooling, and other improvements out of the box.\nRTHandle URP Lit Shaders Reference: https://www.youtube.com/watch?v=E3i2eagy_eI\nOne of the challenges of URP shaders is that we don\u0026rsquo;t have access to surface shaders. Surface shaders are designed to hook into Unity\u0026rsquo;s lighting system and add an entry point for further shader calculations on top. Unfortunately for URP, the configurable rendering approach as well as support more advanced rendering features like PBR don\u0026rsquo;t align well with the concept of surface shaders that are designed to abstract fixed rendering pipelines like BRP.\nOne solution is to use Shader Graph which Unity maintains nodes to get light information.\nFor scripts though, we\u0026rsquo;ll have to stick with HLSL shaders and do our lighting calculation manually. Luckily, we\u0026rsquo;re given access to a bunch more HLSL macros and functions that can help us perform calculations as well as get information like light positions.\nhttps://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/writing-shaders-urp-basic-unlit-structure.html https://docs.unity3d.com/Manual/SL-UnityShaderVariables.html\nhttps://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/use-built-in-shader-methods-shadows.html\nUnlit Shader Template in URP The usual template you want to start with for HLSL shaders in Unity is the Unlit Shader. The annoying part is the template has a lot of unnecessary boilerplate and old references. For example, it comes baked with LOD and Fog calculations that you often end up deleting. As well as old CG references like #include \u0026quot;UnityCG.cginc\u0026quot; using methods from it like UnityObjectToClipPos().\nUnityCG.cginc is one of a few standard helper scripts available to include. Some of these scripts like Lighting.cginc are only compatibile with BRP but the scripts can largely be useful across any pipeline. I\nURP comes with more hlsl scripts that provide comparable methods to the cginc scripts such as the one\u0026rsquo;s in SpaceTransforms.hlsl. When you start including other URP shader scripts, you\u0026rsquo;ll find they conflict with with the cginc counterparts, leading to you having to removing any references to it anyway. Some of these scripts include:\nCore.hlsl SpaceTransforms.hlsl Lighting.hlsl Input.hlsl In any URP project, you can find these files by searching inside the Packages folder\nPipeline Overview When a scene is rendered to the screen, it\u0026rsquo;s done using a series of passes that incrementally draw each part of the scene such as the geometry, the shadows, the post-processing effects, etc.\nForward Rendering By default, Unity\nG-Buffer It\u0026rsquo;s a buffer that stores per-pixel information about the geometry in the scene. The typical components stored in a G-buffer include:\nPosition: Stores the world-space position of each pixel. Normal: Stores the surface normal vector at each pixel. Albedo: Stores the base color of the surface at each pixel. Specular: Stores material properties related to surface reflection or glossiness. Depth: Stores the depth information of each pixel. This is typically used in deferred rendering so lighting calculations can all be done at once at the end using the G-buffer.\nIn forward rendering, every single pixel needs to calculate each light source which results in lot\u0026rsquo;s of recalculations of positions, normals, albedo, etc.\n","permalink":"http://localhost:1313/posts/2024/05/unity-custom-render-pipeline/","summary":"I\u0026rsquo;m documenting some of my adventures looking into the Scriptable Render Pipelines in Unity. I have the goal of creating a stylized 3D pixel-art render pipeline heavily inspired by t3ssel8r.\nWhat is URP Cel-Shading Rendering To an Intermediate Texture During rendering passes, it\u0026rsquo;s common to store information by rendering to a texture instead of immediately rendering to the screen. For example, if you want to just do a pass to collect depth information, you can draw that information in memory and hold on to it to reference for later usage.","title":"Unity Custom Passes with URP"},{"content":"This will be a short tutorial about GitHub to get you quickly up and started conceptually if you have little to no experience. I may gloss over some things if you\u0026rsquo;re already familiar.\nWhat is Git? Git is a version control system. It keeps tracks of a repository of files and records all changes made to the files. Most popular in software development for managing code bases.\nIt provides a system for multiple people to work on the same files by tracking changes, allowing easy reverting, and merging multiple changes.\nEssentially it\u0026rsquo;s like shared cloud saving i.e. OneDrive, DropBox\nBut Why Not Just Use DropBox? Working collaboratively with files can get complicated. Something like DropBox will constantly sync every change to every file for all users. I will list out some things to consider.\nWhat if multiple people change the same files? How can we track each feature as they\u0026rsquo;re finished? What if you\u0026rsquo;re not done and the files your working on are in an unfinished state What if someone elses change drastically effects your current work What if we need to go back I won\u0026rsquo;t answer these specifically but all of these problems can spell disaster if you plan on working on software using a service like DropBox. Git is designed to address these issues by allowing you to choose when and how you want to sync your local files to the remote version as well as receive changes from others.\nWhat is GitHub? It\u0026rsquo;s a popular git repository hosting service. While Git is the system for version control, by itself it doesn\u0026rsquo;t provide you cloud storage of files. GitHub is one of these providers. There\u0026rsquo;s plenty of others (GitLab, BitBucket, etc. ) and you can even host it yourself but GitHub is by far the most popular for individuals and smaller teams.\nGit Basics Repositories Repository in this context just means a place to hold files. On GitHub, when you create a repository it essentially creates an empty folder that\u0026rsquo;s available remotely and you can freely add/remove files to that folder\nCloning Repositories This creates a local copy of the folder to your computer. This is similar to downloading files except it will create a hidden .git folder that holds metadata linking your local version to the remote version and stuff.\nSyncing Changes A change includes adding, deleting, or updating files. The process for relaying any changes to the remote repository is the same.\nIn Git, the process is broken up into 3 steps simplified here.\nAdd Select the files you want to sync\nCommit Give a name to your change. This makes it easier to track the set of changes you\u0026rsquo;re making if we later want to view or revert them.\nPush Syncs all your commits to the remote repo.\nReceiving Changes This process is called pulling, it will grab all the file changes since the last time you pulled.\nGit Branches This is an extremely important concept when it comes to working collaboratively on a repository.\nWhen you\u0026rsquo;re working on a feature, you often don\u0026rsquo;t want to sync your changes with everyone elses until you have a complete package which may be the result of multiple commits. Even more, you don\u0026rsquo;t want to have other peoples work constantly messing with yours everytime you pull.\nIt would be nice if we could hold off on combining our changes with others only when we\u0026rsquo;re done\nThe solution is creating a branch. This will create a snapshot of the codebase at that state and let you work off of that. Once you\u0026rsquo;re done with your work. You can then merge the entire branch back with the master branch\nmaster branch is just a common name used for the main lane of changes in a project.\nMerge Conflicts When merging, Git tries its best to encorporate your changes with other people but often you\u0026rsquo;ll get a CONFLICT warning if you\u0026rsquo;ve changed a file someone else has as well. Git will need input from you to decide what to keep from your changes what\u0026rsquo;s on the remote.\nGitHub Desktop Git is originally designed to be done through the commandline. This can add an additional learning curve so we\u0026rsquo;re going to use GitHub Desktop that will give us a UI to perform all the needed actions.\n","permalink":"http://localhost:1313/posts/2024/05/github-basics/","summary":"This will be a short tutorial about GitHub to get you quickly up and started conceptually if you have little to no experience. I may gloss over some things if you\u0026rsquo;re already familiar.\nWhat is Git? Git is a version control system. It keeps tracks of a repository of files and records all changes made to the files. Most popular in software development for managing code bases.\nIt provides a system for multiple people to work on the same files by tracking changes, allowing easy reverting, and merging multiple changes.","title":"GitHub Basics for Designers"},{"content":"Why I\u0026rsquo;m Writing This This semester I took a role tutoring MATH14998 (Computer Math) at Sheridan College. I originally felt a little out of water given how long it\u0026rsquo;s been since last studied math. Throughout the term, I\u0026rsquo;ve learned a lot about what it takes to teach and some of my short comings and strengths. One of them being, I have a sense of what concepts are particularly difficult grasp and finding ways to explain them so that the students can actually understand the concepts rather than memorize formulas. Now my delivery of the explanations could use some work but I\u0026rsquo;ve put together some explanations that I\u0026rsquo;m quite proud of, one of them being combinatorics. And so, I\u0026rsquo;ve included a compilation of those notes here.\nWhat is Combinatorics Well it\u0026rsquo;s simply about counting and arranging things. How many groups can you make out of a classroom of 40? How many ways can you pull marbles out of bag. These are the types of questions that combinatorics tries to answer. Things can get more complicated as we introduce more complex questions and formulas but one should never forget what it is at core.\nIt\u0026rsquo;s simply about counting and arranging things. This will always be the case and often this topic seems more complicated that it needs to be because we forget this tenet.\nIn this article, I will be covering three topics in combinatorics.\nCounting Permutations Combinations Counting Of the three topics, this is arguably the most important. Permutations and combinations can both be done purely by counting. In fact, their formulas are just shorthands for counting which I will show later on. For now let\u0026rsquo;s take a look at what counting is.\nWhen you have \\(X_1\\) group of ways to do something and \\(X_2\\) group of ways to do something else. You can multiply them together to get the total number of ways to do something. For instance, two dice rolls.\n$$ X_1 \\cdot X_2 = 6 \\cdot 6 = 36 $$\nFor brevity, let\u0026rsquo;s try again with two dice where each die only has 3 sides. I\u0026rsquo;ll also list out all the possible rolls.\n$$ X_1 \\cdot X_2 = 3 \\cdot 3 = 9 $$\n{1, 1} {1, 2} {1, 3} {2, 1} {2, 2} {2, 3} {3, 1} {3, 2} {3, 3} Notice that the set include cases where both rolls are the same or are the same but difference order.\n{1, 1} {1, 2} \u0026amp; {2, 1} A more familiar to describe this would be to say the arrangements in the set\nAllow repetition Ensure order matters Permutations A review of the permutation formula.\n$$ \\frac{n!}{(n-r)!} $$\n\\(n\\) - total number of items \\(r\\) - the number of items to pick from We use this to solve problem where there\u0026rsquo;s\nNo repetition Order matters Arranging all items For example, let\u0026rsquo;s say we have a bag with 1 red, 1 blue, and 1 green marble. How many ways can we pull out 3 marbles? Using the permuation formula, our total items are 3 and the number of items we are picking out are also 3.\n$$ \\frac{3!}{(3-3)!} = 3! = 6 $$\nBut we can actually do this just by counting!\nThe difference from the previous counting is that each time we pick a marble, we remove on from the pool. So each time, we have one less way to choose. As long as we remember this, we can still just multiply.\n$$ (3)(2)(1) = 3! = 6 $$\nIt ends up being the same calculation.\nArranging subset of items Previously, we did the simplest form of a permutation where \\(n\\) = \\(r\\). Which reduces the formula to just \\(n!\\). What about if we don\u0026rsquo;t pick all the marbles?\nLet\u0026rsquo;s pick from the same bag of 1 red, 1 blue, and 1 green marble. How many ways can we pick out a marble?\n$$ \\frac{3!}{(3-1)!} = \\frac{3!}{2!} = 3 $$\nWe can still solve this by counting. All we have to do now is also count the amount of items we aren\u0026rsquo;t including and remove them from our arrangements.\n$$ \\frac{(3)(2)(1)}{(2)(1)} = \\frac{3!}{2!} = 3 $$\nPermutations is counting Ultimately, permutations is just a shorthand for more complex counting problems. We introduce factorials to take care of repetition and introduce division to remove from our selection. But still, each component just makes clever use of counting.\nTo summarize, permutations takes care of problems where:\nWe remove repetition We can choose the number of items to pick from Order still matters Combinations Notice how the combination formula is just the permutation formula with a small alteration.\n$$ \\frac{n!}{r!(n-r)!} $$\nNotice it\u0026rsquo;s nearly identical to the permutation formula except we\u0026rsquo;re moving another \\(r!\\) arrangements from our total.\nThis means combinations should give similar results to permutations. But what do we remove? We add the condition thatorder doesn\u0026rsquo;t matter. In other words, we only consider arrangements different if they have different items, not if they\u0026rsquo;re differently ordered.\nIf we were making groups in a class, something like\n{Sarah, Max, Carl} {Max, Sarag, Carl} would not be counted as two separate groups. They would be the same group.\nMathematically we divide out an additional r! because there are r! to arrange r items where order matters. So we get rid of those items by dividing.\nCombinations is permutations without order Because the combination formula is so similar to permutation, it\u0026rsquo;s now wonder that they\u0026rsquo;re results are so similar. The only difference is we don\u0026rsquo;t differentiate order.\n= There\u0026rsquo;s no repetition\nWe can choose the numbers of items to pick from Order doesn\u0026rsquo;t matter ","permalink":"http://localhost:1313/posts/2024/04/combinatorics/","summary":"Why I\u0026rsquo;m Writing This This semester I took a role tutoring MATH14998 (Computer Math) at Sheridan College. I originally felt a little out of water given how long it\u0026rsquo;s been since last studied math. Throughout the term, I\u0026rsquo;ve learned a lot about what it takes to teach and some of my short comings and strengths. One of them being, I have a sense of what concepts are particularly difficult grasp and finding ways to explain them so that the students can actually understand the concepts rather than memorize formulas.","title":"Fundamentals of Combinatorics"},{"content":"Stealth Gameplay My favourite way to play has definitely been stealth with the mixture of hacking and tech being incredibly satisfying and fits a unique power fantasy I haven\u0026rsquo;t experienced elsewhere. Being able control the battlefield by hacking things like door and explosives, or just zapping enemies on command, adds an almost sandbox like environment for the player to interact with. There\u0026rsquo;s also the satisfying choice of weaponry from silenced pistols and rifles to melee weapons\nSaying that, I have two main gripes with the stealth system that would\u0026rsquo;ve made it much more fun if it was designed differently.\nMelee Takedowns/Executions There are very few options for executions with half of them being locked behind certain weapon and abilities. The basic ones involves sneaking up behind an enemy to grabbing them and following up with either a neck snap or chokeout.\nThis is fine but the problem is that the room to grab enemies is limited and finnicky. You have to stand directly behind the enemy and have your cursor aiming at directly on the enemy. You can\u0026rsquo;t start takedowns from the fronts of the sides of enemies and even when you think you\u0026rsquo;re doing it properly, sometimes the indicator just doesn\u0026rsquo;t show up.\nCombined with the fact that even if you\u0026rsquo;re behind an enemy and are crawling closely, the enemy will quickly turn around and alert your presence. This is incredibly frustrating when you\u0026rsquo;ve carefully distracted enemies in specific positions for a clean takedown only for the grab window to fail you.\nI see no reason why they couldn\u0026rsquo;t let takedowns be done from any angle. With the dash, it would be a nice move to be able to dash infront of an enemy before they can react and take them down that way.\nMulti enemy takedowns would also be amazing. With weapons like the mantis blades, it seems incredibly limiting to not be able to execute two enemies at once from stealth.\nUltimately, melee stealth killing just lacks options when dealing with multiple enemies. A feature like chaining stealth kills without alerting enemies is sorely needed. Dying Light had a similar mechanic a think (haven\u0026rsquo;t played). You can stealth kill one enemy and time slows down letting you chain into another takedown on a nearby enemy or even a knife kill on an enemy further away.\nEnemy Detection Before enemies are alerted, I think the visual detection is quite fair giving enough leeway and challenge to stay out of sight. It is a little frustrating when it comes to hacking as the moment you hack an enemy, they seem to know exactly the person who did it. There\u0026rsquo;s no such thing as non-chalantly hacking enemies as you walk by pretending to be innocent. They will immediately be alerted and start shooting you when they see you.\nIt might seem like a balance choice at first, but it only serves to take fun away from the player while making normal gameplay not much more challenging. Missions that are meant to be challenging will have enemies by alerted by you enemies as you\u0026rsquo;re a trespasser anyways. When you just want to have fun and hack away at a low level gang, the enemies for no good reasson always know where you are.\nThere\u0026rsquo;s already a system where enemies will start tracing the source of a fact to try to find you. The addition of them being to identify the hacker visually just seems unnecessary and takes away from the stealth options of hacking.\nNext, once enemies are alerted, unless there a physical wall between you and the enemy, they always seem to know where you are. If you\u0026rsquo;re standing behind an enemy and they\u0026rsquo;re in alert mode, they will immediately turn around to attack you even though you\u0026rsquo;re not in visible sight. You have to wait until they go from alert to searching before standing behind them works.\n","permalink":"http://localhost:1313/posts/2024/02/01/cyberpunk-2077-thoughts/","summary":"Stealth Gameplay My favourite way to play has definitely been stealth with the mixture of hacking and tech being incredibly satisfying and fits a unique power fantasy I haven\u0026rsquo;t experienced elsewhere. Being able control the battlefield by hacking things like door and explosives, or just zapping enemies on command, adds an almost sandbox like environment for the player to interact with. There\u0026rsquo;s also the satisfying choice of weaponry from silenced pistols and rifles to melee weapons","title":"Cyberpunk 2077 Thoughts"},{"content":"Ubisoft NEXT is an annual competition run by Ubisoft that has contestants compete against a variety of disciplines within game development. From drawing art pieces, to level design, depending on your skillset you can test your abilities against others. The winner for each competition gets a 3-month internship with Ubisoft in Montreal!\nI\u0026rsquo;m entering into the programming category.\nBuilding a 3D Engine I took a look at some older submissions and noticed many of the successful projects tended towards 3D graphics. Makes sense as the technical challenges increase tenfold once you add the extra dimension. In an effort to create a successful project, I also opted to created a 3D engine.\nGoals Transform supporting interpolated movement Ease-in-out, Ease-in, etc\u0026hellip; 3D Rendering Render 3D objects based on blender obj files Vertex shading Game Structure 3D Cube that rotates with scene changes Camera aims closely at one side which holds the level Then zooms out, cube rotates, and camera zooms back in on new side Complete all sides to win Particle system Consider spline based particles? Verlet integration? Pseudo Skybox? Randomly generated levels Of course I don\u0026rsquo;t know the game topic I can start though with generating a maze And hopefully use that knowledge to apply to the game topic I relied heavily on the amazing series from One Lone Coder. It goes through everything from the matrix conversations to vertex creations, to camera controls and more.\nShape Primitives With Flexible Vertex Density Vertex Shading One of my favourite additions is the capability of vertex shading. My prior work in this blog is evident if my enjoyment of creating shader patterns using math functions so I wanted to include that in the engine. The implementation is pretty simple. Loop through all the vertexes and run some function on the position of each vertex.\n// MeshFilter.h void SetVertexShader(std::function\u0026lt;void(float3\u0026amp;)\u0026gt; shader) { vertexShader = shader; } // MeshFilter.cpp for (auto\u0026amp; tri : tris) { Triangle triTransformed = tri; if (vertexShader != nullptr) { vertexShader(triTransformed.p0); vertexShader(triTransformed.p1); vertexShader(triTransformed.p2); } } // Prefabs.cpp IMPLEMENT_PREFAB(Checker, { MeshFilter* meshFilter = entity-\u0026gt;AddComponent\u0026lt;MeshFilter\u0026gt;(); meshFilter-\u0026gt;LoadMesh(PlaneMesh(40)); meshFilter-\u0026gt;SetVertexShader([](float3\u0026amp; vertex) { float height = vertex.y; height += 0.5 * sinf(30.0f * vertex.x + 5.0f * Time::Get().Elapsed()); height += 0.5 * sinf(30.0f * vertex.z); height = 3.0f * std::floor(height); vertex.y += height; }); }) This feature is dependent on high vertex density meshes for there to be enough detail to see the math functions. This is why I put the time into the ability to generate vertex dense meshes.\nCollision Detection Particle System Challenges Projection Issues The development of the 3D rendering for the most part went quite smoothly. However, once I implemented camera controls, I noticed there was a big issue that occurs when objects are placed far off from the camera. The expectation would be that the object would go out of frame, but instead, they would warp depending on they\u0026rsquo;re distance away yet stay in view as if the camera was pointing towards them. Which it wasn\u0026rsquo;t.\nThe issue is the way I normalize my vectors after projecting them into 2D space.\nif (normal.Dot(triTransformed.p0 - parentEntity-\u0026gt;parentScene-\u0026gt;GetCamera()-\u0026gt;transform.position) \u0026lt; 0) { // World -\u0026gt; View triTransformed.ApplyMatrix(parentEntity-\u0026gt;parentScene-\u0026gt;GetCamera()-\u0026gt;GetView()); // Project 3D -\u0026gt; 2D triTransformed.ApplyMatrix(parentEntity-\u0026gt;parentScene-\u0026gt;GetCamera()-\u0026gt;GetProjection()); triTransformed.Normalize(); } The projection matrix already normalizes the points in cartesian space but the last thing we need is to adjust the coordinates based on the z position. I blindly renormalized everthing again which forced every point to stay within a radius-1 circle from the origin;\nInstead, the proper operation for projection is to divide each point by z.\ntriTransformed.p0 = triTransformed.p0 / triTransformed.p0.w; triTransformed.p1 = triTransformed.p1 / triTransformed.p1.w; triTransformed.p2 = triTransformed.p2 / triTransformed.p2.w; Competition Day The topic for this year\u0026rsquo;s NEXT was incredibly broad. \u0026ldquo;Firing projectles\u0026rdquo;.\nIn prior years, the topic was usually some old school game like Bomberman or Gravitar. This gave us a lot for room for creativity on the kind of game we wanted to make.\nDarkstar Showdown Two player turn-based shooter\nDark moon\nShips\nCones Projectiles\nCubes Affected by gravity Camera switching perspectives\nGiant sphere that separates both players\nFloating asteroids orbitting around Shooting asteroids breaks into smaller bits that can be picked up for upgrades Ship speed Repair ship health Projectile size/damage Shooting the star makes it pulse Becomes smaller (less obstacle) Everyone takes 10 damage Fuel is immediately maxed for everyone Turns After two turns, the star will automatically pulse You have 1 bullet each turn, it takes 1 turn to reload Final Menus\nRestart\nClipping\nDepth Buffering\n","permalink":"http://localhost:1313/posts/2024/01/ubisoft-next/","summary":"Ubisoft NEXT is an annual competition run by Ubisoft that has contestants compete against a variety of disciplines within game development. From drawing art pieces, to level design, depending on your skillset you can test your abilities against others. The winner for each competition gets a 3-month internship with Ubisoft in Montreal!\nI\u0026rsquo;m entering into the programming category.\nBuilding a 3D Engine I took a look at some older submissions and noticed many of the successful projects tended towards 3D graphics.","title":"Ubisoft NEXT"},{"content":"First semester finished! With all the final assignments, I completely forgot about this blog. Whoops!\nIn any case, I wanted to do a little retrospective on our final pitch assignment.\nFinal Pitch The idea of this assignment is simple, we create a game idea and we pitch to the class with a short, 5 minute, prerecorded presentation. The presentation should be informative and succinct and give the viewers a clear idea of the game.\nSlime Hunter Inspirations My pitch is about a top-down action rpg called Slime Hunter, where you go around fending off the invasion of slimes. I\u0026rsquo;ll go more into detail but I\u0026rsquo;ll start with my inspirations since I think it will give a clear example of what the game will play like.\nAmorphous+ This is one of my favourite flash games growing up. It\u0026rsquo;s so simple yet to engaging to play because of a clever mix of challenge in the form of scaling enemies and fun achievements.\nYou play a guy holding a comically large sword surrounded by an endless swarm of gloops that range from harmless to dangerous. The character follows your mouse and has one attack which is a wide horizontal swing triggered by a left click.\nGloops will spawn in from all corners of the screen and your goal is to survive as long as possible. The longer you last, the more dangerous the gloops become.\nThere\u0026rsquo;s a huge variety of gloops them can be viewed in a beastiary if you\u0026rsquo;ve encountered them at least once. The most basic gloop is just a green blob that bumps into you. More dangerous variants include the Meltie that will douse you in acid on contact, and the Void Eater that shoots a shadowy beam that will disintegrate you on contact.\nIt\u0026rsquo;s quite exciting encountering these gloops for the first time and finding out what they do.\nTo aid in your fight, you can unlock various rewards that can be used in game. Some examples include an auto aiming turret, a grenade, or the ability to sprint. These are either passive or can be activated with a right click. You can only equip one reward at any given time.\nIn order to earn rewards you need to earn a certain amount of awards which are basically achievements. Things like killing multiple gloops in one sweep or dying to your own turret. Awards requirements range from hard challenges to rather silly conditions.\nAnd that\u0026rsquo;s about it. Deceptively simple but quite addicting trying to unlock all the awards an rewards.\nPost Knight I have a prior post about this game you can read that goes deeper into the magic of this game so I\u0026rsquo;ll keep it brief here.\nThe narrative of the game involves the player travelling from city to city, battling monsters and delivering mail. As you go further out into the world. You and enemies get stronger.\nAt each city, you can buy gear from the locals as well as talk to them to build up your relationship. There\u0026rsquo;s even a pseudo dating-sim where you can give them gifts and unlock more dialogue as you build your relationship.\nThe narrative is also tied into the gameplay. The missions you take are strictly missions put up by other people whether from the official guild or from a local townsperson. Because of this, each mission is tied to person that you get to speak to after the mission is complete. You get a tidbit of narrative with these conversations that can range from one-offs, world building, or even continuous subplots.\nSlime Hunter The goal of Slime Hunter is to merge the gameplay components of Amorphous+ and the world and narrative structure of Post Knight. With the twists of slimes instead of gloops and Slime Hunters instead of Post Knights.\nOne of the biggest changes with Slime Hunter is a move from 2D to 3D. I still want to keep a more retro cartoony style like the below while dialing into the mystical fantasy aspect of the world.\n","permalink":"http://localhost:1313/posts/2023/12/slime-hunter/","summary":"First semester finished! With all the final assignments, I completely forgot about this blog. Whoops!\nIn any case, I wanted to do a little retrospective on our final pitch assignment.\nFinal Pitch The idea of this assignment is simple, we create a game idea and we pitch to the class with a short, 5 minute, prerecorded presentation. The presentation should be informative and succinct and give the viewers a clear idea of the game.","title":"Slime Hunter Pitch"},{"content":"Just some scattered notes for test review.\nObject Composition Object memory is stored contiguously. If an object has pointers, those pointers are still stored contiguously but point to wherever the data is.\nObject Ownership One simple approach is to say that whatever creates the object becomes the owner of the object Thus, it becomes responsible for deleting it Inheritance Polymorphism Zombie* bob = new Zombie(\u0026#34;Bob\u0026#34;); Zombie* sally = new ZombieSoldier(\u0026#34;Sally\u0026#34;, 100); bob-\u0026gt;attack(); // prints \u0026#34;Bob throws a punch\u0026#34; sally-\u0026gt;attack(); // prints \u0026#34;Sally throws a punch\u0026#34; // wait... // ...............?! Objects have static binding by default in cpp. For dynamic binding, methods must have the virtual keyword. static binding by default does offer better performance.\nDerived classes can also use the keyword override to ensure it\u0026rsquo;s base class overloads are virtual.\nIf a class has at least one virtual method, then it should also have a virtual destructor.\nAbstract Classes Abstract classes have at least one pure virtual method.\nvirtual void attack() = 0; Multiple Inheritance Should always be done with interfaces. All (or at least most) methods should be pure virtual with ideally no member variables.\nCopy Semantics Point p(2, 3); Point q = p; Makes a bitwise copy of p to q.\nThis is troublesome if an object contains pointers, or worse, to heap memory. Only the pointer gets copied so we will get two pointers to the same piece of data as oppose to copying the data.\nPass by Value/Reference It\u0026rsquo;s generally best to pass by reference but why is this the case?\nSaves of memory since you\u0026rsquo;re just using an alias for the same memory location. Passing by value does a bitwise copy which can be slow but also may have unintended issues. Pass by reference let\u0026rsquo;s us change the value directly. Even if we don\u0026rsquo;t want to, we can enforce const to avoid changing the original. Initialization You can use initializer lists to assign parameters before the constructor is run. Allows you to assign to const member variables as they can\u0026rsquo;t change after initialization.\n","permalink":"http://localhost:1313/posts/2023/10/c++-test-review/","summary":"Just some scattered notes for test review.\nObject Composition Object memory is stored contiguously. If an object has pointers, those pointers are still stored contiguously but point to wherever the data is.\nObject Ownership One simple approach is to say that whatever creates the object becomes the owner of the object Thus, it becomes responsible for deleting it Inheritance Polymorphism Zombie* bob = new Zombie(\u0026#34;Bob\u0026#34;); Zombie* sally = new ZombieSoldier(\u0026#34;Sally\u0026#34;, 100); bob-\u0026gt;attack(); // prints \u0026#34;Bob throws a punch\u0026#34; sally-\u0026gt;attack(); // prints \u0026#34;Sally throws a punch\u0026#34; // wait.","title":"C++ Test Review"},{"content":"I\u0026rsquo;m in the middle of working on my first mainline c++ project at school.\nWe\u0026rsquo;re tasked with making a simple vertical spaceship shooter using c++ with SDL and some pre-made assets to form the game. At the moment, I\u0026rsquo;ve set up most of the classes I will need and have a simple ship that renders onto the screen.\nNaive Movement My first iteration of ship movement does have some forward thinking but allowing ship movement to continue by holding down keys. Using SDL polling, it\u0026rsquo;s easy to want to bind movement directly to each game iteration and triggering of the SDL keydown event. The problem is, standard keyboards when held have a delay on the first press before sending keyevents on every frame. What that looks like is the shape making a quick jerk, then pause, then start gliding across the screen.\nTo avoid that jerk, I use a simple isMove boolean to toggle ship movement on key presses which removes the dependency on the keyboard for continuous key events. This movements looks like the below.\nYou probably notice the lack of diagonal movement. I could add that quickly but it would be overridden anyways by the below method.\nBetter Movement The main issue with the previous movement strategy is that it feels too static and unnatural. In real life, vehicles don\u0026rsquo;t immediately go from stationary to 100km/h. There\u0026rsquo;s acceleration and decceleration that needs to happen for objects to get up to speed.\n// player input decides the direction vector. direction is 0 with no input float pos[2] = { 0.0 f }; float direction[2] = { 0.0f }; float move[2] = { 0.0f }; float friction = .97f; // normalize vector float magnitude = (float)std::sqrt(direction[0] * direction[0] + direction[1] * direction[1]); if (magnitude \u0026gt; 0) { direction[0] = direction[0] / magnitude; direction[1] = direction[1] / magnitude; } move[0] += direction[0] * speed * deltaTime; move[1] += direction[1] * speed * deltaTime; move[0] *= friction; move[1] *= friction; pos[0] += move[0]; pos[1] += move[1]; There\u0026rsquo;s a quite a big more code now since I\u0026rsquo;m working with vectors to calculate the direction we\u0026rsquo;re moving in at any given time. This still isn\u0026rsquo;t the best implementation, especially since I don\u0026rsquo;t have a nice way to cap out the movement speed. I would consider using either some interpolating or clamping to keep a hard cap on the speed. For now I\u0026rsquo;ve just hardcoded some friction that ensure the accelerated speed doesn\u0026rsquo;t go on to infinity and also smoothly slows down when there\u0026rsquo;s no movement at all.\n","permalink":"http://localhost:1313/posts/2023/10/satisfying-2d-movement/","summary":"I\u0026rsquo;m in the middle of working on my first mainline c++ project at school.\nWe\u0026rsquo;re tasked with making a simple vertical spaceship shooter using c++ with SDL and some pre-made assets to form the game. At the moment, I\u0026rsquo;ve set up most of the classes I will need and have a simple ship that renders onto the screen.\nNaive Movement My first iteration of ship movement does have some forward thinking but allowing ship movement to continue by holding down keys.","title":"Satisfying 2D Movement"},{"content":"As I was looking for games to play during my bus rides, I realized the convenience of being able to play phone games one-handed, or just simply in portrait mode. It\u0026rsquo;s a lot more comfortable to hold, especially in tighter environments like a bus.\nThere\u0026rsquo;s lot\u0026rsquo;s of options for this but they mostly amount to puzzle games and such. Instead, I wanted something more actiony and rpg-like I could play in that format. Most action games are played in landscape as they tend to require more complex controls that aren\u0026rsquo;t condusive to the portrait play area. That\u0026rsquo;s fine and all but I just never was able to fully enjoy action games to that depth on a mobile format. Trying to fit complex inputs onto a touchscreen just isn\u0026rsquo;t that great of an experience so I tend to avoid playing them. Instead, what I want is an in-depth action, portrait-mode, mobile game with simple controls yet engaging gameplay. A big ask huh?\nPost Knight The best example I\u0026rsquo;ve come across so far is called Post Knight. It has a simple formula where you have a hero that\u0026rsquo;s training to be part of an organization known as the Post Knights, whom take on missions to deliver virtually anything. Deliveries are often through dangerous territories infested with monsters which is where the knight part comes in.\nSimple Yet Extensive Gamplay and Controls Fighting through these monsters to make deliveries the main bulk of the game. The inputs are simple. You have 3 buttons which are attack, defend, and a potion. That\u0026rsquo;s it and it\u0026rsquo;s deceptively simple. The game builds on it but letting you combo and chain moves like attacking after shielding or vice versa to go perform different moves with their own advantages and disadvantages.\nSimple controls are great for the player but on the long term can make it difficult to create engaging gameplay for action oriented games like this. I think there\u0026rsquo;s so much potential for more action mobile games to use controls like this.\nBite-sized Story and Characters Besides the gameplay, I really love how it melds story between each level. You get bits and pieces of dialogues from characters that let you learn more about the world and about the characters in it. You also get an overarching story and character development the longer you play through. It\u0026rsquo;s not a lot between each level which is nice since it doesn\u0026rsquo;t distract from the gameplay but it also gives just enough that you feel motivated and engaged in the world. You want to know what happens next. You want to talk more certain characters.\nYou have some dating-game mechanics where you can gain relationship with characters through gifting and unlocking new dialogues through that. I think it\u0026rsquo;s so effective as well as efficient because writing takes a lot of work but this way you get a lot of mileage out of just a few conversations.\nApplying To Zombies I love zombie survival games. One of my favourite classics is the original The Last Stand series. Specifically the second game has mechanics that I think works very well with the simple input format.\nThose mechanics include:\nSimple gameplay controls point and shoot reload swap weapon In between hub between levels Manage gear Dedicate hours to scavenging Manage gear of your team Manage traps Gunplay We would have to reduce the controls away from free aiming otherwise it would have to be dragging on the screen or tapping. Neither are nice to use. However, a shooting game isn\u0026rsquo;t all that enjoyable if you can\u0026rsquo;t have some aim control. It\u0026rsquo;s also not that interesting if you can\u0026rsquo;t strategically prioritize targets.\nI\u0026rsquo;m thinking something in between. Like a swipe or simple button to switch targets left and right. Then a targetting cone that shrinks the longer you stay aimed at an enemy.\nOr wait, aiming in 2D only really needs the vertical axis of aim. A short slider could also work to move aim. I saw video about implementing a better method of aiming 2D games that could be a good reference.\nDesigning a Better Aim Assist for 2D Games - t3ssel8r\nI could imagine having an upgrade stat that improves the aim assist to be tighter at headshots or such.\nIdeas!\nHub Experience The Last Stand doesn\u0026rsquo;t have a strong narrative focus other than some journal logs. I think a mobile game could take some cues from Postknight with dialogue interactions with the survivors you have. This could also go further into survivor management like managing their health and happiness as that could affect their use in levels. This would also be important as you\u0026rsquo;re sending them out on scavenges.\nOf course in addition to the narrative, I\u0026rsquo;d like to continue to capability to manage survior gear and weapons and positioning to affect gameplay. The Last Stand only allowed weapon changes but we can definitely do a lot more that. Maybe also roles! Survivors don\u0026rsquo;t always have to be just shooting. They can be doing repairing, recon, healing, and stuff. This would offer a lot more build variety and strategizing as well as personalization. I also think back to FTL where you can change your crews roles on the fly depending on what is needed.\nProgression The Last Stand\u0026rsquo;s progression involves gathering enough resources to travel to the next location and hopefully making it to a military extraction point. Maybe we could go with a single base that the player is continually improving.\nOr an inbetween. As your improving this base, you gain the ability to go further out to collect more and more resources. Eventually letting you traverse to the next location. This would let get more gameplay mileage out of a single location. As well, the location we\u0026rsquo;re in can have access to different materials resulting in differing base styles, weaponry, and traps.\nEven More Thoughts Actually, the more I think about it, this could also just fit well in PC game. We can move to a 3D game with pixel graphics and an angled top down view. Angled so we get a better visual connection with the survivors. Then now, we can just place the base encampment in the center of the world and have zombies come from all directions. This would also work better if we want work on the base building aspect. Part of me likes this implementation even more than the mobile one. This also allows that aforementioned t3ssel8r video to better shine.\n","permalink":"http://localhost:1313/posts/2023/10/portrait-mobile-games/","summary":"As I was looking for games to play during my bus rides, I realized the convenience of being able to play phone games one-handed, or just simply in portrait mode. It\u0026rsquo;s a lot more comfortable to hold, especially in tighter environments like a bus.\nThere\u0026rsquo;s lot\u0026rsquo;s of options for this but they mostly amount to puzzle games and such. Instead, I wanted something more actiony and rpg-like I could play in that format.","title":"Portrait Action Mobile Games"},{"content":"A compilation of various features and gotchas I\u0026rsquo;ve encountered while studying C++.\nInstantiating Object Member Variables When to Use Initializer Lists Intializer lists offer a secondary method of initializer member variables for a class. A question comes up of why would we use this method as opposed to initializing the variables on declaration of just in the constructor. Below are a few core purposes.\nInitialize Const Members You could initialize these on declaration but what if you we want to pass in their values as arguments to the constructor. Okay, then we can move it to the constructor. Nope, it\u0026rsquo;s a const variable so it can\u0026rsquo;t be modified.\nInitializer lists let\u0026rsquo;s us instantiate const variables as with arguments before they can\u0026rsquo;t be modified.\nclass Test { const int t; public: Test(int t):t(t) {} //Initializer list must be used int getT() { return t; } }; Call a Parent Constructor If you\u0026rsquo;re deriving from a base class, you may want to initialize those base class members with it\u0026rsquo;s constructor.\nclass A { int i; public: A(int ); }; A::A(int arg) { i = arg; cout \u0026lt;\u0026lt; \u0026#34;A\u0026#39;s Constructor called: Value of i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; } class B { A a; public: B(int ); }; B::B(int x):a(x) { //Initializer list must be used cout \u0026lt;\u0026lt; \u0026#34;B\u0026#39;s Constructor called\u0026#34;; } https://www.geeksforgeeks.org/when-do-we-use-initializer-list-in-c/\nStack and Heap Memory Declaring Data Structures on the Heap https://stackoverflow.com/questions/8036474/when-vectors-are-allocated-do-they-use-memory-on-the-heap-or-the-stack\nvector\u0026lt;Type\u0026gt; vect; will allocate the vector, i.e. the header info, on the stack, but the elements on the free store (\u0026ldquo;heap\u0026rdquo;).\nvector\u0026lt;Type\u0026gt; *vect = new vector\u0026lt;Type\u0026gt;; allocates everything on the free store (except vect pointer, which is on the stack).\nvector\u0026lt;Type*\u0026gt; vect; will allocate the vector on the stack and a bunch of pointers on the free store, but where these point is determined by how you use them (you could point element 0 to the free store and element 1 to the stack, say).\nDangling Pointers on the Heap So the idea with dangling pointers is that if you allocate a piece of memory in the heap you need a pointer to pointer to that slot of memory. That\u0026rsquo;s a given.\nint* p = new int(5); Now if you want to release that block of memory, you use the pointer to free it.\ndelete *p But there\u0026rsquo;s a problem! You now have a pointer just pointing to a random block of memory that can be used by anything. This is a dangling pointer.\nDangling Pointers on the Stack If variables go out of scope in the stack, it gets released from the stack so we don\u0026rsquo;t need to depend on delete. However we can still get the dangers of dangling pointers.\nint* DoSomething() { int x; int* p = \u0026amp;x; return p } int* a = DoSomething(); a now points to an unused piece of memory in the stack! This is dangerous cause that memory space could be taken up by something later on and we don\u0026rsquo;t want to mess with that memory.\nDifference With Uninitialized Variables An uninitialized variable is generally a bad idea\nint x;\nIf you dereference it, you could get anything depending on whatever was left there in memory. This is not as bad as the aforementioned dangling pointer though as at least that memory block is being used by x so it won\u0026rsquo;t be used later down line until x is released.\nint x; Memory Leaks If you allocate some memory on the heap, you should always have a pointer leading to it. If you lose those pointers, than you\u0026rsquo;ll have no way of accessing that memory.\nThat is, you won\u0026rsquo;t ever be able to free that memory.\nDouble Pointers and Arrays Quick look at how we can assign a double pointer to an array.\nNormally we can create a pointer array like this.\nint* arr[5]; for (int i = 0; i\u0026lt;5; i++) { arr[i] = new int(1); } If we want another pointer to point to this array we need to use a double pointer. We can also index that pointer like an array once we\u0026rsquo;ve assigned it.\nint** p = nullptr; p = arr; for (int i = 0; i\u0026lt;5; i++) { std::cout \u0026lt;\u0026lt; p[i]; } Iterators Classic Iteration The classic example of iteration is just a normal for loop for you start from an int and increment by 1 until you arrive at the end\nfor (int i = 0; i \u0026lt; 10; i++) { std::cout \u0026lt;\u0026lt; \u0026#34;Hello World!\u0026#34;; } This works well for an integer index but it gets more interesting if we want to iterate over a data structure. We want to be able to go through each index of a data structure by accessing the correct memory address.\nIn case of an array or vector, we can do so by incrementing the memory address since the array holds data in a contiguous fashion (each memory address is adjacent to the next). So we can loop using an incrementing pointer like follows.\nint[5] nums = {1, 2, 3, 4, 5}; int* ptr = nums; //stores address of the first element for (int i = 0; i \u0026lt; 5; i++) { std::cout \u0026lt;\u0026lt; *ptr; ptr++; //increments the memory address by sizeof(int) } Non-Continguous Iterating Data structures don\u0026rsquo;t always store data contiguously. Something like a list acts as a container for each value added to it. Then, links each container with a pointer to the correct memory address. Iterating this requires manually following the cookie crumbs of memory addresses until you get to the index you were looking for.\nInstead of needing to do that manually, many of these data structures offer their own iterators that handle the work needed to get to the next index of a data structure. Most data structures let you extract the iterator with the following syntax.\nstd::list\u0026lt;int\u0026gt;::iterator it; you an then use that iterator traverse the data structure as you please.\nstd::list\u0026lt;int\u0026gt; myList = {1, 2, 3, 4, 5}; // Using an iterator to traverse the list std::list\u0026lt;int\u0026gt;::iterator it; for (it = myList.begin(); it != myList.end(); ++it) { std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // Dereferencing the iterator to access the element } Iterator Random Access The list data structure only allows bi-directional iteration but doesn\u0026rsquo;t allow you to immediately access a particular index as it needs to follow the trail of memory addresses to find it.\nSomething like a vector does allow immediate indexing so you can use the iterator to increment the data pointer by index. In this case, you can also use the typical array-indexing syntax and it will automatically use the iterator for random access as needed.\nstd::vector\u0026lt;int\u0026gt; myVector = {1, 2, 3, 4, 5}; // Using a random access iterator to access elements std::vector\u0026lt;int\u0026gt;::iterator it = myVector.begin(); // Accessing elements at specific positions std::cout \u0026lt;\u0026lt; \u0026#34;Element at index 2: \u0026#34; \u0026lt;\u0026lt; *(it + 2) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Element at index 4: \u0026#34; \u0026lt;\u0026lt; *(it + 4) \u0026lt;\u0026lt; std::endl; // Using the [] operator for random access std::cout \u0026lt;\u0026lt; \u0026#34;Element at index 3: \u0026#34; \u0026lt;\u0026lt; myVector[3] \u0026lt;\u0026lt; std::endl; return 0; Using Range Based Loops Starting from C++11, range-based for loop are provided, to iterate over elements with a simpler syntax.\nstd::vector\u0026lt;int\u0026gt; numbers = {1, 2, 3, 4, 5}; // Using a range-based for loop to iterate over elements for (auto\u0026amp; num : numbers) { std::cout \u0026lt;\u0026lt; num \u0026lt;\u0026lt; std::endl; } Auto https://stackoverflow.com/questions/29859796/c-auto-vs-auto\nUse auto \u0026amp;\u0026amp; for the ability to modify and discard values of the sequence within the loop. (That is, unless the container provides a read-only view, such as std::initializer_list, in which case it will be effectively an auto const \u0026amp;.) Use auto \u0026amp; to pass as reference instead of value. Use auto const \u0026amp; for read-only access. Use auto to work with (modifiable) bytecopies. ","permalink":"http://localhost:1313/posts/2023/10/c++-a-personal-guide/","summary":"A compilation of various features and gotchas I\u0026rsquo;ve encountered while studying C++.\nInstantiating Object Member Variables When to Use Initializer Lists Intializer lists offer a secondary method of initializer member variables for a class. A question comes up of why would we use this method as opposed to initializing the variables on declaration of just in the constructor. Below are a few core purposes.\nInitialize Const Members You could initialize these on declaration but what if you we want to pass in their values as arguments to the constructor.","title":"C++ A Personal Guide"},{"content":"Not many code changes as of late but I\u0026rsquo;ve been more active on one of the packages I\u0026rsquo;m using to play sound, smplr. It\u0026rsquo;s such an amazing package for playing instrument sounds with an incredibly simple and powerful api. Kudos to danigb.\nA bunch of new changes were made recently that have added some awesome features as well as some bugs I found while using them.\nNote Scheduling Throughput The first change was a tweak to note scheduling that fixes an issue of the web audio getting overloaded and introducing lot\u0026rsquo;s of static.\nAn issue was opened a few months back that is now fixed.\ndanigb added a new queuing data structure that looks to better handle large amounts of notes as per feat/player #30. Definitely worth taking a look at in the future!\nonStart callback This was a feature I requested and is key for my project as I need to visualize keypresses as well as key releases on the piano whenever a note is played.\nweb audio nodes don\u0026rsquo;t have a built-in onStart callback so it makes sense why smplr didn\u0026rsquo;t immediately include one either.\nI used a pretty jank method of adding new web audio nodes and playing empty sounds with no duration only for them to trigger a callback when it ends. Since the duration is 0, the onEnded effectively acts as an onStart.\nlet timingNodes = []; const timingContext = new AudioContext(); function scheduleCallback(delay, callback){ const osc = timingContext.createOscillator(); osc.start(timingContext.currentTime+delay); osc.stop(timingContext.currentTime+delay); osc.onended = callback; // keep nodes in a list in case I need to stop them early timingNodes.push(osc); } But after some back and forth with some issues, danigb introduced a built in onstart as per my suggestion! I\u0026rsquo;ve replaced my code with the new callback and it works amazingly.\nOther stuff I put a pause on my work into converting the visualizer into an html canvas for two reasons.\nMy early implementation of canvas painting was actually lagging quite a big in comparison to the current HTML DOM elements. My use of shadows are really slow to draw on the canvas especially when there\u0026rsquo;s so many elements. I don\u0026rsquo;t want to lose the shadows since they look really nice. I still want to use canvas since the current implementations has some animation flickers and issues when the user is on a different tab. I\u0026rsquo;d need to rework my code a good bit since it\u0026rsquo;s not designed well for accessing piano events from a centralized place quickly Currently I have seperate component to handle ribbons for each note. A canvas would have to handle every note from within one component. I also put a start into including piano genie functionality.\n","permalink":"http://localhost:1313/posts/2023/10/pianotypes-devlog-5/","summary":"Not many code changes as of late but I\u0026rsquo;ve been more active on one of the packages I\u0026rsquo;m using to play sound, smplr. It\u0026rsquo;s such an amazing package for playing instrument sounds with an incredibly simple and powerful api. Kudos to danigb.\nA bunch of new changes were made recently that have added some awesome features as well as some bugs I found while using them.\nNote Scheduling Throughput The first change was a tweak to note scheduling that fixes an issue of the web audio getting overloaded and introducing lot\u0026rsquo;s of static.","title":"Pianotypes Devlog 5"},{"content":"Declaring Data Structures on the Heap https://stackoverflow.com/questions/8036474/when-vectors-are-allocated-do-they-use-memory-on-the-heap-or-the-stack\nvector\u0026lt;Type\u0026gt; vect; will allocate the vector, i.e. the header info, on the stack, but the elements on the free store (\u0026ldquo;heap\u0026rdquo;).\nvector\u0026lt;Type\u0026gt; *vect = new vector\u0026lt;Type\u0026gt;; allocates everything on the free store (except vect pointer, which is on the stack).\nvector\u0026lt;Type*\u0026gt; vect; will allocate the vector on the stack and a bunch of pointers on the free store, but where these point is determined by how you use them (you could point element 0 to the free store and element 1 to the stack, say).\nDangling Pointers on the Heap So the idea with dangling pointers is that if you allocate a piece of memory in the heap you need a pointer to pointer to that slot of memory. That\u0026rsquo;s a given.\nint* p = new int(5); Now if you want to release that block of memory, you use the pointer to free it.\ndelete *p But there\u0026rsquo;s a problem! You now have a pointer just pointing to a random block of memory that can be used by anything. This is a dangling pointer.\nDangling Pointers on the Stack If variables go out of scope in the stack, it gets released from the stack so we don\u0026rsquo;t need to depend on delete. However we can still get the dangers of dangling pointers.\nint* DoSomething() { int x; int* p = \u0026amp;x; return p } int* a = DoSomething(); a now points to an unused piece of memory in the stack! This is dangerous cause that memory space could be taken up by something later on and we don\u0026rsquo;t want to mess with that memory.\nDifference With Uninitialized Variables An uninitialized variable is generally a bad idea\nint x;\nIf you dereference it, you could get anything depending on whatever was left there in memory. This is not as bad as the aforementioned dangling pointer though as at least that memory block is being used by x so it won\u0026rsquo;t be used later down line until x is released.\nint x; Memory Leaks If you allocate some memory on the heap, you should always have a pointer leading to it. If you lose those pointers, than you\u0026rsquo;ll have no way of accessing that memory.\nThat is, you won\u0026rsquo;t ever be able to free that memory.\n","permalink":"http://localhost:1313/posts/2023/09/stack-and-heap/","summary":"Declaring Data Structures on the Heap https://stackoverflow.com/questions/8036474/when-vectors-are-allocated-do-they-use-memory-on-the-heap-or-the-stack\nvector\u0026lt;Type\u0026gt; vect; will allocate the vector, i.e. the header info, on the stack, but the elements on the free store (\u0026ldquo;heap\u0026rdquo;).\nvector\u0026lt;Type\u0026gt; *vect = new vector\u0026lt;Type\u0026gt;; allocates everything on the free store (except vect pointer, which is on the stack).\nvector\u0026lt;Type*\u0026gt; vect; will allocate the vector on the stack and a bunch of pointers on the free store, but where these point is determined by how you use them (you could point element 0 to the free store and element 1 to the stack, say).","title":"Stack and Heap"},{"content":"Dot Product Find the interior angle between two vectors $$ A\\cdot B = |A||B|cos\\theta $$\n$$ \\theta = \\arccos(\\dfrac{A\\cdot B}{|A||B|}) $$\n$$ where\\ A\\cdot B = A_x \\times A_y + B_x \\times B_y $$\nCross Product Get perpendicular vector from two vectors Find area created by two vectors (for 2D, parallelogram area, for 3D, parallelepiped area) This is the magnitude of perpendicular vector Matrix Multiplication There is a formula for calculating the resultant matrix from a matrix multiplication.\n$$ \\begin{bmatrix} X_{22} Y_{11} + X_{12} Y_{21} \u0026amp; X_{22} Y_{12} + X_{12} Y_{22} \\\\ X_{11} Y_{21} + X_{21} Y_{11} \u0026amp; X_{11} Y_{22} + X_{21} Y_{12} \\end{bmatrix} $$\nBut it\u0026rsquo;s much more meaningful to think of a matrix multiplication of multipling matrix A by each column of matrix B.\nSimilar how you would transform a vector by multiplying it by the matrix, you\u0026rsquo;re transforming each component of the matrix which describes the coordinate space.\n","permalink":"http://localhost:1313/posts/2023/09/useful-linear-algebra/","summary":"Dot Product Find the interior angle between two vectors $$ A\\cdot B = |A||B|cos\\theta $$\n$$ \\theta = \\arccos(\\dfrac{A\\cdot B}{|A||B|}) $$\n$$ where\\ A\\cdot B = A_x \\times A_y + B_x \\times B_y $$\nCross Product Get perpendicular vector from two vectors Find area created by two vectors (for 2D, parallelogram area, for 3D, parallelepiped area) This is the magnitude of perpendicular vector Matrix Multiplication There is a formula for calculating the resultant matrix from a matrix multiplication.","title":"Useful Linear Algebra"},{"content":"What Matters The dot product has a bunch of properties when you use it\u0026rsquo;s raw scalar output but honestly most of the times you will use the dot product is to find the angle between two vectors by rearranging to equation as follows\n$$ A\\cdot B = |A||B|cos\\theta $$\n$$ \\theta = \\arccos(\\dfrac{A\\cdot B}{|A||B|}) $$\n$$ where\\ A\\cdot B = A_x \\times A_y + B_x \\times B_y $$\nIn games, this equation is used exhaustively to calculate light projections and field of view.\nWhat Is It? A standard definition of the dot product is as follows\nThe dot product, also called scalar product, is a measure of how closely two vectors align, in terms of the directions they point. - Robert Sheldon\nA simpler version of this is just \u0026ldquo;how much are these vectors pointing in the same direction\u0026rdquo;?\nPersonally, though I find this characterization to be confusing as it really only applies with your vectors are of length 1. Still, let\u0026rsquo;s start with this intuition and work forwards\nImagine, you\u0026rsquo;re trying to figure out how similar two vectors A and B are. To start, with can stick them together tail-by-tail and try to get a gauge of how much their direction diverges.\nThe dot product serves to answer that question by saying \u0026ldquo;hey, let\u0026rsquo;s start with the vector A, if we want for it to equal B, what we can do is draw a new vector we\u0026rsquo;ll call C so that A + C = B\u0026rdquo;. Though it doesn\u0026rsquo;t stop here. If we just blindly create a vector connecting A to B, it\u0026rsquo;s too complicated to figure out how A+C is related to B. Instead, let\u0026rsquo;s take this vector C and point is perpendicular to B, like it\u0026rsquo;s casting a shadow onto B. Now that we have this right angled triangle, we can get the adjacent side of the triangle that point in the direction of B and use that length as a gauge of how similar A and B.\nIf you think about it, the further the angle these two vectors point, the smaller the adjacent side will get. As well, if the vectors are the same, the shadow casted by A onto B will just be the same vector.\nThe Whole Dot Product With that in mind, we\u0026rsquo;re left with two points of comparison. The casted shadow of A represented by \\(acos\\theta\\) and the vector B itself.\nIf we multiply them, we get the equation for the dot product\n$$ |A||B|cos\\theta $$\nBut hey wait a minute! Why are we suddenly multiplying B to the equation? Doesn\u0026rsquo;t this ruin the whole intuition of showing how closely these two vectors point in the same direction?\nThis comes back to what I said earlier where this intuition only applies with vectors of length 1, or in other words, \u0026ldquo;unit vectors\u0026rdquo;. Otherwise, we\u0026rsquo;d be better off just removing the second vector for the equation entirely.\nThere are some cases where the pure dot product value is useful such as how an angled force applies onto an object.\nMathematically, this form of the dot product is properties such as the distributive and associative properties that require the product of two vectors to be the case.\nThis article is quite messy as I need work on my understanding through application but I\u0026rsquo;ll come back to this another day.\n","permalink":"http://localhost:1313/posts/2023/09/dot-product/","summary":"What Matters The dot product has a bunch of properties when you use it\u0026rsquo;s raw scalar output but honestly most of the times you will use the dot product is to find the angle between two vectors by rearranging to equation as follows\n$$ A\\cdot B = |A||B|cos\\theta $$\n$$ \\theta = \\arccos(\\dfrac{A\\cdot B}{|A||B|}) $$\n$$ where\\ A\\cdot B = A_x \\times A_y + B_x \\times B_y $$\nIn games, this equation is used exhaustively to calculate light projections and field of view.","title":"Dot Product and How it Relates to Light"},{"content":"Two\u0026rsquo;s Complement Representation Why We Use It In c++, signed integers are represented in two\u0026rsquo;s complement notation. Before I get to how that notation works, I want to explain why we use it.\nComparing both representations, -1 would look like this\n// Decimal 4294967295 // Binary Signed Int 10000000000000000000000000000001 // Two\u0026#39;s Complement Signed Int 11111111111111111111111111111111 The normal signed binary representation is pretty easily understood if you know what a sign bit is. You just have the typical binary for 1 with the most significant bit being the sign bit to indicate that it\u0026rsquo;s negative. Then why the heck do we use two\u0026rsquo;s complement instead? Isn\u0026rsquo;t it just more confusing?\nThere are a few drawbacks to the normal method.\nNotice we have two ways to represent 0 (0000 vs 1000) so we waste a bitset Often, the sign bit can end up outside our initial bit range after an addition Addition between signed ints doesn\u0026rsquo;t work In this scenario we need to increase our bits to get the sign but we incorrectly lose the bit that should be in the 4\u0026rsquo;s column. $$ 5 + (-5) = 0 $$\n$$ 0101 + 1101 = 10010 $$\nHow it Works Ben Eater has a fantastic explanation below\nWe can start by adding the one\u0026rsquo;s complement which involves just flipping all bits (excluding the sign bit). If we try adding both of these binaries together, we get much closer to the answer but you\u0026rsquo;ll find that your result is off by 1.\n$$ 5 + (-3) $$\n$$ 0101 + 1100 = 1 $$\nWhat we can do is simply add 1 to our flipped bits and this solves our problem. The result we get is precisely the two\u0026rsquo;s complement\nThere\u0026rsquo;s actually a nice mathematical meaning if we analyze what the two\u0026rsquo;s complement is really saying.\nTake the two\u0026rsquo;s complement of 5\n$$ 1011 $$\nWhat the most significant bit is really representing in this form is -8, and that\u0026rsquo;s being subtracted from lesser bits which add up to 3, therefore giving a bit representation of -5!\nTherefore, most languages will use two\u0026rsquo;s complement for signed integers under the hood. In particular, for negative numbers, the compiler will know that when they encounter the negative sign bit how to convert to following bits to produce the correct numerical result.\nNegating a Number to Two\u0026rsquo;s Complement To summarize, let\u0026rsquo;s negate the number 5\n$$ 0101 $$\nConvert to one\u0026rsquo;s complement by flipping bits $$ 1010 $$\nConvert to two\u0026rsquo;s complement by adding 1 $$ 1011 $$\nSigned Int Added to Unsigned Int unsigned int un = 2; int n = -3; PrintUnsignedBits(un + n); PrintSignedBits(-2147483647); \u0026gt; Unsigned Int (un + n) Decimal Representation = 4294967295 Bit Representation = 11111111111111111111111111111111 Signed Int (-3) Decimal Representation = -2147483647 Bit Representation = 10000000000000000000000000000001 The unsigned bit takes precedence resulting in the negative sign bit from the -3 being the most significant postive bit in the result.\n","permalink":"http://localhost:1313/posts/2023/09/signed-ints/","summary":"Two\u0026rsquo;s Complement Representation Why We Use It In c++, signed integers are represented in two\u0026rsquo;s complement notation. Before I get to how that notation works, I want to explain why we use it.\nComparing both representations, -1 would look like this\n// Decimal 4294967295 // Binary Signed Int 10000000000000000000000000000001 // Two\u0026#39;s Complement Signed Int 11111111111111111111111111111111 The normal signed binary representation is pretty easily understood if you know what a sign bit is.","title":"Signed Ints and Two's Complement"},{"content":"I\u0026rsquo;m on my second week and to my surprise, all my anxieties and insecurities subsided really quickly. It had been while since I\u0026rsquo;ve had a proper schedule or talk to people in person on a frequent basis but I\u0026rsquo;m glad at how smoothly I\u0026rsquo;ve been able to reacclimate and I\u0026rsquo;ve been feeling a lot happier as a result.\nAnyways so far, it\u0026rsquo;s just been a lot of intro talk and fundamentals of concepts I\u0026rsquo;ve already learned before. There are some teasers of new and interesting content up to come like the graphics pipeline but we\u0026rsquo;ve yet to wok directly with it.\nFirst Assignment Homework wise, we\u0026rsquo;ve just been given one assignment which was to create a blackjack game using whatever prior knowledge and language we had. I actually quite love this as a first assignment. It feels empowering to use what I already know to build games despite my limited experience in game building. As I\u0026rsquo;ve been primarily web programming, I naturally tended to creating a webapp with HTML/JS. People make fun of the language and rightfully so, but I love how easy to is to create interactive applications and how easy to is to get up on a live server for others to try.\nIn my case, I\u0026rsquo;ve just pushed on to a repository and hosted it on Github Pages.\nGive it a try here https://hylu-dev.github.io/blackjack/.\nPlanning Since the start of this semester, I\u0026rsquo;ve return to notebook task scheduling and trying to keep up a consistent flow things I\u0026rsquo;d like to accomplish whether it\u0026rsquo;s school assignments or personal skills like drawing. I\u0026rsquo;ve already been skipping out some days on the drawing\u0026ndash;it\u0026rsquo;s still just hard to get into drawing practice I feel but at least the scheduling is making me feel accountable on it.\nAnother thing I\u0026rsquo;ve been keeping up is the dread Leet coding. I HATE LEET CODING\u0026hellip; But, it\u0026rsquo;s unfortunately just so necessary for technical interviews so it\u0026rsquo;s best I get used to it now. I\u0026rsquo;ve so far done a good job keeping up daily problems and I\u0026rsquo;m feeling more comfortable just booting one up to work on. Here\u0026rsquo;s to keeping my resolve!\nThis is just the start for me so I got a lot work to do. To whoever reading, I hope you the best in your endeavors and to keep a smile on throughout your journey c:\n","permalink":"http://localhost:1313/posts/2023/09/pgdap-log-2/","summary":"I\u0026rsquo;m on my second week and to my surprise, all my anxieties and insecurities subsided really quickly. It had been while since I\u0026rsquo;ve had a proper schedule or talk to people in person on a frequent basis but I\u0026rsquo;m glad at how smoothly I\u0026rsquo;ve been able to reacclimate and I\u0026rsquo;ve been feeling a lot happier as a result.\nAnyways so far, it\u0026rsquo;s just been a lot of intro talk and fundamentals of concepts I\u0026rsquo;ve already learned before.","title":"PGDAP Log 2"},{"content":"Today is my first official day of class attending the Advanced Programming program at Sheridan College.\nTo commemorate this, I\u0026rsquo;ve reorganized my whole directory structure for this blog to be organized by year and month rather than sub topics. I\u0026rsquo;ve been finding the subtopic folders to be growing far to fast (particularly the dev folder) to keep posting all in one place.\nThis way, my working directory is ever only so large for each month and I\u0026rsquo;m also relieved of the duty of figuring out which folder I should be making my post in. I mentioned before how I feel too inclined to make each blog post substantial and with purpose rather than just posting freely. I think uncategorizing each post will make it easier to just post updates and devlogs as well.\nUnrelated, I made a quick update to my tabby extension cause it turned out to have some pretty glaring bugs my friend notified me about.\nThe grouping grabbed the tabs from very window rather than the current working window Group labels were often assigning to the wrong group of tabs The first issue was an easy fix as it I just needed to add an extra parameter to my tab query to only check for the current window.\nchrome.tabs.query({ currentWindow: true}, tabs =\u0026gt; {}); The second issue was a failure of an assumption I made that when I use the tab grouping api, it will place that group in the location of the relevant tabs and I used that index to assign the correct label. In actuality the order that order that groups end up in are a bit more haphazard than that. Or\u0026hellip; something else might be happening but I was too lazy to figure out so I\u0026rsquo;ll just go with that.\nInstead I just made a new query into the tabs of the group and retrieved the domain again from there.\n","permalink":"http://localhost:1313/posts/2023/09/starting-the-school-year/","summary":"Today is my first official day of class attending the Advanced Programming program at Sheridan College.\nTo commemorate this, I\u0026rsquo;ve reorganized my whole directory structure for this blog to be organized by year and month rather than sub topics. I\u0026rsquo;ve been finding the subtopic folders to be growing far to fast (particularly the dev folder) to keep posting all in one place.\nThis way, my working directory is ever only so large for each month and I\u0026rsquo;m also relieved of the duty of figuring out which folder I should be making my post in.","title":"Starting the School Year"},{"content":"I finally put a start into replacing the current ribbon visualizer implementation with an HTML canvas drawn one.\nWhy Bother? To reiterate, the current visualizers works by filling the screen with vertical divs that are positioned identically to the piano keys so each key essentially has a identically width column above it. I then spawn divs inside each lane that act as each of the ribbons.\nThe extending animation works by settings each ribbon to the max size of the lane and running a scaleY transition from 0 to 1 to mimic the ribbon growing from nothing. Once the ribbon is released, I then query the DOM object for it\u0026rsquo;s current length at that moment of the transition and then cancel the rest of the transition and manually set the div to that size. I also add a new transition that translates the ribbon from the bottom of the view until it\u0026rsquo;s offscreen where it\u0026rsquo;s then removed from the DOM.\nIt definitely works but there\u0026rsquo;s two main problems that lead to my desire for replacing with an HTML canvas.\nIt\u0026rsquo;s rather cumbersome to spawn that many DOM objects that are all styled and animating at once. I haven\u0026rsquo;t personally run into any lag issues in my testing but from a design perspective, it\u0026rsquo;s really inefficient. The current scaling transition doesn\u0026rsquo;t play well with round borders as it tries to scale those from 0 as well. This leads to a slight flicker once I end the transition and force the ribbons final size as the rounded borders snap to their final size. Another smaller reason is that I\u0026rsquo;d like to add more effects like particles which is better done with canvas anyways.\nCurrent Progress For now, I\u0026rsquo;ve just got the canvas setup and played around with creating ribbon blocks in it and animating the extension and translations of them. So far it looks great.\nMy next task is to figure out how I want to feed the key press events to the visualizer. Previously, I had a separate component for each note of the key that would just listen to keypresses for their individually note. Now, it\u0026rsquo;s just one component that needs to keep track of all notes being pressed. Additionally, since I can\u0026rsquo;t depend on the flexbox placements for the ribbon lanes, I need to notify the position of the key being pressed as well as the type of key so the canvas knows where and how to draw the ribbon.\nThough, I\u0026rsquo;m not using TypeScript, I mocked up an object type to describe the type of info the canvas will need to hold for all of the current ribbons it needs to draw.\ntype Ribbon { x: number; y: number; width: number; height: number; released: boolean; } I\u0026rsquo;m really conflicted now for how I should implement this. Even more, I\u0026rsquo;m wondering if I should rework how key press events are being handled as a whole\u0026hellip;\nLike maybe I should just keep a single store object that tracks the most recent key event like\nexport const keyPressEvent = writable({}); And just have the piano listen to whenever this variable changes and react to it rather than what it\u0026rsquo;s doing now which is listening to a dictionary that holds the current state of every key\u0026hellip;\n","permalink":"http://localhost:1313/posts/2023/old/dev/pianotypes-devlog-5/","summary":"I finally put a start into replacing the current ribbon visualizer implementation with an HTML canvas drawn one.\nWhy Bother? To reiterate, the current visualizers works by filling the screen with vertical divs that are positioned identically to the piano keys so each key essentially has a identically width column above it. I then spawn divs inside each lane that act as each of the ribbons.\nThe extending animation works by settings each ribbon to the max size of the lane and running a scaleY transition from 0 to 1 to mimic the ribbon growing from nothing.","title":"Pianotypes Devlog 5"},{"content":"Hey, yep I\u0026rsquo;m back already with more mental pain.\nI\u0026rsquo;ve been working on putting together a layered parallax scene as per https://www.youtube.com/watch?v=XaiYKkxvrFM.\nI\u0026rsquo;m a good ways there writing most of the code on my own. I\u0026rsquo;ve made series of repeating trees across a sloping hill and it\u0026rsquo;s time for me to start layering them on top with a for loop.\nIn the tutorial, he wrote all of the shape functions to return vec4 so they include the alpha channel in addition to the colours.\nIn my code, I wanted to keep the shape functions simple so I left them as returning a float of 1 for pixels in the shape and zero elsewhere (plus blur).\nI first started off by adding the layers on top of eachother directly but I realized that that wasn\u0026rsquo;t going to work because despite the fact I was hardcoding 1.0 into the alpha channel, the layers in front were still going to mix with the colours behind it.\nIf I want to make the layers in the front darker, then when I add the layer on top of a light layer, it\u0026rsquo;s just going to inherit the colours below it. This is not the way.\nInstead, if I want to add a layer and replace the colour of whatever is underneath, then the best option turns out to be mix().\nTypically, you would use mix() to smoothly interpolate between two values based on a value x between 1 and 0. Since we just want to pick one of the two colours, we can mix the colour with the canvas and use 1 in areas of the layer to replace what\u0026rsquo;s under and use 0 for areas outside the layer keep what was on the canvas the same.\nfor (float i=0.; i\u0026lt;1.; i+=1./10.) { float scale = mix(30., 1., i); float layer = float(layer(uv*scale+vec2(t+i*100., i)-m)); float alpha = layer; layer *= (1.-i); // darkens front layers color = mix(color, vec4(layer), alpha); } ","permalink":"http://localhost:1313/posts/2023/old/dev/layering-colours/","summary":"Hey, yep I\u0026rsquo;m back already with more mental pain.\nI\u0026rsquo;ve been working on putting together a layered parallax scene as per https://www.youtube.com/watch?v=XaiYKkxvrFM.\nI\u0026rsquo;m a good ways there writing most of the code on my own. I\u0026rsquo;ve made series of repeating trees across a sloping hill and it\u0026rsquo;s time for me to start layering them on top with a for loop.\nIn the tutorial, he wrote all of the shape functions to return vec4 so they include the alpha channel in addition to the colours.","title":"Layering Colours"},{"content":"While I was working on writing simple shape patterns in GLSL, I ended up with an interesting conundrum.\nI fell into the pattern of creating shapes by closing in each side of the shape with a smoothstep in the following form.\nsmoothstep(-blur, +blur, uv.x); In this case, this creates a vertical edge where the left side is 0 and the right side is 1. I then do the same thing for the rest of the side of the shape, until I enclose it so that all values inside the shape are 1 and everywhere else remains 0. For example, a square.\nfloat square(vec2 uv, float width, float height) { float blur = .005; float shape = smoothstep(-blur, +blur, uv.x); shape *= 1.-smoothstep(-blur, +blur, uv.x-width); shape *= smoothstep(-blur, +blur, uv.y); shape *= 1.-smoothstep(-blur, +blur, uv.y-height); return shape; } UV Flipping Problem This works great and the syntax is clean. The only issue is that typically, shape translation are done through adding or subtracting from the uv directly. But with this syntax, the effect is reversed.\nFor example, let\u0026rsquo;s say I wanted to draw a square, shifted 0.5 up and to the right. I\u0026rsquo;d intuitively draw one like this.\nsquare(uv + vec2(0.5, 0.5), .2, .2); The problem is that this actually shifts it down and to the left, the opposite way! The issue with binding the interpolated value with the uv, is that the edge moves in the opposite directions that the uv are moved. Of course, you can just subtract instead for a slightly more meaningful signs.\nsquare(uv - vec2(0.5, 0.5), .2, .2); But intuitively, you\u0026rsquo;d think adding a to the uv would shift the coordinates positively and therefore translate anything drawn on it positively. This isn\u0026rsquo;t a functional problem but something about it not following expectations makes me want to find another solution.\nAn Alternative If I want to achieve the shape without flipping the UV\u0026rsquo;s, then you could do away with influencing the UV and just add a position argument like this\nfloat square(vec2 uv, vec2 pos, float width, float height) { vec2 s = step(pos, uv); float blur = 0.05; s = smoothstep(pos, pos+blur, uv); s *= smoothstep(uv-width, uv-width+blur, pos); return s.x*s.y; } And then now you can sub in the position vector instead.\nFrankly, this even worse lol. It\u0026rsquo;s adds unnecessary verbosity with the additional argument just to solve a trivial issue.\nRethinking the first solution Okay after all that, maybe it\u0026rsquo;s worth rethinking the intuition of the original solution.\nfloat shape = smoothstep(-blur, +blur, uv.x); So this line makes use of smoothstep to create an edge of white and black. Remember that smoothstep has you choose an upper and lower edge and then a value to interpolate between the two. In our case, it\u0026rsquo;s really just a single edge since we just want to blur. So then the edge will occur at the time that uv.x = blur. If we subtract from the uv, then that means we need a higher uv.x value before we hit that edge, effectively moving the edge to the right. Remember, we\u0026rsquo;re not subtracting the edge, that remains the same, we are subtracting the interpolater.\nThinking about it this way is much nicer but I can\u0026rsquo;t deny I\u0026rsquo;m a wee bit unsatisfied. I\u0026rsquo;d still like the semantic understanding to be that we move the coordinates axes and the shape follows exactly but I guess that just won\u0026rsquo;t be the case if we use the uv as the interpolator.\nIn my mind, I thought the shapes were being bound to the uv but they\u0026rsquo;re really bound to the axes and the uv is just a the guy trying to find where 0 is and we\u0026rsquo;re just messing with him.\nThis is very rambly but I\u0026rsquo;ve now accepted the reality of manipulating uvs, time to back to coding.\n","permalink":"http://localhost:1313/posts/2023/old/dev/which-way-does-the-uv-go/","summary":"While I was working on writing simple shape patterns in GLSL, I ended up with an interesting conundrum.\nI fell into the pattern of creating shapes by closing in each side of the shape with a smoothstep in the following form.\nsmoothstep(-blur, +blur, uv.x); In this case, this creates a vertical edge where the left side is 0 and the right side is 1. I then do the same thing for the rest of the side of the shape, until I enclose it so that all values inside the shape are 1 and everywhere else remains 0.","title":"Which Way Does the UV Go?"},{"content":"Not too much to report.\nI had the realization that I can pause the scheduled notes by suspending the AudioContext since they depend on the context\u0026rsquo;s timer. It\u0026rsquo;d be pretty easy to add pause functionality. The only caveat is that if I pause the AudioContext, than all sound from the piano is also paused. Perhaps I could code in a notice to resume the timer given the user inputs a key press but I don\u0026rsquo;t want to resume the context on every key press. Maybe I could write some code to resume on the first user input on pause.\nAll in all, this is starting to seem a bit messy having to handle the AudioContext in weird ways in relation to the piano. Perhaps it\u0026rsquo;d be worth to consider separating out the soundfont player from the piano store so I can better treat them individually. I\u0026rsquo;m not really sure but pause functionality would be nice.\nApart from that, my main concern is the current freezing during generation which this operative solution would be to run it as a background worker. Not sure though how I\u0026rsquo;d be able to delegate the uploaded file to a background file since it runs separately. Maybe there\u0026rsquo;s a way to pass items between the two contexts I\u0026rsquo;d have to look into.\n","permalink":"http://localhost:1313/posts/2023/old/dev/pianotypes-devlog-4/","summary":"Not too much to report.\nI had the realization that I can pause the scheduled notes by suspending the AudioContext since they depend on the context\u0026rsquo;s timer. It\u0026rsquo;d be pretty easy to add pause functionality. The only caveat is that if I pause the AudioContext, than all sound from the piano is also paused. Perhaps I could code in a notice to resume the timer given the user inputs a key press but I don\u0026rsquo;t want to resume the context on every key press.","title":"Pianotypes Devlog 4"},{"content":"A couple years ago I made a little extension for chrome to organize and sort tabs by url domains. It was really quick and crudely designed and since I wasn\u0026rsquo;t even a Chrome user, I didn\u0026rsquo;t think too much about improving it. Fast forward to now, I decided to do exactly that.\nLooking at old code Actually the code was mostly better than I expected, other than some variable naming and redundant lines, I didn\u0026rsquo;t see much need to rewrite it. The html on the other hand needed some tuning and was pretty messily put together. There was lot\u0026rsquo;s of redundant styling and a disappaering timeout on the buttons that I wonder why I even thought was a good idea in the first place.\nNew features I didn\u0026rsquo;t know initially what I wanted to add feature wise but after taking a quick look at the chrome tabs api, I found a few functions I could make use it.\nHiding: I added an option to automatically close the groups when sorting Labeling: I added an option for automatic group titles based on the tab domains Sorting and Redesign: To support the new options I simplified the UI and created checkboxes to add on the hiding/labeling/sorting to the grouping. That\u0026rsquo;s also to say I also replaced the SORTING button to the GROUPING button where it just groups instead of sorting as well. Coding it was fairly simple though I had some annoyances since most of all the tab updating and retrieving functions were promises so I needed to format my code to work with it. Additionally, I hit a dumb wall not being able to hook on to Chrome\u0026rsquo;s tabGroups api before realizing that I needed to request permissions to access that api in the manifest. This is what I get for coming back to this so much later.\nFinal thing I did was created a Chrome developer account and pay a $5 fee to get the extension up on the Chrome Web Store.\nhttps://chrome.google.com/webstore/detail/tabby/enmendkpkmeeaoboologanofpjccoipm\n","permalink":"http://localhost:1313/posts/2023/old/dev/reviving-tabby-sort/","summary":"A couple years ago I made a little extension for chrome to organize and sort tabs by url domains. It was really quick and crudely designed and since I wasn\u0026rsquo;t even a Chrome user, I didn\u0026rsquo;t think too much about improving it. Fast forward to now, I decided to do exactly that.\nLooking at old code Actually the code was mostly better than I expected, other than some variable naming and redundant lines, I didn\u0026rsquo;t see much need to rewrite it.","title":"Reviving Tabby Sort"},{"content":"Today I spent some time working on getting the instructions dialogues up. I originally planned on creating a secondary overlay window that pops up given a trigger. Very similar to how to the toast notifications work but instead with a much larger area. The only problem with that is I wanted a clean way to inject custom html into the dialogue box depending on who triggers it. In the case of the toast notification I just pass it a string which is fine given it\u0026rsquo;s only suppose to be a sure notifier anyways. For the instructions, I may need to add bullets or text stylings depending on what the instructions are for. Strings aren\u0026rsquo;t going to cut it.\nInstead of using some globally triggered dialogue, I instead fashioned a new options block component that with two named slots. One for the actual content and another that contains html for the relevant instructions and with a trigger, it will swap between showing the content or the instructions.\n\u0026lt;div class=\u0026#34;options-block\u0026#34;\u0026gt; {#if !showInfo} \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;slot name=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;font-size: 5rem;\u0026#34;\u0026gt;\u0026amp;#63;\u0026lt;/div\u0026gt; \u0026lt;/slot\u0026gt; \u0026lt;/div\u0026gt; {:else} \u0026lt;div class=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;slot name=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;/slot\u0026gt; \u0026lt;/div\u0026gt; {/if} \u0026lt;button on:click={() =\u0026gt; showInfo = !showInfo}\u0026gt;\u0026amp;#63;\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; This way, the instructions for the dialogues are all nicely encoded into the html rather than needing to send html using js. For the trigger, I just added a small absolutely position questions mark button at the corner of each options block.\nNow the syntax to create a new widget in the options panel with instructions just requires indicating which slot you want to child content to belong in.\n\u0026lt;OptionsBlock\u0026gt; \u0026lt;PianoPedal slot=\u0026#34;content\u0026#34;\u0026gt;\u0026lt;/PianoPedal\u0026gt; \u0026lt;div slot=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;p\u0026gt; piano pedalling \u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;b\u0026gt;sustain pedal\u0026lt;/b\u0026gt; \u0026lt;em\u0026gt;space\u0026lt;/em\u0026gt;: holds the duration of played notes\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;b\u0026gt;sostenuto pedal\u0026lt;/b\u0026gt; \u0026lt;em\u0026gt;rshift\u0026lt;/em\u0026gt;: unimplemented\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;b\u0026gt;soft pedal\u0026lt;/b\u0026gt; \u0026lt;em\u0026gt;lshift\u0026lt;/em\u0026gt;: softens velocity of played notes\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/OptionsBlock\u0026gt; I also took the chance to reorganize the music generation block by separating out the playback portion and he generation portion to make it easier to differentiate the two features.\n","permalink":"http://localhost:1313/posts/2023/old/dev/pianotypes-devlog-3/","summary":"Today I spent some time working on getting the instructions dialogues up. I originally planned on creating a secondary overlay window that pops up given a trigger. Very similar to how to the toast notifications work but instead with a much larger area. The only problem with that is I wanted a clean way to inject custom html into the dialogue box depending on who triggers it. In the case of the toast notification I just pass it a string which is fine given it\u0026rsquo;s only suppose to be a sure notifier anyways.","title":"Pianotypes Devlog 3"},{"content":"I\u0026rsquo;m a bit more than late for a second devlog but I figured there\u0026rsquo;s no better thing to call this. I originally made this blog for the purposes of short and sweet devlogs but my recent posts have been bit larger than what I originally intended. I\u0026rsquo;m hoping with this log to bring that focus back down to just that.\nSince my previous post, I\u0026rsquo;ve done a lot of fine tuning of the UI and functionality of the site. One I\u0026rsquo;m quite happy with was adding draggability to the inputs simialr to other software like Blender. It\u0026rsquo;s a lot more convenient than typing and allows you to set options easily using mouse only.\nFurthermore, I\u0026rsquo;ve added some toast notifications for some short logging and responsive text to user actions. I want to add some text guides to the options and while this is a step in that direction, it isn\u0026rsquo;t quite it. At least I have a way to logging errors to the user for now.\nNext, I took a look at the midi playback, particularly the scheduled playback using smplr. Because the smplr soundfont player only has a callback for the ending of notes, I don\u0026rsquo;t have a way to notify the piano when to press a key, just when to release it. My temporary solution was to just use timeouts but it had time inconsistency issues. I thought I might try making a PR on smplr to add an onstart callback but after looking at the web audio API, it turns out it\u0026rsquo;s not as easy as it might seems. The reason why smplr has an onended callback in the first place is because it use scheduled web audio nodes that come with an onended callback but not an on start.\nI did some scouring and found a hacky solution on StackOverflow to create a duplicate audio node that\u0026rsquo;s silent and play that only for the purpose of it\u0026rsquo;s callback. If I set the delay and duration of that node to be the same, it effectively makes its start and stop identical giving me an onstart callback. I replaced my timeouts with audio nodes, as well as creating a secondary audiocontext so I don\u0026rsquo;t overload my original since now I effectively have double the audio nodes in use. The result is my keyboard presses being completely in sync with the playback! AudioContexts have a much better timing system than timeouts.\nI think my next focus is finally getting those instructions dialogues in cause that\u0026rsquo;s the main feature that\u0026rsquo;s keeping from having more people try the app.\n","permalink":"http://localhost:1313/posts/2023/old/dev/pianotypes-devlog-2/","summary":"I\u0026rsquo;m a bit more than late for a second devlog but I figured there\u0026rsquo;s no better thing to call this. I originally made this blog for the purposes of short and sweet devlogs but my recent posts have been bit larger than what I originally intended. I\u0026rsquo;m hoping with this log to bring that focus back down to just that.\nSince my previous post, I\u0026rsquo;ve done a lot of fine tuning of the UI and functionality of the site.","title":"Pianotypes Devlog 2"},{"content":"I recently finished a course in deep learning and wanted to took a look at a few ways I could apply my knowledge to. The idea the came to me was to create an ai music generation model and feed it into my old pianotypes app. Simple right!\nIt\u0026rsquo;s all broken So I booted up my old, dusty, untouched, forgotten, delapidated, abused, ramshackled, repository, and lo and behold, I can\u0026rsquo;t get it to run. I tried updating packages, my nodeJS version, moving around code, but nothing worked. In fact, I think updating anything at all just made it worse. Mind you, this project has been mostly untouched for a couple years so it was prone to deprecated libaries and syntax that made it very difficult to remedy. At this point I\u0026rsquo;ve all but given up on trying to polish a giant heaping pile of rust.\nTime to rebuild Easiest solution I figured was to just rebuild it all from scratch. I mean, it\u0026rsquo;s not like the code isn\u0026rsquo;t reuseable, it just needs some tuning as well as my on reviewing of the newest VueJS boilerplate. This honestly may have the quicker short-term solution but having been tired wading through annoying JavaScript syntax, I thought it good to look around for some other options.\nSvelte was the number 1 result to came from my quick research around, it was well loved by the community due to it\u0026rsquo;s much simpler and modern syntax a well as it being 30% (allegedly) faster than popular frameworks like React due it compiling your code to pure JavaScript rather than using a middleman like the virtual DOM. They had me at simpler syntax.\nRebuilding with a new framework Well, I don\u0026rsquo;t want to talk too much about this part as it\u0026rsquo;s just the usual finagling with new libraries and figuring out how to get things work. I do think it\u0026rsquo;s funny how I wanted to choose the framework that involved the least amount of work when the least amount of work would\u0026rsquo;ve been to stick with the same one (VueJS) heh. Still, benefits quickly outweighted the cost as I quickly noticed how much less code I needed to write to do essentially the same thing. By the end I confirm, Svelte is fantastic.\nFirst hiccup After I got the hang of reactivity in Svelte, it was pretty smooth sailing porting over everything from VueJS. Except, state management. Vue has a pretty comprehensive set of tool for handling global state which was extremely important for my project. I track nearly the entire piano state in a single class. From a design perspective, I dunno maybe it\u0026rsquo;s bad, it certainly feels like it might be, but for my project it also has its benefits in simplifying state to one object. In VueJS, I can simply wrap the object in a reactivity wrapper and I can simply import the object in any of my components and get the the global state I need just like that.\nIn Svelte, a reactivy wrapper doesn\u0026rsquo;t quite exist, instead they use a concept known as stores. It\u0026rsquo;s honestly great in hindsight but for the way I\u0026rsquo;ve designed my piano state, the purpose of stores doesn\u0026rsquo;t translate 1:1. I wrote another post that goes into more about how I got it done but it took a lot of research as well as trial and error before I could find a clean solution that I was happy with.\nFixing old issues This about the actual design of the old piano types project. I still liked the general aesthetic so I ported over most of the main CSS stylings as is. However the layout wasn\u0026rsquo;t responsive to different screen sizes and crowded the area needed for the piano. This is something I realized even back when I was working on the original but I couldn\u0026rsquo;t be bothered to fix it.\nI had some prior ideas about moving everything into a pull out tab so I started with that approach and created a new pull out tray. I realized that a lot of the features that I planned to add to the piano tended to exist as it\u0026rsquo;s own thing. Like I plan to add a midi player, that is fairly different from the pedalling features so it should probably have some separation in the UI. I added separated blocks in the options panel to contain each new feature that I add as well as some existing ones.\nI put blocks for the piano controls as well as another block for hotkeys. I rounded the edges of each block which ended up making them look like widgets. So, I named the directory they sit in as /widgets and I will presumptuously continue to call them widgets.\nGetting to the new stuff Midi Player The first new feature I needed to add was midi playback. While the goal is to use AI to generate music, I first need a way for the piano to play it. This went sooo much better than I could\u0026rsquo;ve expected. The piano itself is already all purpose while also being simple to operate. All it needs to receive what key to press and receive it at the right time to play a song. I found a nice library for parsing midi files that relays events in time with the midi files. All I had to do was hook up the midi events to key presses on the piano and it immediately worked.\nHotkey Guides Before we get to the Ai part, one of my favourite features I\u0026rsquo;ve added actually comes back to the hotkey system. Though I think I bound the hotkeys as sensibly as I could, it still quite confusing to learn their placements and actually play with them. So, my solution was the add visual guides on hotkey placements. Particularly, darkening and focusing out the keys that aren\u0026rsquo;t bound and my favourite, raising the keys that are bound to the top keyboard row. Even to my surprise, this made it sooo much easier to read and I\u0026rsquo;m very happy with it. So much so, that I\u0026rsquo;ll include a little gif. c:\nMusic Generator with MagentaJS Originally, I planned to train my own music generation model with TensorFlow but this is hard. I\u0026rsquo;m pretty green on building decent models and training takes a lot of time. Instead, I came across MagentaJS which offers an API for AI music generation using high-quality, community made models. This was perfect for my needs and mitigates the problem of needing to build a good model myself.\nThere was one issue integrating it into my project which was the fact that currently, MagentaJS isn\u0026rsquo;t offered as an ESModule but whether just a JavaScript bundle. This is a problem because I\u0026rsquo;m using the Svelte framework which is designed to work with newer node version and ES standards. It expects ESModules and doesn\u0026rsquo;t allow simple usage of the Require() statement to drop in JS bundles. I manage to find a work around through just adding a CDN script tag to the html directly. I prefer not to depend on a CDN and there are some drawbacks (No way for me to delay AudioContext creation), but this will have to work for now until MagentaJS is offered as a ESModule.\nI used an LSTM model they have for generating new midi sequences given a variable length input sequence. The resulting sequences aren\u0026rsquo;t all that spectacular and can only output single voice melodies rather than chords. As an aside, since MagentaJS comes with a bunch of midi processing tools, I was able to create a new midi playback for the music generation that relies on note scheduling rather than real-time inputs. It gives better timed songs and handles a larger thoughput of note. The drawback is scheduled notes don\u0026rsquo;t allow live instrument changing, pedaling, or even playing along with it as that will cancel the scheduled notes. Because of the drawbacks, I opted to keep both midi playback options.\nThe midi generation space in AI is pretty underdeveloped from what I can see, so I think there\u0026rsquo;s a lot of value later on for me to look into building better midi models. Particularly, ones that can play more than one note at a time.\n","permalink":"http://localhost:1313/posts/2023/old/dev/revisting-pianotypes/","summary":"I recently finished a course in deep learning and wanted to took a look at a few ways I could apply my knowledge to. The idea the came to me was to create an ai music generation model and feed it into my old pianotypes app. Simple right!\nIt\u0026rsquo;s all broken So I booted up my old, dusty, untouched, forgotten, delapidated, abused, ramshackled, repository, and lo and behold, I can\u0026rsquo;t get it to run.","title":"Revisting Pianotypes"},{"content":"I\u0026rsquo;ve been working on migrating my old PianoTypes project over from Vue to Svelte. So far the syntax has been much cleaner to use and has been an overall a great experience. My first and primary pain point thus far has been implementing a global reactive state for the piano.\nThe Problem In Vue, it\u0026rsquo;s possible to add a reactive wrapper to any object. In my case, I had created a class containing all of the piano data and property methods so all I had to do was wrap this class in the wrapper and state is maintained across all instances of my piano class.\nMaking Svelete Classes Reactive With Stores Let\u0026rsquo;s start by creating a simple JavaScript.\nclass ClassStore { constructor(num) { this.num = num } increment() { this.num++; } } When managing state with Svelte stores, the overarching paradigm is everything with a subscribe method is a store.\nWe can easily apply this to our class. Additionally, rather than implementing the class subscription manually, we can make use of Svelte\u0026rsquo;s writable function to return all subscribers the most up to date copy of our class.\nimport { writable } from \u0026#39;svelte/store\u0026#39;; class ClassStore { constructor(num) { this.num = num; this._store = writable(this); } increment() { this.num++; this._store.set(this); } subscribe(subscriber) { return this._store.subscribe(subscriber); } } Most notably, every single one of our class methods needs to call the .set method on our writable store. This way, every update to our class will notify subscribers.\nSubscribing to Properties of a Reactive Class Using Derived Stores While we now have a reactive class, any subscription to the class will be set to react to any changes made to the class instance. This can create a lot of unnecessary overhead and also result in undesirable updates.\nFor instance, when making a reactive class for a piano, I kept an internal dictionary of the states of each piano key. Whenever a single piano key is pressed, the list updates resulting in every single (all 88) key to be notified.\nWe can instead, create a new store based off of our main store that only tracks the state of a portion of our original class. This is a derived store.\nI\u0026rsquo;ve included a simplified version of the aforementioned piano store. This piano only holds 7 keys and whether they are being played.\nclass PianoStore { constructor() { this.keyboardStates = { C: false, D: false, E: false, F: false, G: false, A: false, B: false }; this._store = writable(this) } pressKey(note) { this.keyboardStates[note] = true; this._store.set(this) } releaseKey(note) { this.keyboardStates[note] = false; this._store.set(this) } getIsPressed(note) { return this..keyboardStates[note]; } subscribe(subscriber) { return this._store.subscribe(subscriber) } } const piano = new PianoStore(); export default Piano If we referenced the store directly, we\u0026rsquo;ll get updates from any key update.\n// KeyC.svelte \u0026lt;script\u0026gt; import piano from \u0026#39;PianoStore\u0026#39; const note = \u0026#39;C\u0026#39; $: if ($piano.getIsPressed(note)) console.log(note) \u0026lt;/script\u0026gt; \u0026lt;div on:keydown={$piano.pressKey(note)} on:keyup={$piano.releaseKey(note)}\u0026gt;\u0026lt;/div\u0026gt; This component will end up console logging no matter which note is pressed when we actually only want it to log when the current key 'C' is pressed.\n// KeyC.svelte \u0026lt;script\u0026gt; import piano from \u0026#39;PianoStore\u0026#39; import { derived } from \u0026#39;svelte/store\u0026#39;; const note = \u0026#39;C\u0026#39; const isPressed = derived(piano, ($piano) =\u0026gt; $piano.getIsPressed(note)); $: if (isPressed) console.log(note) \u0026lt;/script\u0026gt; \u0026lt;div on:keydown={$piano.pressKey(note)} on:keyup={$piano.releaseKey(note)}\u0026gt;\u0026lt;/div\u0026gt; Now we make a new derived store that is only reactive on the current note 'C' state inside the $piano.keyboardStates.\n","permalink":"http://localhost:1313/posts/2023/old/dev/svelte-reactive-classes/","summary":"I\u0026rsquo;ve been working on migrating my old PianoTypes project over from Vue to Svelte. So far the syntax has been much cleaner to use and has been an overall a great experience. My first and primary pain point thus far has been implementing a global reactive state for the piano.\nThe Problem In Vue, it\u0026rsquo;s possible to add a reactive wrapper to any object. In my case, I had created a class containing all of the piano data and property methods so all I had to do was wrap this class in the wrapper and state is maintained across all instances of my piano class.","title":"Svelte Reactive Classes"},{"content":"Vertex Shaders So far, I\u0026rsquo;ve been primarily writing 2D shaders to create patterns on a flat canvas. In practice, shaders can also be applied to 3D objects to manipulate not only their textures but also their shape.\nTo start, let\u0026rsquo;s take a look at a simple 2D shader placed on a 3D cube.\nNotice that the texture remains flat but is effectively cropped onto the projection of the cube. Ideally, we\u0026rsquo;d like the texture to wrap over the cube instead.\nShaders With Normals Normals refer to vector data stored at each vertice of a mesh. The vector points perpendicular from the mesh and gives us information on which way this vertice is facing. We can use this information to make sure our shader gets properly transformed onto each side of the shape instead of covering its profile.\nTiming Functions It\u0026rsquo;s easy to add moving parts to a shader by including a u_time term. However, if you want more interesting motion, you should be interested in adding shapes and curves using functions.\nSlope Step This was my first implementation of a start and stop timing function. It\u0026rsquo;s very close to working but has a fatal flaw.\nfloat slope_step(float x) { float c = .5*floor(x)*ceil(sin(x*PI)); float b = .5*floor(x+1.)*ceil(sin(x*PI-PI)); float a = x*ceil(sin(x*PI))-floor(x)*ceil(sin(x*PI)); return a + b + c; } https://www.desmos.com/calculator/zj8mjz9tos\nNotice how the shader occasionally glitches positions. Because we need to hardcode a value for PI, the imprecision causes our sine waves to not line up where we need them to and instead, return unexpected values at those junctures.\n","permalink":"http://localhost:1313/posts/2023/old/dev/some-more-glsl/","summary":"Vertex Shaders So far, I\u0026rsquo;ve been primarily writing 2D shaders to create patterns on a flat canvas. In practice, shaders can also be applied to 3D objects to manipulate not only their textures but also their shape.\nTo start, let\u0026rsquo;s take a look at a simple 2D shader placed on a 3D cube.\nNotice that the texture remains flat but is effectively cropped onto the projection of the cube. Ideally, we\u0026rsquo;d like the texture to wrap over the cube instead.","title":"Some More GLSL"},{"content":"While writing this blog, I\u0026rsquo;ve been constantly finding a need to include various JavaScript libraries such as graphics libraries or text processors. I love the simplicity of Markdown but sometimes it can be really helpful to include more meaningful content and visuals to better illustrate a certain topic or idea.\nMy most recent endeavor was adding an easy way to render shaders within each of posts which turned out to be a more painful task than I originally anticipated. Here, I hope to document some of my process of importing modules and hopefully make it easier for whoever is reading to import modules into their own Hugo blog.\nCore Hugo Features to Know Themes Themes are essentially, a set of prebuilt pages, partials, and shortcodes created by other developers for you to use. Each theme has its own page templates that change the way the site and any posts look as well as theme specific features like breadcrumbs and indexing.\nHugo is designed to be highly extensible and easy for you to add these features or design yourself but using a pre-built theme just saves you a lot of work. Instead, your best bet is to choose a theme and extend or override files in that theme as you need.\nthe theme will explicitly sit inside a themes folder and inside you may notice, it has a similar directory structure to your main directory\n// This is a condensed version of items you may find in your Hugo directory - assets - images - layouts themes - ThemeX - assets - images - layouts - partials - shortcodes Some of these directories will be seemingly clones of your original. This the beauty of Hugo\u0026rsquo;s themes. To edit a file in a theme, just copy the file from that theme directory into the corresponding directory in your root directory and edit the file directly. Hugo will prioritize using files in your directory rather than a theme. Any files added that are not in your theme will just be new files added on top of the theme.\nShortcodes Depending on what kind of module your importing, you may or may not need a shortcode. A shortcode is just syntatic way to cleanly inject any html into the middle of your Markdown.\nlayouts \u0026gt; shortcodes \u0026gt; html.html\n\u0026lt;div class=\u0026#34;html-block\u0026#34;\u0026gt;{{.Inner}}\u0026lt;/div\u0026gt; \u0026lt;style\u0026gt; .html-block { display: block; width: 100%; margin: 1rem 0; } \u0026lt;/style\u0026gt; I can then use this in my markdown to drop in pure html in my posts. This may not be good practice, but this is my blog and I have the freedom to do so.\n{{\u0026lt; html \u0026gt;}} \u0026lt;small style=\u0026#34;color: magenta;\u0026#34;\u0026gt;Hello!\u0026lt;/small\u0026gt; {{\u0026lt;/ html \u0026gt;}} Hello! You may notice the use of handle bar notation in the above code. You\u0026rsquo;re also given access to some additional syntax with Go templating for more entry points into Hugo\u0026rsquo;s features. In this case, I\u0026rsquo;m just using {{.Inner}} to grab the text placed within the shortcode and rerending it out between a \u0026lt;div\u0026gt;\u0026lt;/div\u0026gt;.\nPartials You can think of partials as components in a framework like React. They are bits of HTML and Go templating that can be added directly into the base page template.\nLet\u0026rsquo;s say you wanted to add a custom signature at the bottom of all your pages, you can create that signature in a partial and then add that partial directly into your page templates.\nImporting a JS Module As mentioned before, we don\u0026rsquo;t want to load our module globally so instead, we will try to only load the module inside the specific post. To keep things simple and avoid cluttering your directory, we will depend on CDN\u0026rsquo;s to serve the JS that we want.\nAt first, you might try to grab the JS library in a script tag add it to your template.\n\u0026lt;script src=\u0026#34;https://\u0026lt;host\u0026gt;/three.js/0.154.0/three.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; This way all js on the page has access to the library and our JS can look like this:\nimport * as THREE from \u0026#39;three\u0026#39;; This may work but we\u0026rsquo;ll need to create our JavaScript file in every single one of our posts and call it in every one. Ideally, we would like our JS to be in one place and called from every posts that needs it. Naively, we may try placing our JS inline inside our partial or even shortcode but we\u0026rsquo;ll very quickly be met with the message import declarations may only appear at top level of a module.\nInstead, we\u0026rsquo;ll try to grab our JS globally from our /assets directory.\nCreating a Front Matter Param To start, we want to be able to optionally enable our import for each post. We can simply do this by adding a .Params check in the header of our post template for whether the shader parameter is true. If so, we\u0026rsquo;ll insert a partial which will then add our module.\n\u0026lt;!-- Import threeCanvas module --\u0026gt; {{ if (.Params.shader) }} {{ partial \u0026#34;shader.html\u0026#34; }} {{ end }} Now in our post, we can simply enable it in our front matter\n--- title: \u0026#34;Hello World\u0026#34; date: 2023-01-01 draft: False shader: true --- Creating our Import Partial Here we use a bit of a tricky syntax courtesy of Hugo Pipes to process files in our assets directory. Regis Philibert has a great post that goes more in-depth on the pipes feature.\n{{ $script := resources.Get \u0026#34;js/script.js\u0026#34; }} {{- $globalJS := $script | resources.Minify | fingerprint -}} \u0026lt;script type=\u0026#34;module\u0026#34; async src=\u0026#34;{{ $globalJS.Permalink }}\u0026#34; integrity=\u0026#34;{{ $globalJS.Data.Integrity }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; What happens here is that we grab our script at assets/js/script.js, do some extra processing like minification, and then use .Permalink to create a visible link for our script to import the module from.\nAll That\u0026rsquo;s Left is to Use It Now that we\u0026rsquo;re able to run our asset/script.js while inside our post, we can now proceed to write the JS module itself.\nimport * as THREE from \u0026#39;https:\u0026lt;host\u0026gt;.three.module.min.js\u0026#39;; // The rest of the script... Then, for whichever post you\u0026rsquo;d like the module to run, just set the front matter param to true and your module will be imported!\nshader: true In my case, my module loads shaders by query selecting for an HTMLElement with class .three-container. I\u0026rsquo;ve created another shortcode to generate that html.\nThe result for me is a nice shortcode that allows to me easily render shaders within a blog post such as the below.\n","permalink":"http://localhost:1313/posts/2023/old/dev/js-modules-in-hugo/","summary":"While writing this blog, I\u0026rsquo;ve been constantly finding a need to include various JavaScript libraries such as graphics libraries or text processors. I love the simplicity of Markdown but sometimes it can be really helpful to include more meaningful content and visuals to better illustrate a certain topic or idea.\nMy most recent endeavor was adding an easy way to render shaders within each of posts which turned out to be a more painful task than I originally anticipated.","title":"JS Modules in Hugo"},{"content":"Fractals are built off of a single pattern or formula, repeated constantly with smaller and smaller transformations. In glsl, this idea translates nicely into iterative coding with for loops.\nHow to Make a Fractal I\u0026rsquo;ll be following kishimisu\u0026rsquo;s guide on shaders.\nCreate a Shape First off, we can start by creating some base patterns that we want to design our fractal off of. In this case, we can start with a simple circle.\nuniform float u_time; uniform vec2 u_resolution; void main() { // center uv coordinate and normalize aspect ratio vec2 uv = (gl_FragCoord.xy*2. - u_resolution.xy) / u_resolution.y; vec3 color; float dist = length(uv); color = vec3(dist) - .5; color = abs(color); color = .1/color; color = pow(color, vec3(2.)); gl_FragColor = vec4(color, 1.); Add fract() We can then form out a pattern by setting the uv coordinates to its fractional component.\nuv = fract(uv)*2.-1.; Throw It in a Loop Here is where the fractal comes in. We can keep repeating this pattern for larger and larger uv coordinates by constantly iterating over our circle code. Additionally, we add our circle color to a finalColor variable so we get the result of every iteration on our canvas.\nvec3 finalColor; for (float i = 0.; i \u0026lt; 3.; i++) { uv = fract(uv)*2.-1.; float dist = length(uv); color = .1/abs(vec3(dist) - .5); color = pow(color, vec3(2.)); finalColor += color; } gl_FragColor = vec4(finalColor, 1.); Add Some Complexity and Time While we have a fractal, the pattern is rather simple. We can up the interest just by nudging a few values and adding a few terms.\nIt can be helpful to declare some terms outside the loop to keep the pattern from being too repetitive. In this case, I\u0026rsquo;ve added global_dist for the radial distance of a pixel from the center.\nWe can also start animating by wrapping some terms in a sin() and adding a constant u_time to the phase shift for a smooth cycle between values.\nfloat global_dist = length(uv); // declared outside of loop uv = fract(uv*1.5)*2.-1.; float dist = length(uv)*global_dist; color = abs(.6*sin(color + global_dist + dist*2. + u_time)); Final shader\nuniform float u_time; uniform vec2 u_resolution; void main() { vec3 finalColor; vec2 uv = (gl_FragCoord.xy*2. - u_resolution.xy) / u_resolution.y; float global_dist = length(uv); vec3 color; for (float i = 0.; i \u0026lt; 3.; i++) { uv = fract(uv*1.5)*2.-1.; float dist = length(uv)*global_dist; color = .1/abs(vec3(dist) - .5); color = abs(.6*sin(color + global_dist + dist*2. + u_time)); color = pow(color, vec3(2.)); finalColor += color; } gl_FragColor = vec4(finalColor, 1.); } Start Experimenting With this basis, you can continue to add to the fractal by encorporating gradients, adjusting iterations, adding curves, and anything at all really. The tiniest change can generate a wildy different fractal pattern and it\u0026rsquo;s all about experimentation to get something you might like.\nuniform float u_time; uniform vec2 u_resolution; #define PI 3.1415926538 #define TAU 6.2831855 // https://iquilezles.org/articles/palettes/ vec3 palette( in float t ) { vec3 a = vec3(.5); vec3 b = vec3(.5); vec3 c = vec3(1.); vec3 d = vec3(abs(.2*sin(u_time*.1)), abs(sin(u_time*.1)), 0.5); return a + b*cos( 6.28318*(c*t+d) ); } void main() { vec3 color; vec2 uv = (gl_FragCoord.xy*2. - u_resolution.xy) / u_resolution.y; float dist; float global_dist = length(uv); vec3 final_color; for (float i = 0.; i \u0026lt; 3.; i++) { uv = fract(uv*1.5) - .5; dist = length(uv)*exp(-global_dist); color = palette(global_dist+u_time*.5); dist = .1*sin(dist*8. + u_time*2.); dist = abs(dist); dist = .01/dist; dist = pow(dist, 1.2); final_color += color*dist; } gl_FragColor = vec4(final_color, 1.); } ","permalink":"http://localhost:1313/posts/2023/old/dev/basics-of-shader-fractals/","summary":"Fractals are built off of a single pattern or formula, repeated constantly with smaller and smaller transformations. In glsl, this idea translates nicely into iterative coding with for loops.\nHow to Make a Fractal I\u0026rsquo;ll be following kishimisu\u0026rsquo;s guide on shaders.\nCreate a Shape First off, we can start by creating some base patterns that we want to design our fractal off of. In this case, we can start with a simple circle.","title":"Basics of Shader Fractals"},{"content":"I\u0026rsquo;ve been trying to get into the habit of drawing casually as opposed to doing so as part of some contest or project. Part of that is just drawing what I feel in the moment. I\u0026rsquo;m not intending to follow any style but at the same time I\u0026rsquo;m not stopping myself from easing any habits which resulted with a few pictures that do share a style. I always felt with drawing that I have no clue what my style is. For instance, everytime I play Gartic Phone, I tend to do something different each time and I don\u0026rsquo;t find myself easing into any one method. Perhaps more casual art does a better job at bringing out the characteristics of my drawing.\n","permalink":"http://localhost:1313/posts/2023/old/other/some-casual-art/","summary":"I\u0026rsquo;ve been trying to get into the habit of drawing casually as opposed to doing so as part of some contest or project. Part of that is just drawing what I feel in the moment. I\u0026rsquo;m not intending to follow any style but at the same time I\u0026rsquo;m not stopping myself from easing any habits which resulted with a few pictures that do share a style. I always felt with drawing that I have no clue what my style is.","title":"Some Casual Art"},{"content":"Rotation is one such transformation and that can be achieved by locking the desired axes of rotation and transforming the rest of the axes by \\(sin\\) and \\(cos\\).\nIntuition of 3x3Matrix Transformations 3x3 matrices can be used to bend the coordinate space in 3 dimensions, effectively allowing any desired transformations to objects in that space.\nAs the coordinate space is being transformed, it\u0026rsquo;s helpful breaking up the matrix into its unit vectors\n$$ \\vec{a} = x\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix} + y\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} + z\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix} $$\nImagine taking a 1x1x1 unit cube from our coordinate space and aligning each of the sides to the given unit vectors of the matrix. Every unit in our coordinate space will be the same and any vectors within that space will have each component vector scaled.\nThe Rotation Matrix The same idea applies if we want to rotate our coordinate space. We want to be able to translate each unit axis in a circle based off of some angle. Remember at each point of a unit circle, height is equal to \\(sin\\theta\\) and width is equal to \\(cos\\theta\\). We can can then use those as our vector inputs to follow a constant circle based off of \\(\\theta\\).\nWe get the following matrices for rotating about each axis of rotation.\n$$\\def\\rmatrices{ R_x(\\theta) = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; cos\\theta \u0026amp; -sin\\theta\\\\ 0 \u0026amp; sin\\theta \u0026amp; cos\\theta\\\\ \\end{bmatrix} R_y(\\theta) = \\begin{bmatrix} cos\\theta \u0026amp; 0 \u0026amp; sin\\theta\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ -sin\\theta \u0026amp; 0 \u0026amp; cos\\theta\\\\ \\end{bmatrix} R_z(\\theta) = \\begin{bmatrix} cos\\theta \u0026amp; -sin\\theta \u0026amp; 0\\\\ sin\\theta \u0026amp; cos\\theta \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix} } \\rmatrices $$\nLet\u0026rsquo;s take a look at how the \\(R_z(\\theta)\\) matrix was formed as it\u0026rsquo;s the easiest to visualize since looks like a 2D rotation. If we want to rotate by an axis, we don\u0026rsquo;t want any coordinate on that axis to move while everything rotates around it.\nTherefore we set the zcomponent to \\(\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}\\) to multiply any values on it by 1.\n-unfinished-\nSandbox Example $$\\rmatrices$$\n","permalink":"http://localhost:1313/posts/2023/old/dev/3d-matrix-rotations/","summary":"Rotation is one such transformation and that can be achieved by locking the desired axes of rotation and transforming the rest of the axes by \\(sin\\) and \\(cos\\).\nIntuition of 3x3Matrix Transformations 3x3 matrices can be used to bend the coordinate space in 3 dimensions, effectively allowing any desired transformations to objects in that space.\nAs the coordinate space is being transformed, it\u0026rsquo;s helpful breaking up the matrix into its unit vectors","title":"3D Matrix Rotations"},{"content":"Neural Networks See Machine Learning\nHow do you decide how many neurons to use per layer?\nOne way is to start with all layers uing the same amount of neurons and continue adding them until they start overfitting the data\nDropout: Regularlization technique to avoid overfitting. Leaves out data to better deal with general cases. 20%-50% dropout is a good starting range. Momentum: Helps finding the direction of next descent and prevent oscillations. Typical choice between 0.5 to 0.9 Epochs #times the whole training data is shown to netowrk during training. 1 epoch = one forward and one backward pass of all training data Back Propagation See Machine Learning\nUsing the gradient (slope) of the cost function, we take steps down the slope through multiple learning iterations in order to find some local minima to the cost. Ideally leading to a high accuracy model.\nCommon Problems Overfitting Model gets too comfortable with the test set and becomes less able to classify data outside of it. Accuracy improves, validation (outside data) worsens.\nVanishing Gradients Gradients often get smaller as back propagation moves to lower layers (closer to start input). Can result earlier layers being virtually unchanged Result\nTraining doesn\u0026rsquo;t converge to a good solution Prevent network from learning long term dependencies Exploding Gradients Gradients get too large and algorithm starts to diverge.\nStabilizing Gradients Techniques to avoid and remedy problems such as the aforementioned.\nBatch Normalization Normalize the layers inputs (recenter and scale to a normal distribution) at each given layer\nCan remove the need of normalizing your inputs Adds complexity to model as well as runtime penality Gradient Clipping Clamps every component of the gradient vector to [-1.0, 1.0]\nRegulariation Methods to avoid overfitting data\nL1 \u0026amp; L2 Regularization Both these methods involve adding another term to the cost function that increases the cost as complexity of the model increases. Typically done before activation.\nL1: Used in Lasso Regression adds an absolute value value of weights. This linearly increases cost of certain features and unimportant costs will eventually converge to zero, effectively reducing our relevant feature set.\nL2: Used in Ridge Regression adds the sum of squares weights. This increases the cost of features down to near but not quite zero. Doesn\u0026rsquo;t remove complexity as effectively but can better retain accuracy of the model.\nDropout Every training step, any neuron has a chance of being left out and ignored for that training step. Reduces model complexity and avoid picking up too much noise. Typically set at 50%.\nConvolutions Analying images is a complex matter for machine learning as our feature vector scales with the number of pixels in our image. If we were to directly use those inputs, we are likely to get a highly overfit model with very long training times.\nThe role of ConvNet is to reduce the images into a form that is easier to process, without losing features that are critical for getting a good prediction. This is important when we are to design an architecture that is not only good at learning features but also scalable to massive datasets. - https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/\nConvolution Operation in 1D The standard operation in 1D works as follows:\nLet\u0026rsquo;s take two arrays\n$$A = [\\textbf{5,3,7},5,9,7]$$ $$B = [1,2,3]$$\nWe then we take the elements in B and multiply it by the first n elements in A where n is the length of B. We then sum up the values.\n$$A\\cdot B = 5\\cdot 1 + 3\\cdot 2 + 7\\cdot 3 = 17$$\nWe then slide our choice of n elements by 1 and repeat the operation.\n$$A = [5,\\textbf{3,7,5},9,7]$$ $$B = [1,2,3]$$\n$$A\\cdot B = 3\\cdot 1 + 7\\cdot 2 + 5\\cdot 3 = 22$$\nWe then repeat this operation until our choice of n elements reaches the end, giving the following convoluted array.\n$$A\\times B = [17, 22, 39, 44]$$\nNotice the length of our convoluted array is \\(len(A)-len(B)+1\\)\nConvolutions in 2D (Images) The operations for 2D is largely the same, the difference being we are now operating on matrices.\nWe take the matrix of the pixels in our original image and multiply it by a filter matrix known as a kernel. The output of the convolution is known as the feature map.\n$$ \\begin{bmatrix} 1\u0026amp;0\u0026amp;1\\\\ 0\u0026amp;1\u0026amp;0\\\\ 1\u0026amp;0\u0026amp;1 \\end{bmatrix} $$\nFilters can be configured as any combination of 1\u0026rsquo;s and 0\u0026rsquo;s. For example, you might have one filter of a vertical line and another with a horizontal line. A vertical filter would blur the horizontal features of the image and similarly vice versa. Imagine how choosing specific filters can allow your feature map to focus on specific details of your image.\nAs we slide our kernel across the pixel array, we must move across the vertial and horizontal pixels to cover the whole image.\nBuilding a Convolutional Network with TensorFlow Prepare the Input Data Convolutional networks are most commonly used for image analysis so we want to prepare our data as a 2D array of pixels. The dimensions of our data will depend on the dimensions of the image, the colour channels, the numbers of classifiers, as well as how many samples we have.\n# we are using mnist in which which we have 10 digit classification num_classes = 10 # input image dimensions img_rows, img_cols = 28, 28 # colour channels, in this case we\u0026#39;re using b/w images num_channels = 1 # the data, split between train and test sets (X_train, y_train), (X_test, y_test) = mnist.load_data() # returns numpy arrays # 80/20 training and validation set (X_valid, y_valid) = X_train[:12000], y_train[:12000] X_train, y_train = X_train[12000:], y_train[12000:] # Shape of X_train: (60000, 28, 28) # Shape of y_train: (60000,) Here we do some shaping of our arrays to better used in our model.\n# numpy reshape from array of 2D arrays to array of 3D arrays # The first dimension is the number of samples in our dataset, the other 3 are the image dimensions, we add another dimension for the colour channels X_train = X_train.reshape(60000, img_rows, img_cols, num_channels) X_test = X_test.reshape(10000, img_rows, img_cols, num_channels) # Converts our y_train from a 1D vector to a 2D matrix # Before our y_train contains 60000 truth samples from 0-9 # Now it contains 60000 vectors of size \u0026lt;num_classes\u0026gt; where instead of containing the number explicitly, # the number is one-hot encoded into the indices # Ex. 4 would now be encoded [0,0,0,1,0,0,0,0,0,0] in the matrix y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) Constructing the Model Creating the model is straight forward and involves setting hyperparameters depending on how we want to tune our training.\nmodel = Sequential() model.add( Conv2D( filters=4, kernel_size=(3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(img_rows, img_cols, num_channels) ) ) model.add(Conv2D(1, (3, 3), activation=\u0026#39;relu\u0026#39;)) # apply max pooling to downsize the output by a factor of 2 model.add(MaxPooling2D(pool_size=(2, 2))) # apply dropout for regularization model.add(Dropout(0.25)) # flatten the output features to perform standard softmax classification model.add(Flatten()) # the dense layer learns the \u0026#39;classification\u0026#39; part of our task model.add(Dense(128, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(num_classes, activation=\u0026#39;softmax\u0026#39;)) # output layer Compiling the Model batch_size = 128 epochs = 10 model.compile( loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=[\u0026#39;accuracy\u0026#39;] ) history = model.fit( X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_valid, y_valid) ) score = model.evaluate(X_test, y_test, verbose=0) print(\u0026#39;Validation loss:\u0026#39;, score[0]) print(\u0026#39;Validation accuracy:\u0026#39;, score[1]) Transfer Learning Datasets are integral to the success of a neural network but often there just isn\u0026rsquo;t big enough datasets that exist for every problem. In the case of images, trying to build a model to classify a niche subject matters can be difficult without ample data.\nTransfer learning allows us to use some or all of pre-trained models and iterate over them to create models. Many fantastic image models have already been professionally built across many talented minds so it\u0026rsquo;s advantageous to piggy back off of them.\nBut how is someone elses model useful if we\u0026rsquo;re trying to solve a niche problem? Early layers in neural nets are more generalized in the features they pick up and become more and more fine tuned the deeper the net goes. We can leverage the earlier layers in an existing model that\u0026rsquo;s well trained to classify broad shapes in images and freeze the remaining layers and instead use our own layers that are trained to more specifically classify those shapes in the way we want. That\u0026rsquo;s the general idea.\nVGG16 is one such powerful neural net for image recognition we can leverage in our own model.\n# VGG16 pre-trained model without fully connected layers and with different input dimensions image_w, image_h = 180, 180 model = tf.keras.applications.VGG16(weights = \u0026#34;imagenet\u0026#34;, include_top=False, input_shape = (image_w, image_h, 3)) # We freeze several layers in VGG16 to avoid training them for layer in model.layers[:17]: layer.trainable = False We can preprend a new model with the existing model and add our own layers for training.\nnew_model = Sequential([ model, Flatten(name=\u0026#39;flatten\u0026#39;), Dense(260, activation=\u0026#39;relu\u0026#39;, name=\u0026#39;new_fc1\u0026#39;, kernel_initializer=\u0026#34;HeNormal\u0026#34;), Dense(100, activation=\u0026#39;relu\u0026#39;, name=\u0026#39;new_fc2\u0026#39;, kernel_initializer=\u0026#34;HeNormal\u0026#34;), Dense(5, name=\u0026#39;new_predictions\u0026#39;) ]) new_model.compile( loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer = tf.keras.optimizers.Adam(lr=0.01), metrics=[\u0026#34;accuracy\u0026#34;] ) Sometimes it can be advantageous to freeze all of the layers of the pretrained to avoid overfitting, especially if we have a small dataset. As always, finding methods to the madness in a neural network is highly involved with trial and error and constant comparison.\nDeep Computer Vision Computer vision is the field of computer science dedicated to enabling computers to understand images and videos.\nObject Classification and Localization Being able to identifY specific objects within an image frequently used in a variety of technological applications. For example, face filters need to identify where human faces and specific features are. This can become compilicated as any single picture can have a variety of sizes, aspect ratios, colours, and objects of the interest the computer needs to filter through in order to identify an object.\nTF Model for Identifying Single Objects A simple model involves training a network to identify an object in an image by outputting a bounding box at the desired location. For identifying a single object, this is a fairly simple task\nFirst we acquire a dataset of images with box labelling\n# Construct a tf.data.Dataset ds_train = tfds.load(\u0026#39;voc/2012\u0026#39;, split=\u0026#39;train\u0026#39;, shuffle_files=True) ds_test, ds_validation = tfds.load(\u0026#39;voc\u0026#39;, split=[\u0026#39;test\u0026#39;,\u0026#39;validation\u0026#39;], shuffle_files=True) This particular dataset has lot\u0026rsquo;s of components as shown below\nprint(ds_train.take(1)) # creates a new dataset with a max size of 1 \u0026gt; \u0026lt;_TakeDataset element_spec={\u0026#39;image\u0026#39;: TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), \u0026#39;image/filename\u0026#39;: TensorSpec(shape=(), dtype=tf.string, name=None), \u0026#39;labels\u0026#39;: TensorSpec(shape=(None,), dtype=tf.int64, name=None), \u0026#39;labels_no_difficult\u0026#39;: TensorSpec(shape=(None,), dtype=tf.int64, name=None), \u0026#39;objects\u0026#39;: {\u0026#39;bbox\u0026#39;: TensorSpec(shape=(None, 4), dtype=tf.float32, name=None), \u0026#39;is_difficult\u0026#39;: TensorSpec(shape=(None,), dtype=tf.bool, name=None), \u0026#39;is_truncated\u0026#39;: TensorSpec(shape=(None,), dtype=tf.bool, name=None), \u0026#39;label\u0026#39;: TensorSpec(shape=(None,), dtype=tf.int64, name=None), \u0026#39;pose\u0026#39;: TensorSpec(shape=(None,), dtype=tf.int64, name=None)}}\u0026gt; Some of the important components include\nimage: An image tensor labels: The label for the image bbox: The bounding box of object in this sample. bbox[0]: \\(l\\) from top bound to top edge of box bbox[1]: \\(l\\) from left bound to left edge of box bbox[2]: \\(l\\) from top bound to bottom edge of box bbox[3]: \\(l\\) from left bound to right edge of box Next we section off our dataset for training\n# Map our dataset to just the image, label, and bounding box and form into batches of 32 for training ds_train_image = ds_train.map(lambda a: (tf.image.resize(a[\u0026#39;image\u0026#39;], [300,300]), (a[\u0026#39;objects\u0026#39;][\u0026#39;label\u0026#39;][0], a[\u0026#39;objects\u0026#39;][\u0026#39;bbox\u0026#39;][0]))).batch(32) ds_test_image = ds_test.map(lambda a: (tf.image.resize(a[\u0026#39;image\u0026#39;], [300,300]), (a[\u0026#39;objects\u0026#39;][\u0026#39;label\u0026#39;][0], a[\u0026#39;objects\u0026#39;][\u0026#39;bbox\u0026#39;][0]))).batch(32) ds_validation_image = ds_validation.map(lambda a: (tf.image.resize(a[\u0026#39;image\u0026#39;], [300,300]), (a[\u0026#39;objects\u0026#39;][\u0026#39;label\u0026#39;][0], a[\u0026#39;objects\u0026#39;][\u0026#39;bbox\u0026#39;][0]))) Create the model\n# We start with the Xception model (71 layers) as well with pre-trained weights (transfer learning) base_model = tf.keras.applications.Xception(weights=\u0026#34;imagenet\u0026#34;, include_top=False) # Teras model functional API https://keras.io/guides/functional_api/ as opposed to sequentially adding layers avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output) avg = tf.keras.layers.Dense(200, activation=\u0026#34;relu\u0026#34;)(avg) n_classes = 20 optimizer = tf.keras.optimizers.Nadam(learning_rate=0.00001) class_output = tf.keras.layers.Dense(n_classes, activation=\u0026#34;softmax\u0026#34;,name=\u0026#34;class\u0026#34;)(avg) loc_output = tf.keras.layers.Dense(4, name=\u0026#34;loc\u0026#34;)(avg) model = tf.keras.Model(inputs=base_model.input, outputs=[class_output, loc_output]) # Freeze layers from training for layer in model.layers[:110]: layer.trainable = False for layer in model.layers[110:]: layer.trainable = True model.compile(loss=[\u0026#34;sparse_categorical_crossentropy\u0026#34;, \u0026#34;mse\u0026#34;], loss_weights=[0.1, 0.9], # depends on what you care most about optimizer=optimizer, metrics=[\u0026#34;accuracy\u0026#34;]) model.summary() model.fit(ds_train_image, epochs=50, validation_data=ds_test_image) Multiple Object Classification Things become much more complicated when you want your model to identify multiple objects witin a given image. Different bounding boxes are needed with their associated locations and dimensions.\nSliding Window Method A naive approach involves sliding a bounding box across the image and having the model identify the bounding boxes that identify an object. This is computationally expensive as we need to consider differing box dimensions.\nSingle Shot Detector Recurrent Neural Networks These are a special type of neural net designed to work for a sequence of data. Normally each set of inputs are independent from eachother but there are needs for them to depend on each other such as sequences of words or sounds. The main difference is that we feed our output back into the next set of inputs and keep a concept of \u0026ldquo;memory\u0026rdquo; in our net.\nUnfolding Layers Unfolding is a terminology to describe a very simple process. Examining a recurrent layer over multiple timesteps. Rather than showing one recurrent network, we show it as multiple networks that feed into eachother to create that recurrent behavior. This makes it easier to see how our recursion occurs as well as how backpropagation occurs.\nBackpropagation in Time (BPTT) Our normal backpropagation algorithm is updated for use in RNN\u0026rsquo;s.\nchoose \\((k\\)) time steps for training Propagate through unfolded network for \\(k\\) time steps compute output at all timesteps Calcualte error as: \\e = (y_{t+k} - p_{t+k}) \\(y\\) is our output \\(p\\) is our target output at the time step Backpropagate the error across unfolded network and update weights. LSTM Standard RNN\u0026rsquo;s often suffer from vanishing or exploding gradients when the sequence of data gets too large. It doesn\u0026rsquo;t do well on remembering data from very old time steps. LSTMs add another component to the network that learns what timesteps should be remembered or forgotten. This way, we can keep track of long term sequential data.\nNatural Language Processing (NLP) The computational treatment of human language. Many large datasets of text exist for training known as a corpus that are large, structured sets of text for machine learning.\nGenerally, the process of creating an NLP model requires a lot of preprocessing of your training text and figuring which method of text chunking and labelling best suits the model you want.\nLater when we talk about word embeddings, the learning component of NLP comes into play where can train a model to set a weights for each word in a vocabulary.\nCommon Problems Ambiguity: The same sentence can have multiple meanings Synonyms and Homonyms Mispellings Sarcasm Allegory Dialects Tokenization A strategy to deal with complex sentences is to break it up into smaller chunks. Most often this simply by splitting the word by whitespace. You may also decide to drop any punctuation to simplify the problem at the cost of some understanding.\nText Preprocessing Raw text often contains loads of unnecessary words, punctuations, grammar, and word variations that can make it harder to discern between words and phrases as well as being computationally expensive. To remedy this, before we enter any machine learning, we do some preprocessing on the text to help simplify our task.\nSome common processing includes removing stop word, which are common words that don\u0026rsquo;t add much meaning to a text (\u0026ldquo;the\u0026rdquo;, \u0026ldquo;this\u0026rdquo;, \u0026ldquo;and\u0026rdquo;). We can do lemmatization do reduce variations of words to their root (\u0026ldquo;finally\u0026rdquo;, \u0026ldquo;final\u0026rdquo;) so we can better compare them. You can find some prebuilt text preprocessors among ML libraries that do more or less depending on what you\u0026rsquo;re looking for.\nInteger Encoding vs Word Embeddings Traditional word encoding is done by one-hot-encoding every word in your vocabulary. As you can imagine, vectors become huge and memory heavy as well as the fact that each word is independent and we don\u0026rsquo;t capture meaning or context.\nWord Embeddings instead represent each word as a multidimensional vector of floating point values. In essence, weights. The value here as that use floating point values are trainable and we can therefore train our model to capture more meaning out of each word by training those weights.\nword2vec are a family of techniques used to train word embeddings\nWhen you\u0026rsquo;re using word embeddings, you actually don\u0026rsquo;t need to do any text preprocessing. Since we\u0026rsquo;re embedding trainable features into each word, we can get a lot of granular meaning trained into a corpus whether they\u0026rsquo;re stop words or punctuations. The training should do the job of placing less emphasis on less important words as well as developing meaningful weights to compare words together with. Nonetheless, this doesn\u0026rsquo;t mean you can\u0026rsquo;t do any preprocessing, but it\u0026rsquo;s not as necessary as it is for integer encoding and it\u0026rsquo;s effect is more nuanced.\nTF-IDF \u0026amp; Similarity Term Frequency is purely the frequency that a term appears in a document. It can be measured many different ways such as just the raw occurences or logarithmically \\(log(1+ rawcount\\)).\nInverse Document Frequency is how common a word is among a given corpus\u0026ndash;and then we take the inverse. We take the inverse because often we want to minimize the weight of common terms like \u0026ldquo;the\u0026rdquo; or \u0026ldquo;a\u0026rdquo; so infrequent terms can have a greater impact.\nTF-IDF puts both of these concepts together and places importances on terms that are frequently used and have rarity in a corpus.\nWhen it comes to actually comparing text, a common method to use is known as Cosine Similarity\nEncoder-Decoder Architecture https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\nAuto Encoders These are a special case of encoder-decoder archetecture where the input and output domains are (typically) the same. More specifically, we take out input and compress them down to a lower dimensionality (called a bottleneck) before reconstructing it back to its original dimensions in the output\nFor example, here we take an image and encode it down to a smaller dimension of latent_size before decoding it back to size.\n# set random seeds to aid reproducibility tf.random.set_seed(42) np.random.seed(42) latent_size = 30 # defines the dimensionality of the bottleneck def rounded_accuracy(y_true, y_pred): return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred)) # define our encoder stacked_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=\u0026#34;selu\u0026#34;), keras.layers.Dense(latent_size, activation=\u0026#34;selu\u0026#34;), ]) # define our decoder stacked_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=\u0026#34;selu\u0026#34;, input_shape=[latent_size]), keras.layers.Dense(28 * 28, activation=\u0026#34;sigmoid\u0026#34;), keras.layers.Reshape([28, 28]) ]) # compile and train the model stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder]) stacked_ae.compile( loss=\u0026#34;binary_crossentropy\u0026#34;, optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=[rounded_accuracy] ) # Note, in this case, the targets are the same as the input! That\u0026#39;s why we\u0026#39;ve # passed X_train for the \u0026#39;y_train\u0026#39; parameter. history = stacked_ae.fit( X_train, X_train, epochs=20, validation_data=(X_valid, X_valid) ) But what\u0026rsquo;s the point of training in a loop like this? Well after we\u0026rsquo;ve trained the model, the bottleneck layer learns the important features of the inputs. We can then drop the decoder and use just the encoder for other classification tasks. This is the autoencoder.\nAnother application, instead of having the exact same inputs and targets, we can use this same model and use noisy images as the input and the source images as the outputs. This would be training to model on how to denoise images!\nGenerative Algorithms First, for a refresher on Naive Bayes, refer back to here\nVariational Auto Encoders https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\nWhere traditional autoencoders encodes an input to a single point, VAEs encode inputs into a probability distribution over the latent space. Specifically, we want the latent space created by the encoder to be regular enough that we can take a point within the space and decode it to generate new content. With a normal autoencoder, this latent space is typically unorganized and overfitted so some points in the space will decode into meaningless content. VAEs regularize the latent space so it has good properties for the generative process.\nA latent space is a representation of compressed data. You can imagine this as the bottleneck of an autoencoder. Latent attributes are single piece of that representation.\nCourse Retrospective As of 8/8/2023, I have finished SCS3546 Deep Learning course from UoTSCS. Overall, I\u0026rsquo;ve gained a much better understanding of the fundamentals of deep learning and insight into how modern AI models are built today. There\u0026rsquo;s a lot more to the building of ML models than I expected. Logically, I knew there was a ton of incredibly complicated math and theory behind ML but I was kind of hoping a lot of that would be abstracted behind the code. To be fair, a lot of it is but there\u0026rsquo;s a huge importance to understanding what\u0026rsquo;s happening behind the seens to know how good models are built.\nAnother surprise was how much data processing is core to machine learning. I\u0026rsquo;m realizing now it\u0026rsquo;s really all about the data you use, the quality of it, and how you can format it, that creates the basis of the model you build. Furthermore, it\u0026rsquo;s just the pure fact that machine learning is relatively new and fast growing. While there are libaries that can help you process data, it\u0026rsquo;s rarely ever a \u0026ldquo;one function does it all\u0026rdquo; solution. Depending on the data and problem you\u0026rsquo;re dealing with, it\u0026rsquo;s on you to figure out how to get that data is a ML ready form.\nStatistics is also a very core part or ML, particularly for gauging and comparing the quality of a model. There is a lot of probability happening within the models themselves but I find it\u0026rsquo;s the graphs and charts where you really benefit from having a strong statistics background.\nAll this stuff I\u0026rsquo;m honestly quite bad at and have mostly disliked in school but I think my fascination AI use cases will have me applying my new knowledge is future projects.\n","permalink":"http://localhost:1313/posts/2023/old/ai/intro-to-deep-learning/","summary":"Neural Networks See Machine Learning\nHow do you decide how many neurons to use per layer?\nOne way is to start with all layers uing the same amount of neurons and continue adding them until they start overfitting the data\nDropout: Regularlization technique to avoid overfitting. Leaves out data to better deal with general cases. 20%-50% dropout is a good starting range. Momentum: Helps finding the direction of next descent and prevent oscillations.","title":"Intro to Deep Learning"},{"content":"I\u0026rsquo;ve always had trouble understanding probability, especially when entering into the more theoretical aspects of it. Here, I want to cover some of the basic concepts and functions core to probability in an easily digestible format that I can refer to later on when I inevitably forget it all.\nRandom Variable Whenever there\u0026rsquo;s a question of probability, you tend to have some range of possible outcomes sourced from a specific event.\nWe label some process that has multiple outcomes as a random variable.\nTypically, we use \\(X\\) to name this random variable though it can be any name capitalized as convention.\nAn easy example is a six-sided die where there are precisely six possible outcomes. In essence, our random variable can be thought of as a list of these outcomes.\nX = [1, 2, 3, 4, 5, 6] Expected Value As the name suggests, this averages out the probabilities of all the outcomes and returns the mean (\\(\\mu\\)) of all the outcomes.\n$$E[X] = \\sum_{x} P(X=x)*x$$\nFor example, in the case of a six-sided fair die, the probability of each outcome is uniform leading to the following expected value.\n$$E[X] = \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = 3.5$$\nThe average roll is 3.5\nVariance To preface, standard deviations and variance are nearly identical and both describe the dispersion of values. Standard deviation is often more useful in applications and visualizations than variance. However, standard deviations is derived from variance so it\u0026rsquo;s important to know what variance is though, seeing the purpose of variance this way can be difficult. Rather, comprehend the math of variance in relation to expected value, and then move on to standard deviation to understand its usefulness.\nThe variance tells us how spread out a probability distribution is. We do this by finding the difference between each outcome and the expected value (\\(\\mu\\)), and then take that expected value (in other words, we get the average difference each outcome is from the mean).\n$$V(X) = E[(X - \\mu)^2] = \\sum_{x} P(X=x)*(x-\\mu)^2$$\nYou may notice that we actually square that difference. Other than ensuring our difference is positive, taking square makes the math nicer later on.\nIn the case of a fair die, we get the following variance.\n$$V(X) = \\frac{1}{6}(17.5) = \\frac{35}{12}$$\nthe squared distance between each die face and the mean is 2.916.\nStandard Deviation Let\u0026rsquo;s start with this comparison with variance.\n$$\\sigma = \\sqrt{V(X)}$$\nWhen we talk about variance being the average difference from the mean, it can be difficult to picture as it is the squared distance from the mean. Instead, if we want to normalize the units we take the standard deviation as the square root of the variance.\nNow when we say the average distance each result is from the mean, we can properly denote it as \\(n\\) standard deviations away from the mean.\nFrom the prior die example, we get this standard deviation\n$$\\sigma = \\sqrt{\\frac{35}{12}} = 1.71$$\nNow let\u0026rsquo;s say we roll a 1 which deviates from the mean by 2.5, comparing this to our standard deviation we get the following\n$$\\frac{2.5}{\\sigma} = 1.46$$\nOur dice roll of 1 was roughly 1.46 standard deviations away from the mean which is actually just another way to say it was 2.5 away from the mean like we started.\nStatistical Uses Those who work with probabilities on large datasets are generally familiar with how data fits within a range of standard deviation. The chart above is saying:\n68% of values are within 1 \\(\\sigma\\) away from the mean 95% of values are within 2 \\(\\sigma\\) away from the mean 99.7% of values are within 3 \\(\\sigma\\) away from the mean Knowing the standard deviation gives great insight into how clustered dataset is around the mean. Larger values suggest a more spread out dataset.\nConditional Probabilities Conditional probabilities ask, \u0026ldquo;what is the probability of A given a condition B\u0026rdquo;.\nMore formally,\n$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\nThe intuition is that we\u0026rsquo;re looking at subset of the data that satisfies this condition B. Then, we\u0026rsquo;re taking a fraction of those that satisfy A within the subset B.\nBayes Theorem $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\nThe thing you might notice is that we\u0026rsquo;re also describing conditional probabilities, so what\u0026rsquo;s the difference?\nInstead of using the joint probability \\(P(A\\cap B)\\), we use the reverse condition \\(P(B|A)P(A)\\). But these are actually equivalent! Think about it this way: The probability of A and B happening is the same thing as saying the probability of A happening and the probabilty of B given that A happened.\nBut what\u0026rsquo;s the point?\nBayes Theorem is better suited to conditional problems where our condition updates our original probability. In other words, we can visualize the probability as a hypothesis and a piece of evidence. We can repeatedly update our probability as new evidence comes in.\n$$P(A\\cap B) = P(B|A)P(A) = P(New\\ evidence\\ after\\ current\\ event)P(current\\ event)$$\nAn Example Let\u0026rsquo;s say our hypothesis is that Anne is a vegetarian and let\u0026rsquo;s say that 1/4 of people are vegetarians. Then the probability of Anne being one is just so.\n$$P(H) = 1/4$$\nNow, let\u0026rsquo;s say we get some evidence that Anne is a climate change activist. Now the question is, \u0026ldquo;what\u0026rsquo;s the probability that Anne is a vegetarian given that she\u0026rsquo;s also a climate change activist (activist for short).\nLet\u0026rsquo;s say we infer that a fifth of vegetarians are activists and a tenth of non-vegetarians are activists. Then we can solve as follows\n$$P(H|E) = \\frac{P(Vegetarian)P(Activist\\ if\\ Vegetarian)}{P(Activist)} = \\frac{\\frac{1}{4}*\\frac{1}{5}}{\\frac{1}{5}+\\frac{1}{10}} = \\frac{1}{6}$$\nImagine from here, we can repeat the process with new evidence using the previous evidence as our hypothesis instead, further refining our probability. This forms the basis of many uses cases like risk analysis and machine learning.'\n","permalink":"http://localhost:1313/posts/2023/old/ai/overview-of-probability/","summary":"I\u0026rsquo;ve always had trouble understanding probability, especially when entering into the more theoretical aspects of it. Here, I want to cover some of the basic concepts and functions core to probability in an easily digestible format that I can refer to later on when I inevitably forget it all.\nRandom Variable Whenever there\u0026rsquo;s a question of probability, you tend to have some range of possible outcomes sourced from a specific event.","title":"Overview of Probability"},{"content":"In preparation for a deep learning course I\u0026rsquo;m taking over the Summer, I\u0026rsquo;m taking a short intro course on machine learning to help prepare me for some of the fundamental concepts. I\u0026rsquo;ve been avoiding AI for a while but given its ongoing application in nearly everything now, I figure it\u0026rsquo;s more than worth getting my feet wet.\nML Overview flowchart LR\rsubgraph Shader Lifecycle\rdirection LR\rd[(\"Dataset\")] --\u003e m((\"Model\")) --\u003e o(\"Predicted Values\") ---|compare with| a(\"Actual Values\") --\u003e|calculate| l[\"Loss\"]\rl --\u003e|training data| m\rend\rLoss Functions When we compare our predicted results to the actual results, how do we calculate the loss?\nLoss functions can vary but they can be as simple as calculating the difference.\n$$loss = sum(|y_{real} - y_{predicted}|)$$\n$$loss = sum((y_{real} - y_{predicted})^2)$$\nDatasets Training Set: Data to train the model Validation Set: Data that the model has not seen to ensure model can handle unseen data Testing Set: Data to test the model (seen or unseen) Types of Machine Learning Supervised Learning - Using labelled input data to train models, e.g. inputting animal pictured labelled with the coorresponding animal names. Unsupervised Learning - Using unlabelled input data to learn about pattern in data, e.g. inputting unlabelled animal pictures and having your machine try to group them Reinforcement Learning = Agent learning in an environment based on rewards and penalties Supervised Learning With a supervised dataset, you have a bunch of sampled data that each have an output label that classifies that data sample, as well as having 1 or more features which are just information about that subject.\nFor example you may be sampling for animals that are dogs, you\u0026rsquo;d have an output label dictating whether that simple is or isn\u0026rsquo;t a dog as well as features of the sample like their weight, size, and color.\nWe call the set of features the feature vector. We call the output label the target\nMachine Learning Models Once we have our data that we want to perform machine learning on, we need to decide on the model we use to classify new, unlabelled data.\nK-Nearest Neighbours (KNN) The intuition here is we have a labelled dataset, if get a new point in the dataset, we can infer how it should be labelled based on it\u0026rsquo;s distance from other points.\nMore formally, we take \\(K\\) of the nearest data points to our new point and based on the most common label, we infer the label of our new point. We can choose any number of \\(K\\) and depending on our dataset, can have varying levels of effectiveness.\nNaive Bayes Recall Bayes Theorem\n$$ \\def\\series{x_1, x_2,\u0026hellip;, x_n} P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\nNaive Bayes is a generalization of this formula for a series of evidence.\nFor some category \\(C_k\\) (e.g. cats, dogs, etc.) and evidence \\(\\series\\) (furry, red, small, etc.)\n$$ P(C_k|\\series)\\ \\alpha\\ \\Pi_{i=1}^n P(x_i|C_k)P(C_k) $$\nOur target acts as the category \\(C_k\\) Our feature vector act as the evidence \\(\\series\\) Derivation step for reference $$ P(C_k|\\series)\\ = \\frac{P(\\series|C_k)P(C_k)}{P(\\series)} $$ Notice we remove the demoninator from Bayes Theorem. Since it isn\u0026rsquo;t influenced by \\(C_k\\), it acts as a constant. In following, \\(\\alpha\\) stands for proportional since they are no longer equal.\nBut how do we use Naive Bayes to classify data?\n$$\\hat{y}\\ (predicted\\ category) = argmax_k P(C_k|\\series)$$\nWe want to find the category that maximizes the probability given by the Naive Bayes. Essentially, we\u0026rsquo;ll be running Naive Bayes on each category and compare the categories that returns the highest probability.\nNotice why Naive Bayes works despite only being a proportional value. We only want to compare probabilities, we don\u0026rsquo;t actually need the probability itself.\nLogistic Regression Support Vector Machines Neural Network For more complex data, modern approaches have been using neural network models.\nThe intuition of a neural network is that we want to break up our classification model into a set of neurons that take in\nflowchart LR\rsubgraph Neuron\rdirection LR\rx1 --\u003e|w0| h\rx2 --\u003e|w1| h\rxn --\u003e|wn| h\rh((\"Neuron\")) --\u003e|output| p(\"Activation Function\")\rb(\"bias\") --\u003e h\rend\rflowchart LR\rsubgraph Neural Network\rdirection LR\ri[(\"Input\")] --\u003e h[\"Hidden Layers\"] --\u003e p((\"Output Neurons\"))\rend\rEquation for a Neuron $$ \\begin{bmatrix} x_0\\\\ x_1\\\\ \\vdots\\\\ x_n \\end{bmatrix} \\cdot \\begin{bmatrix} w_0\\\\ w_1\\\\ \\vdots\\\\ w_n \\end{bmatrix} \\rightarrow z = \\sum_{i} w_i x_i + b \\rightarrow f(z) = a $$\nNeurons \u0026amp; Hidden Layers Neurons are grouped into layers known as hidden layers. We can have as many layers as deemed necessary, and each layer will have to job of classifying the data given by the previous layer.\nFor example, one layer may being classifying line strokes of an image while the next layer starts classifying different strokes together into shapes. The end goal may be to be able to identify text.\nWeights \u0026amp; Biases Each neuron\u0026rsquo;s job is to pick up on specific aspects of our input into it. They way we do this is by assigning weights to each of the inputs into the neuron.\nFor example, if our input is the pixels of a screen, perhaps we want our neuron to pick up only on a specific area on the screen of pixels. We would then have more postive weights on those pixels and weaker or potentially negative weights on the other pixels.\nEvery neuron will have it\u0026rsquo;s own associated weights that it assigns it\u0026rsquo;s inputs.\nDepending on our data, we want our neuron to only be active at a certain magnitude of values. For instance, we want our neuron to start activating given a weighted sum \u0026gt; 10. The solution is adding a constant called a bias that shifts our sum so that it only activates at the values we want.\nYou might be wondering, \u0026ldquo;okay but, how do we decide these weight and biases?\u0026rdquo;. If we were to do this manually, it\u0026rsquo;d be an astronomical test of patience. Instead, the work of finding the right set of weights \u0026amp; biases will come later in the learning portion.\nActivation Function Once the weighted sums are calculated, they are run through an activation function as a final step before we take the value of that neuron.\nActivation functions serve two primary purposes. Removing linearity from the network and for use in binary classification\nLinearity in a network would mean multiple layers could just be represented as a single linear combination, removing the complexity of having multiple layers in the first place. Why does this help? Most phenomena in the world can\u0026rsquo;t be represented linearly and if they can, there\u0026rsquo;s likely not a need to use a neural net in the first place. We want to describe non-linear phenomena using a non-linear model.\nAs for the second purpose, the values from the weighted sums can be virtually anything but often we want them to be between 0 and 1. This is where an activation function can come in to add a final normalization to the value of a neuron.\nA common activation function is the logistic function (sigmoid).\n$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\nThis is almost exclusively used for output layers for the binary classification\nA popular choice for hidden layer neurons is ReLU (Rectified Linear Unit)\n$$ReLU(a) = max(0,a)$$\n0 for a \u0026lt; 0, increases linearly otherwise\nAs an Equation Luckily, this sequence of weighted sums, activations, and biases can be computed as a matrix.\n$$ \\def\\activation{ a_0^{(1)} = \\sigma( \\begin{bmatrix} w_{0,0} \u0026amp; w_{0,0} \u0026amp; \\dots \u0026amp; w_{0,n}\\\\ w_{0,0} \u0026amp; w_{0,0} \u0026amp; \\dots \u0026amp; w_{0,n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ w_{k,0} \u0026amp; w_{k,0} \u0026amp; \\dots \u0026amp; w_{k,n} \\end{bmatrix} \\begin{bmatrix} a_{0}^{(0)}\\\\ a_{1}^{(0)}\\\\ \\vdots\\\\ a_{n}^{(0)} \\end{bmatrix} + \\begin{bmatrix} b_0\\\\b_1\\\\ \\vdots \\\\b_n \\end{bmatrix} ) } $$\nAny single neuron \\(a_n^{layer}\\) is the combination of normalized (\\(\\sigma\\)) weights and biases, of all the neurons from the previous layer.\nGradient Descent (or how calculus learns) In order to start learning, we need an idea of how bad or good a particular set of weights and biases are. We call this the cost function.\nThe idea is that on every learning iteration, we want to minimize the cost function. However creating such a function and directly calculating some minimum isn\u0026rsquo;t a trivial task.\nInstead, if we have some cost function, all we need to know is the slope of the function at a given input weights. Then we just need to shift the weights so that we move in the direction on the downward slope of the cost function. Hence, gradient descent.\nGradient descent just means walking in the downhill direction to minimize the cost function. - 3b1b\nBack Propagation This is the algorithm for efficiently computing gradient descent.\nTo reiterate, the goal of gradient descent is to systematically lower the cost function by adjusting the weights and biases for a given set of neurons.\nA common cost function is to simply take the squared difference of the models prediction vs the actual values.\n$$C(x) = (x - y)^2$$\nThe result is a series of values representing how closely or far away the model was the ideal answer.\nLet\u0026rsquo;s however start our intuition with how would we want our outputs to change to get a lower cost. If we were trying to train our model to identify numbers, we would have outputs of all the digits 0-9. Now let\u0026rsquo;s say we give our model the number 2. Our ideal output is for the 2 output light up to one while every other output is zero. We\u0026rsquo;d likely want it to increase the weights \u0026amp; biases to lead to 2 as well as lower the weights \u0026amp; biases of all the other outputs.\nNot only that, we out cost function, we know how far away each of our prediction are from the actual outputs. The more the cost, the more we want to lower or increase the weights \u0026amp; biases leading to that output.\nFurther more, we have 3 avenues to alter the activations for a neuron (the output of a neuron)\nChange the bias Change the weights Change the activations of the previous neuron Recall the equation for the activation of a given neuron\n$$\\activation$$\nNotice that we multiply the weights with the activations of the previous neurons. The brighter a neuron is, the more effect a change in weight will have. This is core to altering weights as we need to nudge them in proportion to the previous activations.\nWe can go through every one of the outcomes and collect an average of how much we want each of the preceeding neurons to change in order to get a better outcome. We can then repeat this for the previous layer\u0026rsquo;s neurons and so forth. Hence, back propagation.\nIn summary\n","permalink":"http://localhost:1313/posts/2023/old/ai/intro-to-machine-learning/","summary":"In preparation for a deep learning course I\u0026rsquo;m taking over the Summer, I\u0026rsquo;m taking a short intro course on machine learning to help prepare me for some of the fundamental concepts. I\u0026rsquo;ve been avoiding AI for a while but given its ongoing application in nearly everything now, I figure it\u0026rsquo;s more than worth getting my feet wet.\nML Overview flowchart LR\rsubgraph Shader Lifecycle\rdirection LR\rd[(\"Dataset\")] --\u003e m((\"Model\")) --\u003e o(\"","title":"Intro to Machine Learning"},{"content":"While trying to import a plane from Blender to Unity, I ran into the issue of the normals facing the wrong direction once imported into Unity. This was particularly an issue when it came to vertex shaders as any vertex transform performed incorrectly.\nThis crux of the issue is that Blender considers the z-axis to be the vertical axis while unity considers the y-axis to be. So, for a plane in Blender, the normals would face towards the positive z-axis but when imported to Unity, they remain so which to Unity is actually along the horizontal plane. Oddly, the actual model imports in the correct orientation, it\u0026rsquo;s just the normals that don\u0026rsquo;t adjust after import (which creates the mismatch problem of normals not facing the right direction. If the whole mesh and normals rotated equally, that\u0026rsquo;d be fine).\nAt the moment, I don\u0026rsquo;t have the best understanding of the solution but turns out when exporting as a .fbx, there\u0026rsquo;s an \u0026ldquo;apply transform\u0026rdquo; option that bakes the transforms into the model before importing. This fixes the issue. I\u0026rsquo;ll take a look this again another time, but for now this will be my goto solution.\n","permalink":"http://localhost:1313/posts/2023/old/dev/handling_normals_from_unity_to_blender/","summary":"While trying to import a plane from Blender to Unity, I ran into the issue of the normals facing the wrong direction once imported into Unity. This was particularly an issue when it came to vertex shaders as any vertex transform performed incorrectly.\nThis crux of the issue is that Blender considers the z-axis to be the vertical axis while unity considers the y-axis to be. So, for a plane in Blender, the normals would face towards the positive z-axis but when imported to Unity, they remain so which to Unity is actually along the horizontal plane.","title":"Handling Normals from Unity to Blender"},{"content":"From what I\u0026rsquo;ve read, one of the classic shader introductions people reference is this online module-of-sorts known as TheBookOfShaders. On there, it covers topics of how math can generates different shapes and textures within shaders along with plenty of examples and tip from basic to more advanced creations.\nDrawing Circles A Circle Using step To start, this is what it looks like using the step() function where for a circle of a radius, if the pixel is outside of this radius, give a value of 1, if not, return a 0. Make pixels within radius away from the center white, otherwise black\nfloat circle(in vec2 sst, in float radius) { vec2 dist = vec2(0.5)-st; float length = sqrt(pow(dist.x, 2.)+pow(dist.y,2.)); // getting magnitude of dist vector pct = step(length, radius); } smoothstep Something that confused me with smoothstep was that I expected the value returned to be the interpolated result of a start a and end b based off a fraction c. However it\u0026rsquo;s actually the inverse operation. You instead give it a value between a and b and value c in between the two and you\u0026rsquo;ll that percentage between the two back as a fraction of 1.\nThis is similar to the inverse lerp described by Freya Holmér\nThis was confusing as most interpolation I\u0026rsquo;ve used previously (typically in a function just called lerp) has its third argument be the fraction of 1 to interpolate with.\nfloat lerp(float a, float b, float t) { return (1.0 - t) * a + b * t // for 0 \u0026lt;= t \u0026lt;= 1, return a value as a fraction t of the range a -\u0026gt; b } // ex. lerp(0, 10, .5) = 5 float invLerp(float a, float b, float v) { return (v - a)/(b-a) // normalizes v between a and b to be between 0 and 1 and return that fraction } // ex. invLerp(0, 10, 5) = .5 Once this was understood, things like generating circles with smoothstep made a lot more sense.\nA Circle Using smoothstep() Now approaching with smoothstep()\nfloat circle(in vec2 st, in float radius) { float length = distance(st, vec(0.5)) // using GLSL built-in instead return 1. -smoothstep(radius-(radius*0.01), radius+(radius*0.01), length); } Effectively. this does the same thing except changes around the logic and blurs the edges of the circle.\nThis works largely the same as the step function except instead of a single number edge, you have a range between radius-(radius*0.01) and radius+(radius*0.01). If the interpolated value is under this range, smoothstep will return 0. If the interpolated value is above this range, it will return 1.\nNow, if it\u0026rsquo;s between this range, you will get a smooth fractional between 0 and 1 causing a transitional blurred edge to the circle as instead of flipping from black to white (0 -\u0026gt; 1), it blends towards it for the width of that range.\nThe range, TheBookoOfShaders gives it is small in order to keep the circle quite sharp. But, by increasing the range, the blur becomes much clearer.\nDot Product to Calculate Vector Length? Generally, division is a costly operation for computers and thus so is the square root. For this reason, if there\u0026rsquo;s an alternative to getting the length of a vector than the Pythagoras Theorem, we\u0026rsquo;d like to use it.\nA Doom developer when to great lengths to avoid performing the costly inverse square root operation that he developed a brilliantly cursed alternative to it.\nThe dot product returns the projection of one vector v1 onto another and multiplies the length of the projection with that other vector v2. There are a couple things to note.\nThe result is the magnitude of |v1projected| * |v2| The length of v1projected decreases the wider the angle between it and v2 The general formula is |A||B|cosθ In the case that v1 == v2, the magnitude of v1projected == v1 as well. This means the result of the dot product is actually |v1|**2!\nmore formally... if v1 == v2 then v1⋅v2 = v1⋅v1 = |v1||v1|cos(0) = |v1|**2 We\u0026rsquo;re now close to getting the length of the vector, but again need to take to sqrt of it all the get the final length of v1.\nLet\u0026rsquo;s look at what TheBookOfShaders does\nfloat circle(in vec2 st, in float radius) { vec2 dist = st-vec2(0.5); float length = dot(dist, dist)*4. return 1. -smoothstep(radius-(radius*0.01), radius+(radius*0.01), length); } Instead, what TheBookOfShaders opts to do is to multiply it all by 4 and call it day dot(dist, dist)*4. I\u0026rsquo;ve done a bunch of playing around and all I conclude is, this is not equivalent to the actual length of the vector. Comparing it to using a distance function, radius of the circle isn\u0026rsquo;t accurately represented. Instead, it following an exponential curve as you increase/decrease the radius value compared to a linear one where the value you input maps to the actual circle radii.\nUltimately, the purpose of their example is to showcase a circle in GLSL and even with the square of the vector length, it\u0026rsquo;s still enough to show it, just not for the correct radius. I will say, the author should\u0026rsquo;ve multiplied it by 2, this way at least the max circle radius of 0.5 would match up with the correct distance function.\nSo in the end, you\u0026rsquo;re better off using the square root here.\nfrac the frac() function is . It\u0026rsquo;s often use to help visualize values in you\u0026rsquo;re shader outside of 1. Specifically, shaders in Unity don\u0026rsquo;t natively clamp values between 0 and 1 so if you\u0026rsquo;re trying to create a gradient shader where color is based on the UV position on a mesh, that UV position can go above and below 0 and 1.\nThe implementation of frac() is incredibly simple and is as follows.\nfloat frac(float v) { return v - floor(v); } Frac will only return values between a range of 0 and 1 and if its fed a v out of this range, the pattern will repeat. So you can surround your shader code with a frac and get a repeating pattern that looks like this.\nGenerating Shapes with Arctangent Trigonometric functions are closely tied with circles and are a key component to generating repeating, cyclical patterns. Here, we are taking a look at the power of the arctangent function.\nTo begin, the arctangent function when graphed is a right rotation and mirror of the standard tangent graph.\nIt can be hard to wrap your head around how each value of x in the arctan graph correlates to its plotted value but the key is to focus on how the arctan function relates to the tan function itself.\narctan(x) = θ ==\u0026gt; tanθ = x It\u0026rsquo;s primarily used to find the angle between the opposite and adjacent sides of a trangle\nθ = arctan(opposite/adjacent) When wanting to create patterns, this relation should be your tool to visualizing how patterns can be built with arctan.\nFor example, for every pixel we have a vector2 of it\u0026rsquo;s position and by subbing that into arctan, we can get the angle that the pixel lies from the origin.\nLet\u0026rsquo;s move the origin to the center and try plotting the angle directly to the color of the shader.\nvoid main(){ vec2 st = gl_FragCoord.xy/u_resolution.xy; vec3 color = vec3(0.0); vec2 pos = st*2. - 1.; float r = length(pos)*2.0; float a = atan(pos.y,pos.x); color = vec3(a); gl_FragColor = vec4(color, 1.0); } In GLSL, atan has two overloads, first used here can have deceptive behavior when it comes to signs\natan returns either the angle whose trigonometric arctangent is yx or y_over_x, depending on which overload is invoked. In the first overload, the signs of y and x are used to determine the quadrant that the angle lies in. The value returned by atan in this case is in the range [−π,π]. The result is undefined if x=0\n.\nFor the second overload, atan returns the angle whose tangent is y_over_x. The value returned in this case is in the range [−π2,π2] . n.\nIf we then sub the angles into cos, we get the adjacent side of the right triangle angled θ from the origin.\nThe adjacent component is only positive in quadrants I and IV\nWhat\u0026rsquo;s exciting now is remember this is still a cosine function. We can increase the period in order to see more cycles.\nfloat f = cos(a*8.); // 8 cycles f = floor(f + .5); // round to solid values Morphing Mosaic void main() { vec2 uv = gl_FragCoord.xy / u_resolution; uv = uv*2. - vec2(1.); // center origin float angle = atan(sin(uv.y*PI), sin(uv.x*PI)); vec4 color = vec4(ceil(sin(angle*10. + u_time))); gl_FragColor = color; } Mixing Patterns By encorporating smaller, simpler patterns into your shader, you can combine them and get surprisingly complex designs that also show off the beauty of the math behind it.\nvoid main() { vec2 uv = gl_FragCoord.xy / u_resolution; uv = uv*2. - vec2(1.); // center origin float dist = length(uv); float angle = atan(uv.y*PI, uv.x*PI); float waves = sin(dist*5.*TAU - u_time*0.); float umbrella = sin(angle*5. + u_time*0.); vec4 color; color = vec4(umbrella); color += waves; gl_FragColor = color; } A ripple and a star pattern\nCombined together\n","permalink":"http://localhost:1313/posts/2023/old/dev/playing_with_glsl/","summary":"From what I\u0026rsquo;ve read, one of the classic shader introductions people reference is this online module-of-sorts known as TheBookOfShaders. On there, it covers topics of how math can generates different shapes and textures within shaders along with plenty of examples and tip from basic to more advanced creations.\nDrawing Circles A Circle Using step To start, this is what it looks like using the step() function where for a circle of a radius, if the pixel is outside of this radius, give a value of 1, if not, return a 0.","title":"Playing With GLSL"},{"content":"Shaders are programs used to describe how pixels should be arranged, colored, and transformed on the screen. The simple definition extends to so many applications especially in 3D graphics in movies and games. Every computer generated prop placed in a scene is carefully designed to look a certain way, all with the help of shaders.\nShader\u0026rsquo;s make use of the GPU to constantly run concurrent calculation on every pixel and you can decide what those calculations do through programming with languages such as HLSL and GLSL. You can decide that very pixel should be moved slightly to the left, or that they should be slightly more saturated under certain conditions, or even that they have follow the movement of a sine curve and create waves like an animation. You might start to realize that there is a lot a math involved and there is. Specifically, the way you want to arrange pixels makes heavy use of linear algebra to orient points in space as well as creative calculus to take advantage of functions when wanting to create tailored movements and patterns.\nNowadays, shader graphs also exist to help abstract way some of the underlying math and allow a more approachable method of developing shaders at the cost of some control. It\u0026rsquo;s is a fantastic tool that aids in the accessibility of shader programming as well as better visual understanding of how the flow of calculations vastly change the look of a mesh.\nThis post covers a summary of [Freya Holmér\u0026rsquo;s course] on Shaders (https://www.youtube.com/watch?v=9WW5-0N1DsI/ \u0026amp; https://www.youtube.com/watch?v=kfM-yu0iQBk/). Note, that the following notes are in the context of Unity shaders as shaders across different frameworks and engines are implemented differently and thus invite differing workflows and architecture. Conceptually they will be mostly the same.\nHow Are They Used It\u0026rsquo;s better to talk about shaders in the context of how they\u0026rsquo;re used which is typically starting from some material being fed into a shader and then placed onto an object. Depending on the framework or engine that you\u0026rsquo;re using, shader\u0026rsquo;s can be used very differently. For instance, in Unity, your first have to start with a base material and then feed that material information into the shader to perform calculations on to get the final texture.\nflowchart LR\rsubgraph Shader Lifecycle\rx(\"Mesh\") --\u003e s(\"Shader\")\ry(\"Material\") --\u003e s\rs --\u003e f((\"Final Mesh\"))\rend\rCredits to Navendu for the tutorial on setting up Mermaid diagrams on Hugo\nIn other cases, you can directly apply a shader to an object or create the objects purely using shaders. It depends on the implementation of the graphics software but application is the same. Manipulating pixels using the GPU.\nThe Unity Shader File flowchart TB\rs(.shader)\rs --- p(Properties)\rs --- ss(SubShader)\rss --- pa(Pass)\rpa --- v(\"Vertex Shader\")\rpa --- f(\"Fragment Shader\")\rsk((ShaderLab)) --\u003e p \u0026 ss\rh((HLSL)) --\u003e v \u0026 f\rVertex Shader From the Mesh, the shaders gets an overview of all the different vertices(points) that make up the 3D object and the following information for each vertex.\nPosition Coordinates Normal (Where that vertex is facing perpendicular to it\u0026rsquo;s surface) UV Coordinates Vertex Color More\u0026hellip; (typically not needed) The code you then write here can them perform any mathematical manipulation of the vertex as you\u0026rsquo;d like and the code you write will run concurrently for every vertex by your GPU. At a lower level, the vertices that the shader receives will by local to the objects origin for example it\u0026rsquo;s position. After you perform your manipulation, it must then by converted into Clip Space to be used in the 3D world.his is also the set of vertices the fragment shader will receive. The next subsection will describe in more detail these coordinate systems.\nflowchart LR\rz(\"Local Space Vertices\") --\u003e x{\"Vertex Manipulation\"} --\u003e c(\"Clip Space Coordinates\") --\u003e|sent to| v((\"Fragment Shader\"))\rIn short, the job of the vertex shader is to\u0026hellip;\nManipulate the position of vertices Pass data into the fragment shader (ex. altering UV coords) Coordinate Systems https://learnopengl.com/Getting-started/Coordinate-Systems\nWhen meshes are shown onto the screen, the vertex information goes through multiple coordinate systems before it ends up mapped as a position on your screen. The shader will receive data in the first stage of these coordinate systems and will require conversion in order to understand how those vertex transformations relate to it\u0026rsquo;s actual position in space and finally on your screen.\nThough the following coordinate flow is what\u0026rsquo;s used for OpenGL while Unity uses DirectX, the explanation of the differing coordinate spaces are helpful to know and relevant.\nLocal Space - coordinates in relation to the objects origin World Space - coordinates in respect to the larger environment world View Space - coordinates in respect to what the camera perspective see\u0026rsquo;s (particularly important when we compare how vastly different orthographic vs perspective camera represent space) Clip Space - Normalizes the coordinates between -1 and 1 based on the camera. Off camera view vertexes will be outside of this range and not visible on your screen. Screen Space - Transforms normalized coordinates to viewport coordinates. For the purposes of Unity, what\u0026rsquo;s important is the Local Space which is how vertices are fed into a shader and Clip Space which we can convert to using a built-in Unity API.\nFragment Shader Here, rather than calculating per vertices, Unity will rasterize the received vertices so calculations can be done per pixel and the shader will run only on pixels visible in the clip space (between -1 and 1). This process is done behind the scenes and it known as Frustrum Culling.\nA cool side effect of this is aliasing. Imagine unity trying to figure out how to fit square pixels into the shape of a triangle face on a mesh. Some of those pixels will overlap outside the shape of the face and result in jagged edges which is what anti-aliasing serves to minimize.\nSimilarly to before, all code will run concurrently for every pixel.\nflowchart LR\rz(\"Clip Space Vertices\") --\u003e x(\"Culled into Rasterized Pixels\") --\u003e c(\"Calculations\") --\u003e v((\"Output on Screen\"))\rIn order to rasterize, an important process that happens is Barycentric Interpolation. This blends the colours of each of the vertices to make approximate the colour of pixels between vertices. Otherwise, there\u0026rsquo;s no explicit information for Unity to know how these areas should be displayed.\nThe fragment shader does not receive the vertex colours but instead, all the interpolated pixel colours.\nIn short, the job of the fragment shader is to\u0026hellip;\nDetermine the color of each pixel Unity Materials Shader files are always associated with material files. You never apply a pure shader file to a object but instead a material which is then associated with a shader. This way, we can use the same shader across different materials.\nShader Properties You can pass information into the shader for use in calculation such as the game time or perhaps the health bar of a character. Using the health bar example, this can be useful as perhaps you want to proportion the filled amount of the health bar shown in the shader to it\u0026rsquo;s actual game value. These properties can be set in the Material\nCode Structure of Shaders Shader \u0026#34;Unlit/SimpleShader\u0026#34; { Properties { // input data // _MainTex(\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Value (\u0026#34;value\u0026#34;, Float) = 1.0 } SubShader { Tags {\u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34;} // allows determination of rendering properties. For the SubShader, this largely includes sorting alpha values, render queuing, etc. LOD 100 // level of detail, when you have multiple subshaders, they are prioritized based on this value Pass { Tags {} // Determination of graphics properties: blending modes, stencil properties, etc. CGPROGRAM #pragma vertex vert // tells compiler which function is vertex shader #pragma fragment frag // which function is fragment shader #include \u0026#34;UnityCG.cginc\u0026#34; // adds in built-in function from unity to aid in shader writing float _Value; // grabs from properties // sampler2D _MainTex; //texture stuff // float4 _MainTex_ST; struct MeshData { // per-vertex mesh data // Here you can decide what information you want to get from the mesh and store it in a variable by semantically declaring the data after a : float4 vertex: POSITION; // vertex position // float3 normals : NORMAL: // float4 tangent : TANGENT // float4 color : COLOR; float2 uv: TEXCOORD0; // uv0 coordinates //float2 uv1: TEXCOORD1; // uv0 coordinates }; struct v2f { // data passed from vertex 2 frag shader float2 uv: TEXCOORD0; // float4 vertex: SV_POSITION; // clip space coord of vertex }; v2f vert(MeshData v) { v2f o; o.vertex = UnityObjectToClipPos(v.vertex); // converts local space to clip space return o; } // ## DATA TYPES // bool 0 1 // int // float (32 bit float) good for world space // half (16 bit float) good for most things // fixed (low precision) -1 to 1 // vectors: float4 -\u0026gt; half4 -\u0026gt; fixed4 // matrices: float4x4 -\u0026gt; half4x4 fixed4 frag(v2f i): SV_Target { return float4(1, 1, 1, 0); } ENDCG } } } A Note on Shader Optimization Generally, a mesh will have more pixels when rendered than vertices. It\u0026rsquo;s usually best to do the calculations in the shader that has the least iterations. There may be more vertices than pixels in the case you have a high poly model being rendered from extremely far way. In which case after rasterization, you\u0026rsquo;ll only have a few pixels for the fragment shader to process.\nCreating Some Simple Fragment Shaders Displaying \u0026lsquo;Normals\u0026rsquo; as Colours Notice how the colors coorespond to the normals. The top is green as the normal on the top of the sphere is the vector (0, 1, 0), therefore mapping to green as the color.\nFirst, we grab the normal data from the mesh\nstruct appdata { float4 vertex : POSITION; float2 uv: TEXCOORD0; float3 normal : NORMAL; }; Next, we assign the normals in our vertex struct to the normals we took of the mesh.\nv2f vert(appdata v) { v2f o; o.vertex = UnityObjectToClipPos(v.vertex); o.normal = v.normal; return o; } Then in our fragment shader, we display each pixel as a color with rgb set to the normal vector and an alpha we\u0026rsquo;ll just set to opaque.\nfloat4 frag(v2f i) : SV_Target{ return float4(i.normal, 1); } Repeating Patterns There are a litany of ways repeating patterns can be achieved and naturally so, many involve the use of wave functions.\nTo start, try feeding the uv position into a sin function and watch the results.\nfloat2 col = sin(i.uv.xy); return float4(col.xy, col.x , 1); You\u0026rsquo;ll find that there\u0026rsquo;s virtually no difference compared to just feeding those uv coordinate to the fragment shader directly. If you think about what the sin(x) graph looks from 0 -\u0026gt; 1, it doesn\u0026rsquo;t oscillate in the range and instead only increases. To be able to see any amount of repetition, we need to increase the period. Increasing it by 2π or τ, will fit the whole first cycle into the 0 -\u0026gt; 1 range allowing us to see the beginning of some repetition. But let\u0026rsquo;s get several more cycles in with 6τ so we get a fuller hatching pattern instead.\nFrom there, we may also want to phase-shift to get a more symmetrical pattern by having the peak of the sin wave in the center.\nNow the hatching bars are quite blurry since there is a smooth shift between colours as the the pattern moves across the sin wave. Instead let\u0026rsquo;s have the values round to the nearest integer with a cool trick floor\nfloor(x + 0.5) // Rounds down below .5 exclusive and rounds up above .5 inclusive. Finally, we get this hatched shader\nfloat2 col = floor(sin(i.uv.xy*6*TAU - PI/2) + .5); return float4(col.xy, 0, 1); We get can even fancier and add a wiggle to the stripes. For simplicity, let\u0026rsquo;s add the wiggle just to the horizontal stripes. Intuitively, we want to do is shift the position of the pixels along the horizontal stripe up and down in a wave pattern.\nLet\u0026rsquo;s start by just adding an offset to those stripes.\nfloat offset = i.uv.x; float2 col = floor( cos( (i.uv.xy+offset)*12*TAU)*.6 + .5 ); // I\u0026#39;ve made some period and amplitude adjustments as well return float4(col.xy, 0, 1); You\u0026rsquo;ll notice that the horizontal stripes are now moving diagonally upwards because we\u0026rsquo;re adding to them as we move left to right on the x-axis.\nNow, it\u0026rsquo;s as simple as adding a wave function to our offset and we\u0026rsquo;ll get a smooth up and down wiggle to our line.\nfloat offset = .03*cos((i.uv.x)*6*TAU); float2 col = floor( cos( (i.uv.xy+offset) *12*TAU) *.6 + .5 ); return float4(col.xy, 0, 1); Similarly to the stripes themselves, I\u0026rsquo;ve adjusted the period and amplitude to change the look of the wiggles.\nNotice, though I\u0026rsquo;m only focusing on adding the wiggle to the horizontal stripes, they\u0026rsquo;ve also influenced the period of the vertical stripes. As the wave function of the offset peaks, we\u0026rsquo;re also phase-shifting the vertical lines which is why there are tighter grouping of vertical stripes after the peaks of the horizontal waves and bigger gaps after the dips.\nBlending If you\u0026rsquo;re familiar with blending modes in programs such as Photoshop, the theory is the same. When pixels are being layered on top of eachother, you can manipulate how pixels above interact with the pixels below. For example, you can add the values of the pixels to result in brighter colors when layering colors or multiply colors resulting in darkening (as colors as normalized between 0-1).\nThe process of blending looks like this\nThe achieve different blending effects on shaders, you will be manipulating this equation to decide how the colors defined in your shader will be effected by the colors behind it.\nCull Off // don\u0026#39;t cull back or front faces (allows a transparent box to render the front and back) ZWrite Off // disable writing to depth buffer ZTest LEqual // \u0026lt;default\u0026gt; adjust render should read from depth buffer (Only render if the depth is less than or equal to) Blend One One // set blending to additive (src*1 + dst*1) //Blend DstColor Zero // multiply (src*dst + dst*0) Depth Buffer When objects are in view of the camera, Unity will write its depth position from the camera as a value between 0 and 1. For other objects, their depth positions are also recorded but also compared to items already in the depth buffer. If they are behind an object, they will not render.\nObjects that aren\u0026rsquo;t in view can have their fragment shaders removed from the scene in order to save on resources that wouldn\u0026rsquo;t be seen by the player anyways.\nThis can have side affects for transparency textures as you want to objects behind them to render fully in order to see the transparent objects in front them overlay their colors.\nTo remedy this, you can set Tags to notify Unity\u0026rsquo;s renderer how they should deal with the shader\nTags { \u0026#34;RenderType\u0026#34; = \u0026#34;Transparent\u0026#34; // tags to inform render pipeline of the type of shader (used in things like post processing) \u0026#34;Queue\u0026#34; = \u0026#34;Transparent\u0026#34; // adjusts draw order for transparency (render after opaque materials are rendered) } Creating a Vertex Shader Now that we\u0026rsquo;re familiar with manipulating shaders on a per pixel basis, we can start thinking about how we can apply the same process into the mesh vertices themselves. Similar to how we\u0026rsquo;re able to create patterns using math functions, we can also render our vertices along the same math functions as well. As an example, we\u0026rsquo;re going to create a rippling effect where the shader creates actual ripples in the mesh shape just like a drop of water onto a still lake.\nFirst, let\u0026rsquo;s create the radial wave pattern in the fragment shader and apply that knowledge later into the vertex shader. The idea is identical to the linear striped patterns from earlier but instead of following along one of the uv components, we will get the distance vector to the center of the mesh instead.\nfloat4 frag(v2f i) : SV_Target{ float distance = length(float2(.5,.5)-uv.xy); // gets distance from center float wave = cos((distance) * 6 * TAU + _Time.y); return wave; } Now that we have our pattern, we can map that function directly into our vertex shader. The only difference is that instead of assigning the wave value to a pixel color, we will now assign it as a transform to the y component of each vertex, effectively bending the mesh up an down according to the cosine wave.\nFor posterity, let\u0026rsquo;s also just stick the wave code into its own function as well as tweaking it to look more ripple-like.\nfloat getWave(float2 uv) { float distance = length(uv * 2 - 1); // [another way] normalize the uv coordinate from -1 to 1 with 0 as center and get distance float wave = cos((distance) * 6 * TAU - _Time.y); wave *= _WaveAmp; // property to adjust wave height in inspector wave *= (1 - distance); // dissipate over distance return wave; } v2f vert(appdata v) { v2f o; v.vertex.y = getWave(v.uv0); o.vertex = UnityObjectToClipPos(v.vertex); o.normal = mul((float3x3)unity_ObjectToWorld, v.normal); // UnityObjectToWorldNormal(v.normals); o.uv = (v.uv0 + _Offset) * _Scale; return o; } float4 frag(v2f i) : SV_Target{ float wave = getWave(i.uv)*2; return wave; } And just like that, we have waves in both the pattern and shape of our plane.\nWith dissipation\nWithout dissipation\nTexture Sampling So far, we\u0026rsquo;ve been using shaders on textureless materials but often we want to use shaders to enhance existing textures that are on materials. For example, we might have a photo-realistic mud texture that we want to apply a shader to add some bumps and dips based on the texture.\nCompared to the previous shaders, there\u0026rsquo;s a few new pieces of code we need to include for us to use the colour data from the current material.\nProperties{ _MainTex(\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} // pass the texture data, 2D is most common but other types like 3D exist } // variables sampler2D _Maintex; // variable for texture data float4 _Maintex_ST; // optional, holds tiling scale (_ST is a Unity semantic and automatically gives it the tiling data) v2f vert(appdata v) { v2f o; o.vertex = UnityObjectToClipPos(v.vertex); o.uv = TRANSFORM_TEX(v.uv, _MainTex); // optional, scales the uv based on the tiling offset above o.uv = v.uv // otherwise, assigning the uv directly works the same return o; } float4 frag(v2f i) : SV_Target{ fixed4 col = tex2D(_MainTex, i.uv); // samples a color from the texture based on the uv return col; } Mapping a Texture to World Space Normally, when you map a material onto a mesh, you want to link the material position strictly to the UV coordinates of the object. In some cases such as for ground textures with planes, it can be really useful to instead map those into the world. This makes it so as you move or add more objects using, they can seamlessly use the same pattern.\nTo do this, we first need to convert each vertex coordinate into its respect world coordinate. This is also an opportunity to take advantage of TEXCOORDS as we need someplace to store these coordinates so they are accessible in the fragment shader.\nstruct v2f { float2 uv: TEXCOORD0; float3 worldPos: TEXCOORD1; // empty coordinates we will later use to store the converted vertex coords float4 vertex: SV_POSITION; }; v2f vert(appdata v) { v2f o; o.worldPos = mul(UNITY_MATRIX_M, v.vertex); // converts vertex coords to world coords by multipling be Unity\u0026#39;s model matrix o.vertex = UnityObjectToClipPos(v.vertex); o.uv = TRANSFORM_TEX(v.uv, _MainTex); o.uv.x += _Time.y*.1 ; return o; } Now, instead of using the UV coordinates to sample a color from the texture, we\u0026rsquo;re going to use the world position.\nfloat4 frag(v2f i) : SV_Target{ float2 topDownProjection = i.worldPos.xz; // We want to apply the texture only to the top of a flat plane so we take the x and z coordinates. fixed4 col = tex2D(_MainTex, topDownProjection); return col; } Texture Masking We can also produce a masking effect by drawing another texture and using its values to mask out specific parts of our main texture. This is the same as you would do when masking textures on photoshop.\nTo begin, let\u0026rsquo;s create a mask and play with it. We\u0026rsquo;ll do a simple mask of black and white. This is drawn simply in Clip Studio.\nCreate a new texture input we\u0026rsquo;ll call _MaskTex and use the mask as an input.\nTo see how we can play with the values, sample the colors from the mask with our object uv and run it through a sign function to get waves of values based on the color at each pixel of the mask.\nfloat GetWave(float coord) { float wave = sin( (coord + _Time.y)*2*TAU ); // 2*TAU makes the period do two full cycles between an x input of 0-1 // notice the repeating waves around transition between white and black from the original mask wave *= coord; // keeps colors that are already dark from cycling to white return wave; } float4 frag(v2f i) : SV_Target{ col = tex2D(_MaskTex, i.uv); return GetWave(col); } Mixing Textures When you think of the ground texture of a surface, there\u0026rsquo;s many component to it. There are dirt surfaces, grassy areas, rocks, and everything in between. You need a way to smoothly transition from one texture to another.\nWe can also mix between multiple textures through the use of lerp where the final parameter decides which texture to show. Instead, let\u0026rsquo;s sub in our sample from our mask to decide between textures. With more detailed masks, you can get much more realistic transitions between different textures.\nfloat4 frag(v2f i) : SV_Target { float2 topDownProjection = i.worldPos.xz/5; float4 tex0 = tex2D(_MainTex, topDownProjection); float4 tex1 = tex2D(_SecondTex, topDownProjection); float mask = tex2D(_MaskTex, i.uv); col = lerp(tex0, tex1, mask); return col; } We\u0026rsquo;re still mapping the texture based on world coordinates so the textures is consistent no matter the position of the plane. Additionally, techniques such a noise are used to create better transition between textures but we remain this simple example for now.\nMipmaps When textures are stored in Unity, they are often converted into a series of lower and lower resolution images kept on one file. This way, when textures are shown on screen, Unity can sample from the lower resolution images based on the position of the camera from the texture to get cleaner and less noisy results.\nIsotropic - Stores series of smaller textures for better sampling at further distances Anisotropic - Stores series of squished textures for better sampling at extreme angles Filtering Single - No blending between pixels (nearest neighbour) Bilinear - Blends between pixels Trilinear - Blends between pixels and mip levels Sometimes lines can show up on the texture when the sampler switches mip levels, especially for low angle anisotropics ","permalink":"http://localhost:1313/posts/2023/old/dev/intro_to_shaders/","summary":"Shaders are programs used to describe how pixels should be arranged, colored, and transformed on the screen. The simple definition extends to so many applications especially in 3D graphics in movies and games. Every computer generated prop placed in a scene is carefully designed to look a certain way, all with the help of shaders.\nShader\u0026rsquo;s make use of the GPU to constantly run concurrent calculation on every pixel and you can decide what those calculations do through programming with languages such as HLSL and GLSL.","title":"Introduction to Unity Shaders"},{"content":"When describing rotations of an object, the typical way would be through linear algebra and trigonometry. However, similarly to how complex numbers can be used to describe rotations in 2D, quaternions allow efficient and more pragmatic methods describing rotations in 3D.\nHow It\u0026rsquo;s Done With Trigonometry Depending on the framework or software using, a lot of the work is already done for you with API available to you to perform rotations on shapes. But for a simple point in space rotating in about an origin, what would that look like?\nYou can represent the x and y position of a point as a function of the angle and radius of the point on a unit circle. That is using our good friend SOH CAH TOA, you can represent x and y as:\nx = r x cos(\\theta) y = r x sin(\\theta) Then you can sub into \\theta, the angle around the origin you\u0026rsquo;d like the point to sit. However, this requires knowing the current angle the point sits at which adds another layer of computation. I got this proof from Khan Academy that gives the rotated point in relation to it\u0026rsquo;s original position and the desired angle of rotation.\nx′=x×cos(β)−y×sin(β) y′=y×cos(β)+x×sin(β) From there, you can apply this formula to every point and depending on the axes you\u0026rsquo;re rotating in space, we just swap the coordinates in the equation to the ones that change such as the z coordinate.\nFor example, instead of rotating about the Z-axis to rotate about the X-axis you\u0026rsquo;d do the following.\nz′=x×cos(β)−y×sin(β) y′=y×cos(β)+x×sin(β) In the case of a framework like ThreeJS, you\u0026rsquo;d represent each object rotation as a Vector3 containing all three axes. You can determine set each individual rotation directly without doing any of the math.\nSo far, we\u0026rsquo;ve been looking at 3D rotations as a manipulation of 3 axes of rotation also known as Euler Angles. However, this method of handling rotations is susceptible to 2 main probles.\nGimbal Lock - When two axes of rotation line up, two of the rotation axes perform the same rotation causing a loss of a dimension of rotation. This is shown beautifully by GuerrillaCG Interpolation - It\u0026rsquo;s exceedingly difficult to accurately move smoothly between different rotations Quaternions At first, the idea of using a whole different number system just for rotations can be quite abstract and even visualizing it is even more so. But, once the fundamentals are understood of how and why this works so efficiently, quaternions can be used very easily in software packages like Unity that handle the math calculations for you. A great explanation of application can be found here while a more comprehensive look at the underlying math is given by the always fantastic 3Blue1Brown\nVisual Understanding in Software Applied in blender, you may notice that influencing a single quat value doesn\u0026rsquo;t rotate the way you expect. Instead, altering a lone quat value will only flip the mesh into different orientations as follows, no rotations in between.\nSmooth rotation starts happening when you influence multiple values at once. In essence, the orientations are mixing depending on the weight you give to each value. Therefore, when thinking about how each quaternion value relates to its 3D rotation, you can think of a quaternion rotation as mixing together these four orientations in order to create the desired rotation. Of course, it\u0026rsquo;s not that simple as the way the mixing and rotating happens is a result of the underlying math but I think this way of thinking offers better visual and it can aid in understanding what\u0026rsquo;s really happening.\nFor instance, if you start with the model facing towards us with the quarternion (0, 0, 0, 1) and we want to flip it 90deg about the X-axis. What we should really be thinking is \u0026ldquo;Which two orientations can I mix to get what I want\u0026rdquo;. You might notice in the above example that a quaternion of (1, 0, 0, 0) rotates 180deg about the X-axis. So, wouldn\u0026rsquo;t we be wanting some rotation in between these two? This is precisely the answer. (1, 1, 0 0 ) mixes evenly the starting position and the 180deg rotation to result in a 90deg rotation.\nFor more complex rotations, you can imagine mixing in the other values to get further rotations. Here I add to Y and the mesh rotation mixes towards the upside-down, front facing position.\nit turns out you can achieve every 3D rotation this way without any of the drawbacks of Euler rotations.\nOne thing to notice is when mixing rotations, you can scale each value infinitely and the dominant value will mix the rotation the most. But to get a precise orientation, for example facing forwards, it\u0026rsquo;s best to return to whole numbers and setting unneeded rotations to 0 instead of battling out which rotation wins.\nSo Why Should I Use It? It comes down to if and when you may encounter the draw backs of using Euler angles. Euler angles are much easier for understand and manipulate mentally but come with the Gimbal drawbacks and interpolation issues that can become a larger issue in physics and animations.\nTypically, physics won\u0026rsquo;t be an issue as you can allow game engines to take care of that for you. For animations, it can be a benefit to use Quaternions over Euler angles to circumvent undesirable rotation paths. Otherwise, if you\u0026rsquo;re only performing 2D linear rotations, Euler angles can perform this perfectly and it only really comes to a mathematical and performance benefit with Quaternions.\nIn summary, Quaternions are superior in every way except for human readability.\n","permalink":"http://localhost:1313/posts/2023/old/dev/quaternion/","summary":"When describing rotations of an object, the typical way would be through linear algebra and trigonometry. However, similarly to how complex numbers can be used to describe rotations in 2D, quaternions allow efficient and more pragmatic methods describing rotations in 3D.\nHow It\u0026rsquo;s Done With Trigonometry Depending on the framework or software using, a lot of the work is already done for you with API available to you to perform rotations on shapes.","title":"Understanding Quaternions"},{"content":"I followed this tutorial https://www.youtube.com/watch?v=bXNFxQpp2qk\u0026amp;list=PLwyUzJb_FNeQrIxCEjj5AMPwawsw5beAy\u0026amp;index=3\nAdding a 3D Character Animation States Unity has it\u0026rsquo;s own skeleton rigging system called Mecanim.\nTo add animations to a character, you first need to add an animator component to your character model and then create a character controller object and link it to the component. Double clicking the controller will then open up a panel to handle state transitions between animations.\nThere you can add each of the states you want to transition your model to and from. I set idle to be the default state which is self-explanatory. From there, I connect transition arrows between each of the states and set a condition for each of the transitions to occur.\nThe condition will just be a boolean that I can use later in script. These booleans can be declared on a side panel called paramters.\nLastly, you assign your animation to the motion property of each animation state.\nOnce this is setup, immediately you will see your character using the playing the animation and using the default state that\u0026rsquo;s been set.\nCharacter Inputs The Old Way The old way of managing inputs in Unity is by using the input manager accessible in the project settings. There you can set mapping of keys to actions and use those new naming in code. For example, you can map W and UP to jump, and use that naming in your code. Whenver you need to change it, just change it from the input manager instead of digging through code.\nif (Input.GetKeyDown(\u0026#34;jump\u0026#34;)) { // do stuff } The New Input System However, this system has a few drawbacks\nDynamically changing the bindings is difficult Can have some confusion over the types of inputs (Controller vs Joystick) The new input system comes with addresses to these as well as a load of new features that make it much easier to manage all types of inputs and switch them on the fly without needing to write a bunch of your own code.\nThese are the basic steps from what I\u0026rsquo;ve gathered\nCreate an InputActions object (Usually like to create a seperate directory for the file)\nOpen it and setup new maps and bindings (there are defaults that may fit your needs as is)\nAdd a new C# script to your character to handle movement\nIn the awake function access your components as well as creating a PlayerInput object to access your InputActions\nprivate void Awake() { input = new PlayerInput(); characterController = GetComponent\u0026lt;CharacterController\u0026gt;(); animator = GetComponent\u0026lt;Animator\u0026gt;(); } Notice this PlayerInput class was generated by ticking the \u0026ldquo;Generate C# Class\u0026rdquo; option from the InputActions object. This is an alternative way to access PlayerInputs instead of adding the component directly to your character and attaching the relevant scripts. From there you can access your inputs, animations, as well as character manipulation. I won\u0026rsquo;t go into the logic but I will leave some examples of how to use these classes.\nInput callbacks\ninput.Player.Move.started += onMoveInput; input.Player.Move.canceled += onMoveInput; input.Player.Move.performed += onMoveInput; A list of callback functions you add to be triggered after the indicated events\nAnimator Parameters\nisWalkHash = Animator.StringToHash(\u0026#34;isWalking\u0026#34;); bool isWalking = animator.GetBool(\u0026#34;isWalking\u0026#34;); animator.SetBool(isWalkHash, true); Altering the parameters let\u0026rsquo;s you start and stop animations based on your state graph. The StringToHash is a performance optimization to use instead of pure strings to access a parameter\nManipulating the character\ncharacterController.Move(currMove * currRunMultiplier * Time.deltaTime); Use Unity\u0026rsquo;s built in controller to handle movement and collisions. In this case, to move the character.\n","permalink":"http://localhost:1313/posts/2023/old/dev/3d_character_unity_setup/","summary":"I followed this tutorial https://www.youtube.com/watch?v=bXNFxQpp2qk\u0026amp;list=PLwyUzJb_FNeQrIxCEjj5AMPwawsw5beAy\u0026amp;index=3\nAdding a 3D Character Animation States Unity has it\u0026rsquo;s own skeleton rigging system called Mecanim.\nTo add animations to a character, you first need to add an animator component to your character model and then create a character controller object and link it to the component. Double clicking the controller will then open up a panel to handle state transitions between animations.\nThere you can add each of the states you want to transition your model to and from.","title":"3D Character Unity Setup"},{"content":"I decided to make my own osu banner. My profile was looking pretty empty without one and I figured after all that work to put together my own rendition of Enchanted Love, might as well put those meshes to more use. I wanted to keep it fairly simple as I know I\u0026rsquo;m pretty limited in gif size and quality for it to work as a profile banner (otherwise if it\u0026rsquo;s too large, the auto-compression will convert the image into a static jpeg).\nDesigning the Banner I did some playing around and took some inspiration from this amazing art work. Linear ring - Enchanted love@karmabethe pic.twitter.com/9nUHgIcG5u\n\u0026mdash; BigheadCrusher (@bigheadcrusher) January 2, 2022 From my previous artwork, I already had a lot of the patterns created as well as familiarity with the style so it really was a process of just stamping around different shapes and decorations until I liked it.\nOnce the main banner was done, I exported them in layers so I can seperate out where I want the foreground and backgrounds to be in relation to the animated characters.\nLayering into Blender With those exported, I moved on to organizing them into Blender. It took me a while to realize that just importing them is as images wasn\u0026rsquo;t going to render. Turns out, pure image objects are more for reference than anything. Instead, I needed to create an actual object in Blender and have the image on it as a texture. Luckily Blender has a nifty addon to import images directly as plane objects which will render.\nHowever, I had a lot of trouble getting images as planes to show up because in my original project, I forgot I removed the world lighting entirely, so any objects with just the base shaders would just render black with no light. Once I realized this, I had to create a new shader. It seemed simple enough by just plugging in the image color directly into the material output but I forgot about the transparent parts of the images, they would also just render black this way. I managed a solution by mixing the image colors with a transparent shader and deciding when to mix into transparent based on the alpha channel of the image. Looking at it now, seems pretty obvious, but boy it did not come easy.\nAs a self note, there was a bit of white dithering around the pngs when I imported them in. By changing the image node interpolation from linear to closest, the issue was gone. The white dithering is still in the original image though this is just an issue with Clip Studio\u0026rsquo;s exporting. Apparently, Photoshop has a \u0026ldquo;matted\u0026rdquo; option when exporting png\u0026rsquo;s which removes dithering. Hopefully, a future version of CSP will also have the feature.\nOnce I finally got the images texture to show in full color, I could start layering up the banner.\nAnimating Animating was pretty simple, I only animated the positions of the characters and props with a subtle rotation to Mocha\u0026rsquo;s head to give her a little bit more life as well as the beach ball. Seeing the rotations in the final render really adds a cool depth to what is otherwise a very 2D composition.\nLast thing to do is crank out final render and we\u0026rsquo;re done!\nWell, almost. I had to convert it to a gif and do some optimizing to make it fit as a banner. As per usual, the online tool ezgif is overpowered. In general, I converted it to a banner sized gif, removed duplicate frames at the beginning and end, extended the final frame duration so the loop isn\u0026rsquo;t too distracting, removed some in between frames to save space, and ran their optimizer to reduce the file size (around .94mb seems to be the sweet spot to not get auto-compressed). Now we have a finished animated Osu banner!\n","permalink":"http://localhost:1313/posts/2023/old/osu/osu_banner/","summary":"I decided to make my own osu banner. My profile was looking pretty empty without one and I figured after all that work to put together my own rendition of Enchanted Love, might as well put those meshes to more use. I wanted to keep it fairly simple as I know I\u0026rsquo;m pretty limited in gif size and quality for it to work as a profile banner (otherwise if it\u0026rsquo;s too large, the auto-compression will convert the image into a static jpeg).","title":"Making an Animated Osu Banner"},{"content":"Now that I\u0026rsquo;m starting on Slime Hunter, I figured I should get back to using some project management tools. In particular, Kanban boards as they are simple to manage and don\u0026rsquo;t come with nearly as much fluff as something like Jira issue tracking systems. The nice thing as well is that there\u0026rsquo;s so many great project management tools out there. In this case, I\u0026rsquo;ve decided to use Trello as I\u0026rsquo;m familiar with Atlassian tools (again Jira) so I feel comfortable continuing to use their products. There also an industry standard in this field and I expect to continue on for many years. There are definitely are tools out there that might fit the simplicity I need much better given there size, I don\u0026rsquo;t want to worry about the chance that may lack software maintenance or one day just disappear altogether.\nWhile the obvious benefits of having a more organized workflow is clear, I personally value it most for keeping the mentality of \u0026ldquo;one step at a time\u0026rdquo;. Getting a project going takes a long time and it can feel like nothings coming together as you continue to hammer out pieces of it with no discernable progress being made. Splitting up small tasks on a Kanban board and completing them, no matter how trivial the tasks is, you have something palpable you can consider \u0026ldquo;done\u0026rdquo;. It\u0026rsquo;s a nice mental reminder that you have done something today and a reminder of all that you\u0026rsquo;ve done the days before.\n","permalink":"http://localhost:1313/posts/2023/old/other/using_kanban_boards/","summary":"Now that I\u0026rsquo;m starting on Slime Hunter, I figured I should get back to using some project management tools. In particular, Kanban boards as they are simple to manage and don\u0026rsquo;t come with nearly as much fluff as something like Jira issue tracking systems. The nice thing as well is that there\u0026rsquo;s so many great project management tools out there. In this case, I\u0026rsquo;ve decided to use Trello as I\u0026rsquo;m familiar with Atlassian tools (again Jira) so I feel comfortable continuing to use their products.","title":"Using Kanban Boards"},{"content":"When using a programming engine or framework, one of the things I get stuck on the most is how to organize all the directories and files for a project. Every technology has its own separate ethos when it comes to the separation of concerns. It\u0026rsquo;s such an important part in order to not have a growing project becoming a nightmare for other\u0026rsquo;s to work with, or even yourself after spending some time away from it.\nI often get to caught up in getting and not getting it right as I\u0026rsquo;m afraid of the above reasons, but honestly I think I should just let go a little bit and allow myself to make mistakes. One thing to note about folder structure is that the best structure depends on your project and your workflow and that can\u0026rsquo;t be decided until you at least make some headway into the project itself first.\nI often go back to this one quote I heard from Jacob Collier\n\u0026ldquo;Less is more. But, less is only more if you know what more is.\u0026rdquo; - Jacob Collier\nThis very well may be a gross paraphrase, but I stand by it strongly for most things in life. In the end, it comes back to not being afraid of making mistakes but even more so, it means you can only get the true appreciation of the sublime if you let yourself experience the amiss. Then, when you take a step back, you finally know at the core, why doing something one way is better and why it isn\u0026rsquo;t.\nIt\u0026rsquo;s also an opportunity for your to decide if someone else\u0026rsquo;s sublime is also yours.\nAnyways, this came about because I was harrowing over how to set up my Unity folder structure. There\u0026rsquo;s some good tips I found here https://unity.com/how-to/organizing-your-project. I suppose it\u0026rsquo;s good to have some pointers but perhaps I\u0026rsquo;ll follow my own tenet above and don\u0026rsquo;t overthink and instead, just do.\n","permalink":"http://localhost:1313/posts/2023/old/other/folder_structure/","summary":"When using a programming engine or framework, one of the things I get stuck on the most is how to organize all the directories and files for a project. Every technology has its own separate ethos when it comes to the separation of concerns. It\u0026rsquo;s such an important part in order to not have a growing project becoming a nightmare for other\u0026rsquo;s to work with, or even yourself after spending some time away from it.","title":"Folder Structure and Making Mistakes"},{"content":"Journey into a Beatmap Getting Started After doing my first fanart, I didn\u0026rsquo;t have any intention of doing another but this one\u0026rsquo;s theme instantly gave me the idea of incorporating 3D modelling into the art piece. In summary, the theme is to create an Osu fanart with the OC\u0026rsquo;s inside a beatmap.\nI immediately had the idea of drawing Enchanted Love as I fell in love with the visuals from the original MV. I also had some inkling that the video itself was done in 3D after examining the characters movements. I thought, \u0026ldquo;well isn\u0026rsquo;t this a good excuse to learn some Blender then?\u0026rdquo; That\u0026rsquo;s how I got started.\nAt first, the idea of recreating these visuals in Blender was purely conceptual but after doing some digging, I came across some helpful posts on Mozuya\u0026rsquo;s twitter that shed some light into their process. シェイプレイヤーっぽくみせるために腕や足は円柱で作成して、動いても幅が変わらないようにしました。\n顔のパーツは頭の内部にあるので、パーツごとにレンダリングしてAEで合成してます。詳しくはaepを参照してください。 pic.twitter.com/xVXN1CtrCZ\n\u0026mdash; 百舌谷 (@mozuya_) November 20, 2020 From there, I took a detour from working purely on the art piece to learning some blender fundamentals through a doughnut rendering tutorial from BlenderGuru\nMy result from the tutorial looked pretty bad I think, especially if you compare it to his results. I decided not to overthink it as I just wanted to get a feel for how to use the software anyways. Also a note for myself, I chose to render with Eevee as opposed to Cycles.\nThe Background Once I got some Blender fundamentals to work with, I figured it was time to move on to modelling the characters for real\u0026hellip; Except I really procrastinated this. It just felt daunting to actually start modelling anything on my own and when I did start I\u0026rsquo;d just scrap it after a few minutes and give up. Instead, I again took a detour and decided to put together a background for the art piece.\nI had a lot fun with this. Since the background is purely graphic rather than painterly, I felt a lot less of that artist daunt that kept me from starting anything. I didn\u0026rsquo;t have to worry about line consistency or symmetricity, in fact, I did everything with my mouse.\nThe repeating patterns (the hearts) were really satisfying to make. A simple picture suddenly becomes a visually pleasant mosaic just through some basic tiling. Perhaps the speed of putting together a more graphic art style made it more enjoyable.\nOnce the background was finished, I had a new found motivation to get started on the Blender portion (in actuality, I still procrastinated to the last couple days before the deadline).\nModelling Modelling was mostly a trial and error process of moving points around until I got the right shape. I made heavy use of the Mirror modifier to keep the shape consistent and essential do half the modelling work. I would include some more examples of this process\u0026hellip; But once again, I am writing this long after the fact and I don\u0026rsquo;t feel like backtracking to get a few clips heh, apologies!\nI will however highlight the rigging and weight painting because that really boggled my mind. I had done some weight painting for my practice doughnut to paint over the sprinkle distribution over the top of the doughnut. I did not expect weight painting to be an integral part of skeleton rigging though. Turns out, each joint in the skeleton rig is associated to the mesh through a heatmap which determines which part of the mesh should move. Hot spots means the mesh will firmly twist and bend on the joint, cold spots, less so.\nThere is an option to have Blender automatically weight paint for you but it was extremely off when I used it on my mesh so I had to manually weight paint everything.\nOften, I get lazy whether it\u0026rsquo;s making my bed or cleaning the house. Creative work is no different. The shadows of Mocha (the green haired girl) aren\u0026rsquo;t actually shadows at all. I just duplicated all of the meshes and shaded them to the colour I wanted. I then shifted their axes behind and slightly to the side of the real meshes to mimic shadows. I\u0026rsquo;m sure there\u0026rsquo;s some way to project to model onto some plane under it and get a much more accurate and resourceful shadow but I was running out of time (less than an hour left) and brain cells to explore that solution. So instead this is what I did and it worked. I remember a quote that went something like it\u0026rsquo;s better to fake it than to make it. Something one of the Rainworld devs said I believe. Well, here it is in action.\nFinally, I composited my background image behind my meshes and gave it the final render.\n","permalink":"http://localhost:1313/posts/2023/old/osu/osu_fanart2/","summary":"Journey into a Beatmap Getting Started After doing my first fanart, I didn\u0026rsquo;t have any intention of doing another but this one\u0026rsquo;s theme instantly gave me the idea of incorporating 3D modelling into the art piece. In summary, the theme is to create an Osu fanart with the OC\u0026rsquo;s inside a beatmap.\nI immediately had the idea of drawing Enchanted Love as I fell in love with the visuals from the original MV.","title":"Osu Fanart Again!?"},{"content":"osu! Fanarts? Osu holds several fanart contest every year with themes often related to the time of year. Themes such as Winter or Halloween are such examples. Regardless of theme, the general idea seems to always be to draw any of the osu mascots inside or partaking in said theme.\nAt the time I had been playing osu for about 4 months and really enjoying it. On the other hand, I\u0026rsquo;ve been drawing casually for a few years as a hobby so it seemed naturally to want to give these contests a shot. Particularly, I saw it as an opportunity to push my art skills (as usually I just doodle around rather than create full pieces) as well as something to occupy my time.\nNew Beginnings This was my first ever entry into a fanart contest. The theme of this year was as the name suggests was, new beginnings, as a hallmark of the coming new year and for the future of Osu. The original post can be found here.\nThe newness of it all actually gave me a lot of motivation and ideas for what to draw. Admittedly, I enjoy drawing anime girls so naturally I acclimated to making the girls a focal point. Actually anime is quite closely connected with Osu in it\u0026rsquo;s mascots and beatmaps anyways so its a pretty natural direction. I ended up deciding on drawing Pippi in a kimono partaking in a a Japanese New Year Festival. In my early sketches, I wanted to go with a fairly exaggerated perspective to hopefully make the piece feel more dynamic and also as practice for me in drawing perspective.\nI tried sketching out the perspective freehand but it didn\u0026rsquo;t take long to realize that I needed some reference. Luckily, Clip Studio Paint has a 3D model feature that let me place some character as well as background models.\nThese fireworks were a memorable challenge cause it was one of the times I where I just felt completely lost as to how to draw something. I had some fundamentals of painting and texturing that I could apply to something like the clouds but when it came to the vibrancy of fireworks, I didn\u0026rsquo;t even know where to start. I started off by following some tutorial I found online but it just kept looking like neon lightsticks sprawled out into a flower. After a few hours I gave up following direct tutorials and just went back to looking at a bunch of anime references (think Kimi No Na Wa) and tried my best to replicate it through blending layers. The result look 10x better and although it wasn\u0026rsquo;t nearly anything close to the references I used, I was more then happy to move on. Though I must add, later I added some of the fireworks lighting onto the clouds and that really sold the look I wanted.\nOnce I got most of the base colors and shadows worked out, it came time to work out the lighting. I went through a lot of iterations to try to get something that looked good. I really agonized over where the light source would be and how that would shine on the character and where shadows would be. In the end I kept getting results that had shadows that didn\u0026rsquo;t feel right and also made Pippi less of the focal point I wanted. In the end, I just illuminated the parts I wanted to be on focus and then added a bunch of ambient light to balance out the shadows until it looked right. I\u0026rsquo;m a little disappointed I couldn\u0026rsquo;t apply the lighting with more intention and spatial awareness but this was the best I could do for now. Perhaps next time I\u0026rsquo;ll use more references to base my lighting upon. There were plenty of more things that had gone through my mind while drawing this fanart but I am writing this log long after the fact so much of it isn\u0026rsquo;t fresh in my mind and I might just be too lazy right now to recount it all heh. In any case, below is a final timelapse of my work using Clip Studio\u0026rsquo;s built in timelapse function. It doesn\u0026rsquo;t include any of the CTRL+Zing I did which may as well have been 50% of the whole drawing process.\nThere should have been a video here but your browser does not seem\rto support it.\rAll in all, I\u0026rsquo;m really proud of what I managed to create but if anything it let me appreciate how talented the other art submissions were (seriously, check them out) and where my current strengths and weaknesses are. It\u0026rsquo;s so valuable and it makes me want to participate in more of these contests.\n","permalink":"http://localhost:1313/posts/2023/old/osu/osu_fanart/","summary":"osu! Fanarts? Osu holds several fanart contest every year with themes often related to the time of year. Themes such as Winter or Halloween are such examples. Regardless of theme, the general idea seems to always be to draw any of the osu mascots inside or partaking in said theme.\nAt the time I had been playing osu for about 4 months and really enjoying it. On the other hand, I\u0026rsquo;ve been drawing casually for a few years as a hobby so it seemed naturally to want to give these contests a shot.","title":"Making Osu Fanart"},{"content":"Hi, I\u0026rsquo;m a developer. That in itself doesn\u0026rsquo;t mean too much really and for me it only really marks the beginning of what I would like to do and achieve. I\u0026rsquo;ve read my share of devlogs and appreciate the amount of value it gives to readers while also envying the sheer knowledge and coherence each of the authors are able to display. I hope I can also create devlogs of similar value. Saying that, I don\u0026rsquo;t really expect to have an audience in this blog\u0026ndash;infact part of me is too embarrassed to have this shown to anyone.\nI\u0026rsquo;d like to treat this as something more of a pseudo diary for my own personal review in a way of progress marking as well as cementing the things I\u0026rsquo;ve learned over the course of my journey. However, the frightening nature of digital media is that these logs may be here to stay forever which may be long enough to outlast my current views and mindset. What I\u0026rsquo;m saying is, if it\u0026rsquo;s not myself reading this now, then either your pilgrammage has led you to somewhere quite remote and I hope you safe passage back, or I\u0026ndash;with many emotions\u0026ndash;have decided these are fit for public consumption to which I look up to that me with equal fondness and contempt.\n","permalink":"http://localhost:1313/posts/2023/old/other/first-post/","summary":"Hi, I\u0026rsquo;m a developer. That in itself doesn\u0026rsquo;t mean too much really and for me it only really marks the beginning of what I would like to do and achieve. I\u0026rsquo;ve read my share of devlogs and appreciate the amount of value it gives to readers while also envying the sheer knowledge and coherence each of the authors are able to display. I hope I can also create devlogs of similar value.","title":"New Beginnings"}]